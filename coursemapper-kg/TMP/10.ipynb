{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "# from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_per_columns(value_counts: pd.core.series.Series, nbr_of_rank=6): # video=6 AND article 3\n",
    "    ranks = list(range(1, (nbr_of_rank + 1)))\n",
    "    cols_values_tuple = [c for c in value_counts.items()]\n",
    "    ranks_found = [v[0] for v in cols_values_tuple]\n",
    "    ranks_not_found = [rank for rank in ranks if rank not in ranks_found]\n",
    "    values_tuple_second = [(v, 0) for v in ranks_not_found]\n",
    "    values_tuple_complete = cols_values_tuple + values_tuple_second\n",
    "\n",
    "    values_tuple_complete = sorted(values_tuple_complete, key=lambda x: x[0])\n",
    "    counts = [v[1] for v in values_tuple_complete]\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for Chi-Square Goodness of Fit Test\n",
    "def prepare_data(df: pd.DataFrame, nbr_of_rank=6):\n",
    "    # remove user column\n",
    "    if \"user\" in df.columns.values.tolist():\n",
    "        df = df.drop(columns=['user'])\n",
    "    \n",
    "    # nbr_of_rank = 6\n",
    "    factors_row = {}\n",
    "    new_rows = []\n",
    "    columns_refs = []\n",
    "    for name in df.columns.values:\n",
    "        r = get_counts_per_columns(df[name].value_counts(), nbr_of_rank=nbr_of_rank)\n",
    "        factors_row[name] = r\n",
    "        new_rows.append(r)\n",
    "        columns_refs.append(name)\n",
    "\n",
    "    columns_names = [f\"rank {i}\" for i in list(range(1, (nbr_of_rank + 1)))]\n",
    "    df = pd.DataFrame(new_rows, columns=columns_names)\n",
    "    df.insert(0, 'factors', columns_refs)\n",
    "\n",
    "    return (df, columns_refs, new_rows, factors_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def get_chi_test_result(observed_info: list):\n",
    "    # observed_info = [[100, 200, 300], [50, 60, 70]]\n",
    "\n",
    "    # Print the observed frequencies\n",
    "    # print(\"Observed frequencies:\")\n",
    "    # for row in observed_info:\n",
    "    #     print(row)\n",
    "\n",
    "    # Perform the chi-square test\n",
    "    # stat, p, dof = chi2_contingency(observed_info)\n",
    "    chi2, p, dof, expected = chi2_contingency(observed_info)\n",
    "\n",
    "    # Set the significance level (alpha)\n",
    "    significance_level = 0.05\n",
    "\n",
    "    # Print the degree of freedom and p-value\n",
    "    print(\"p-value:\", p)\n",
    "    print(\"\\nDegree of freedom:\", dof)\n",
    "    print(\"chi2-value:\", chi2)\n",
    "\n",
    "    # Interpret the results\n",
    "    if False:\n",
    "        if p <= significance_level:\n",
    "            print(\"Reject NULL HYPOTHESIS: There is a significant association between the variables OR factors.\")\n",
    "            # print(\"Reject the null hypothesis: There is a significant association between the variables.\")\n",
    "        else:\n",
    "            print(\"ACCEPT NULL HYPOTHESIS: No significant association between the variables OR factors.\")\n",
    "            # print(\"Fail to reject the null hypothesis: There is no significant association between the variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample of n (default 100) User Opinion Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def save_csv(df: pd.DataFrame, filename: str = \"sample_data.csv\"):\n",
    "    df.to_csv(filename, encoding='utf-8', index=False)\n",
    "\n",
    "def generate_rows_data(columns_names: list, nbr_of_rank=6, nbr_of_sample = 100, csv_file = True):\n",
    "    data = []\n",
    "    for i in list(range(nbr_of_sample)):\n",
    "        row = random.sample(range(1, (nbr_of_rank + 1)), nbr_of_rank) # random.sample(range(1, 7), 6)\n",
    "        row = [f\"Rank {i}\" for i in row]\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=columns_names)\n",
    "    df.insert(0, 'user', range(1, len(df) + 1))\n",
    "\n",
    "    if csv_file:\n",
    "        save_csv(df=df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Factor Weight for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_read_1 = ['Similarity Score', 'Creation date', 'No. of Views', \n",
    "                   \"No. of Likes on YouTube\", \"Rating on CourseMapper\",\n",
    "                   \"No of. Save on CourseMapper\"\n",
    "                ]\n",
    "columns_to_read_2 = ['Similarity Score', \"Rating on CourseMapper\",\n",
    "                   \"No of. Save on CourseMapper\"\n",
    "                ]\n",
    "\n",
    "nbr_of_rank = 6\n",
    "# df = pd.read_excel(\"factors ranking 2 (Responses).xlsx\", usecols=columns_to_read_1)\n",
    "# df = pd.read_excel(\"factors ranking 2 (Responses)_3_f.xlsx\", usecols=columns_to_read_2)\n",
    "# df = generate_rows_data(columns_names=columns_to_read_1, nbr_of_rank=6, nbr_of_sample=100)\n",
    "# df = generate_rows_data(columns_names=columns_to_read_2, nbr_of_rank=nbr_of_rank, nbr_of_sample=10)\n",
    "\n",
    "ROOT = \"/Users/wkana001/Desktop/work/tests/sch\"\n",
    "# df = pd.read_csv(f\"{ROOT}/sample_data.csv\", usecols=columns_to_read_1)\n",
    "# df = pd.read_csv(\"sample_data.csv\", usecols=columns_to_read_1)\n",
    "\n",
    "\n",
    "columns_default = [  \n",
    "                     \"No. of Likes on YouTube\", \n",
    "                     \"Creation Date\", \n",
    "                     \"No. of Views on YouTube\", \n",
    "                     \"Similarity Score\", \n",
    "                     \"No. of Saves on ABC\",\n",
    "                     \"User Ratings on ABC\"\n",
    "                ]\n",
    "df = pd.read_csv(\"factors ranking.csv\", usecols=columns_default)\n",
    "\n",
    "def rename_label_rank(x):\n",
    "    if x == \"Rank 1\":\n",
    "        return 1\n",
    "    elif x == \"Rank 2\":\n",
    "        return 2\n",
    "    elif x == \"Rank 3\":\n",
    "        return 3\n",
    "    elif x == \"Rank 4\":\n",
    "        return 4\n",
    "    elif x == \"Rank 5\":\n",
    "        return 5\n",
    "    elif x == \"Rank 6\":\n",
    "        return 6\n",
    "    return x\n",
    "\n",
    "def reverse_rank_to_weight(x):\n",
    "    if x == 1:\n",
    "        return 6\n",
    "    elif x == 2:\n",
    "        return 5\n",
    "    elif x == 3:\n",
    "        return 4\n",
    "    elif x == 4:\n",
    "        return 3\n",
    "    elif x == 5:\n",
    "        return 2\n",
    "    elif x == 6:\n",
    "        return 1\n",
    "    return x\n",
    "\n",
    "for name in df.columns.values:\n",
    "   df[name] = df[name].apply(lambda x: rename_label_rank(x))\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chisqt = pd.crosstab(BIKE.holiday, BIKE.weathersit, margins=True)\n",
    "# # cross_tab = pd.crosstab(df['Similarity Score'], df['Creation date'], df['No. of Views']\n",
    "# #                         # df['No. of Likes on YouTube'], df['Rating on CourseMapper'], \n",
    "# #                         # df['No of. Save on CourseMapper']\n",
    "# #                         )\n",
    "# # cross_tab = pd.crosstab(df['Similarity Score'], [df['Creation date'], df['No. of Views'], \n",
    "# #                                        df['No. of Likes on YouTube'], df['Rating on CourseMapper'], \n",
    "# #                                        df['No of. Save on CourseMapper']\n",
    "# #                                        ]\n",
    "# #                         )\n",
    "# # cross_tab\n",
    "\n",
    "# # data = df.head(5).to_dict(orient='list')\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Given dictionary with 'list' orientation\n",
    "# data = {\n",
    "#     'A': [2, 2, 2, 2, 2],\n",
    "#     'B': [5, 4, 6, 5, 4],\n",
    "#     'C': [4, 1, 1, 6, 3],\n",
    "#     'D': [3, 6, 5, 3, 5],\n",
    "#     'E': [1, 5, 4, 4, 1],\n",
    "#     'F': [6, 3, 3, 1, 6]\n",
    "# }\n",
    "\n",
    "# # Convert dictionary to DataFraÃ¥me\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Perform cross-tabulation\n",
    "# # cross_tab = pd.crosstab(df['A'], [df['B'], df['C'], df['D'], df['E'], df['F']])\n",
    "# cross_tab = pd.crosstab(df['B'], df['C'])\n",
    "\n",
    "# # Display the cross-tabulation\n",
    "# print(\"Cross-tabulation:\")\n",
    "# cross_tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd = prepare_data(df=df, nbr_of_rank=nbr_of_rank)\n",
    "get_chi_test_result(observed_info=ppd[2])\n",
    "\n",
    "ppd[0].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_highest_counts = {}\n",
    "for k,v in ppd[3].items():\n",
    "    factor_highest_counts[k] = max(v)\n",
    "\n",
    "factor_weights = factor_highest_counts\n",
    "factor_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factor_weights(df: pd.DataFrame):\n",
    "    # remove user column\n",
    "    if \"user\" in df.columns.values.tolist():\n",
    "        df = df.drop(columns=['user'])\n",
    "        \n",
    "    factors_weights_cols = {}\n",
    "    for name in df.columns.values:\n",
    "        df[name] = df[name].apply(lambda x: reverse_rank_to_weight(x))\n",
    "        col_values = df[name].tolist()\n",
    "        factors_weights_cols[name] = round((sum(col_values) / len(col_values)), 3)\n",
    "\n",
    "    return factors_weights_cols\n",
    "\n",
    "# factor_weights = get_factor_weights(df)\n",
    "# factor_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize as normalize_sklearn, MinMaxScaler as MinMaxScaler_sklearn\n",
    "\n",
    "def normalize_factor_weights(factor_weights: dict=None, values: list=[], method_type = \"l1\", complete=True, sum_value=True): # List[float]\n",
    "    \"\"\"\n",
    "    https://www.pythonprog.com/sklearn-preprocessing-normalize/#Normalization_Techniques\n",
    "    TypeScript: https://sklearn.vercel.app/guide/install\n",
    "\n",
    "    factor_weights = { 'similarity_score': 0.7, 'creation_date': 0.3, 'nbr_views': 0.3, \n",
    "            'nbr_likes_youTube': 0.1, 'rating_courseMapper': 0.1, 'nbr_save_courseMapper': 0.1\n",
    "        }\n",
    "\n",
    "    method_type: normalization techniques\n",
    "        l1: L1 normalization, also known as L1 norm normalization or Manhattan normalization\n",
    "        l1: L2 normalization, also known as L2 norm normalization or Euclidean normalization\n",
    "        max: Max Normalization\n",
    "        min-max: Min-Max\n",
    "    \"\"\"\n",
    "    normalized_values = None\n",
    "    scaled_data = None\n",
    "    \n",
    "    if factor_weights:\n",
    "        values = [value for key, value in factor_weights.items()]\n",
    "        key_names = [key for key, value in factor_weights.items()]\n",
    "\n",
    "    if method_type == \"l1\":\n",
    "        normalized_values = normalize_sklearn([values], norm=method_type).tolist()\n",
    "    if method_type == \"l2\":\n",
    "        normalized_values = normalize_sklearn([values], norm=method_type).tolist()\n",
    "    if method_type == \"max\":\n",
    "        normalized_values = normalize_sklearn([values], norm=method_type).tolist()\n",
    "    if method_type == \"min-max\":\n",
    "        data = np.array(values).reshape(-1, 1)\n",
    "        scaler = MinMaxScaler_sklearn()\n",
    "        scaler.fit(data)\n",
    "        scaled_data = scaler.transform(data)\n",
    "        scaled_data = scaled_data.tolist()\n",
    "        scaled_data = [value[0] for value in scaled_data]\n",
    "\n",
    "    if normalized_values:\n",
    "        normalized_values = normalized_values[0]\n",
    "        normalized_values = [round(value, 3) for value in normalized_values]\n",
    "    elif scaled_data:\n",
    "        normalized_values = scaled_data\n",
    "\n",
    "    if sum_value:\n",
    "        print(\"sun values: \", sum(normalized_values))\n",
    "\n",
    "    if complete:\n",
    "        normalized_values = dict(zip(key_names, normalized_values))\n",
    "    \n",
    "    return normalized_values\n",
    "\n",
    "def get_factor_weight_by_scores_normalized(scores: list):\n",
    "    sum_scores = sum(scores)\n",
    "    weights_final = []\n",
    "    for score in scores:\n",
    "        cal = round((score / sum_scores), 3)\n",
    "        weights_final.append(cal)\n",
    "    return weights_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 normalization, also known as L1 norm normalization or Manhattan normalization\n",
    "# values = [22, 28, 24, 25, 25, 21]\n",
    "# values = [1, 0.6, 1, 0, 0, 0]\n",
    "\n",
    "# normalized_weights = normalize_factor_weights(values=values, method_type=\"l1\", complete=False, sum_value=True)\n",
    "normalized_weights = normalize_factor_weights(factor_weights=factor_weights, method_type=\"l1\", complete=False, sum_value=True)\n",
    "print(\"normalized_weights: \", normalized_weights)\n",
    "print(normalize_factor_weights(factor_weights=factor_weights, method_type=\"l1\", complete=True, sum_value=True))\n",
    "\n",
    "\"\"\"\n",
    "# l1: L2 normalization, also known as L2 norm normalization or Euclidean normalization\n",
    "\n",
    "normalized_weights = normalize_factor_weights(factor_weights=factor_weights, method_type=\"l2\", complete=False, sum_value=False)\n",
    "normalized_weights = get_factor_weight_by_scores_normalized(normalized_weights)\n",
    "print(\"normalized_weights: \", normalized_weights)\n",
    "print(\"sun: \", sum(normalized_weights))\n",
    "print()\n",
    "\n",
    "# max: Max Normalization (dividing by the highest value)\n",
    "\n",
    "normalized_weights = normalize_factor_weights(factor_weights=factor_weights, method_type=\"max\", complete=False, sum_value=False)\n",
    "normalized_weights = get_factor_weight_by_scores_normalized(normalized_weights)\n",
    "print(\"normalized_weights: \", normalized_weights)\n",
    "print(\"sun: \", sum(normalized_weights))\n",
    "print()\n",
    "\n",
    "# min-max: Min-Max\n",
    "\n",
    "normalized_weights = normalize_factor_weights(factor_weights=factor_weights, method_type=\"min-max\", complete=False, sum_value=False)\n",
    "normalized_weights = get_factor_weight_by_scores_normalized(normalized_weights)\n",
    "print(\"normalized_weights: \", normalized_weights)\n",
    "print(\"sun: \", sum(normalized_weights))\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(factor_weights):\n",
    "    col = [str(factor_weights[key])]\n",
    "    col.append(normalized_weights[i])\n",
    "    factor_weights[key] = col\n",
    "\n",
    "df = pd.DataFrame(factor_weights)\n",
    "new_column_data = [\"highest count\", \"normalized score\"]\n",
    "new_column_name = ''\n",
    "df.insert(0, new_column_name, new_column_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"video\": {\n",
    "        \"views\": 0.2,\n",
    "        \"rating\": 0.1,\n",
    "        \"creation_date\": 0.3,\n",
    "        \"similarity_score\": 0.1,\n",
    "        \"bookmark\": 0.1,\n",
    "        \"like_count\": 0.1\n",
    "    },\n",
    "    \"article\": {\n",
    "        \"rating\": 0.4,\n",
    "        \"similarity_score\": 0.3,\n",
    "        \"bookmark\": 0.3\n",
    "    }\n",
    "}\n",
    "weigths_normalized = {}\n",
    "for k,v in data.items():\n",
    "    weigths_normalized[k] = normalize_factor_weights(  factor_weights=v, \n",
    "                                                    method_type=\"l1\", \n",
    "                                                    complete=True, \n",
    "                                                    sum_value=False\n",
    "                                                )\n",
    " \n",
    "result = {\n",
    "    \"original\": data,\n",
    "    \"normalized\": weigths_normalized\n",
    "}\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = {\n",
    "    # 'No. of Likes on YouTube': 22,\n",
    "    # 'Creation Date': 31,\n",
    "    # 'No. of Views on YouTube': 22,\n",
    "    'Similarity Score': 23,\n",
    "    'No. of Saves on ABC': 30,\n",
    "    'User Ratings on ABC': 23\n",
    "}\n",
    "\n",
    "normalized_weights = normalize_factor_weights(factor_weights=article, method_type=\"l1\", complete=False, sum_value=True)\n",
    "print(\"normalized_weights: \", normalized_weights)\n",
    "# print(normalize_factor_weights(factor_weights=factor_weights, method_type=\"l1\", complete=True, sum_value=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
