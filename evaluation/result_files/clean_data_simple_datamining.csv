,index,name,uri,wikipedia,abstract,cid,type,related_to,createdAt,abstract_contents,article_contents,category,super_category,relrel_concepts,link_ratio,entropy
0,2,Indicator function,http://dbpedia.org/resource/Indicator_function,http://en.wikipedia.org/wiki/Indicator_function,"In mathematics, an indicator function or a characteristic function of a subset of a set is a function that maps elements of the subset to one, and all other elements to zero. That is, if A is a subset of some set X, one has if and otherwise, where is a common notation for the indicator function. Other common notations are and The indicator function of A is the Iverson bracket of the property of belonging to A; that is, For example, the Dirichlet function is the indicator function of the rational numbers as a subset of the real numbers.",4381514003829306916,related_concept,Conditional independence,2024-06-23 21:17:09.728,[],['Fuzzy set'],set(),set(),0,4.0,4.714138054045377e-304
1,6,Sigma-algebra,http://dbpedia.org/resource/Sigma-algebra,http://en.wikipedia.org/wiki/Sigma-algebra,,6160039003463786395,related_concept,Conditional independence,2024-06-23 21:17:09.728,[],['Theorem'],set(),set(),0,1.8596491228070176,0.35792250397270964
2,7,Conditional dependence,http://dbpedia.org/resource/Conditional_dependence,http://en.wikipedia.org/wiki/Conditional_dependence,"In probability theory, conditional dependence is a relationship between two or more events that are dependent when a third event occurs. For example, if and are two events that individually increase the probability of a third event and do not directly affect each other, then initially (when it has not been observed whether or not the event occurs) ( are independent). But suppose that now is observed to occur. If event occurs then the probability of occurrence of the event will decrease because its positive relation to is less necessary as an explanation for the occurrence of (similarly, event occurring will decrease the probability of occurrence of ). Hence, now the two events and are conditionally negatively dependent on each other because the probability of occurrence of each is negatively dependent on whether the other occurs. We have Conditional dependence of A and B given C is the logical negation of conditional independence . In conditional independence two events (which may be dependent or not) become independent given the occurrence of a third event.",6152235663017743238,related_concept,Conditional independence,2024-06-23 21:17:09.728,['Conditional dependence'],['Conditional dependence'],set(),set(),0,1.8125,1.3355828453646175
3,8,Frequency probability,http://dbpedia.org/resource/Frequency_probability,http://en.wikipedia.org/wiki/Frequency_probability,,8538285637166552251,related_concept,Conditional independence,2024-06-23 21:17:09.728,[],"['Probability', 'Probability theory']",set(),set(),0,0.21635883905013192,0.3191559862362648
4,14,Graphoid,http://dbpedia.org/resource/Graphoid,http://en.wikipedia.org/wiki/Graphoid,"A graphoid is a set of statements of the form, ""X is irrelevant to Y given that we know Z"" where X, Y and Z are sets of variables. The notion of ""irrelevance"" and ""given that we know"" may obtain different interpretations, including probabilistic, relational and correlational, depending on the application. These interpretations share common properties that can be captured by paths in graphs (hence the name ""graphoid""). The theory of graphoids characterizes these properties in a finite set of axioms that are common to informational irrelevance and its graphical representations.",280177841342901575,related_concept,Conditional independence,2024-06-23 21:17:09.728,[],"['Axiom', 'Conditional independence', 'Bayesian network']",set(),set(),0,1.0,1.383598854891882
5,16,Event (probability theory),http://dbpedia.org/resource/Event_(probability_theory),http://en.wikipedia.org/wiki/Event_(probability_theory),"In probability theory, an event is a set of outcomes of an experiment (a subset of the sample space) to which a probability is assigned. A single outcome may be an element of many different events, and different events in an experiment are usually not equally likely, since they may include very different groups of outcomes. An event consisting of only a single outcome is called an elementary event or an atomic event; that is, it is a singleton set. An event is said to occur if contains the outcome of the experiment (or trial) (that is, if ). The probability (with respect to some probability measure) that an event occurs is the probability that contains the outcome of an experiment (that is, it is the probability that ). An event defines a complementary event, namely the complementary set (the event not occurring), and together these define a Bernoulli trial: did the event occur or not? Typically, when the sample space is finite, any subset of the sample space is an event (that is, all elements of the power set of the sample space are defined as events). However, this approach does not work well in cases where the sample space is uncountably infinite. So, when defining a probability space it is possible, and often necessary, to exclude certain subsets of the sample space from being events (see , below).",6931716051753240135,related_concept,Conditional independence,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.325,4.551171943306599e-304
6,18,De Finetti's theorem,http://dbpedia.org/resource/De_Finetti's_theorem,http://en.wikipedia.org/wiki/De_Finetti's_theorem,"In probability theory, de Finetti's theorem states that exchangeable observations are conditionally independent relative to some latent variable. An epistemic probability distribution could then be assigned to this variable. It is named in honor of Bruno de Finetti. For the special case of an exchangeable sequence of Bernoulli random variables it states that such a sequence is a ""mixture"" of sequences of independent and identically distributed (i.i.d.) Bernoulli random variables. A sequence of random variables is called exchangeable if the joint distribution of the sequence is unchanged by any permutation of the indices. While the variables of the exchangeable sequence are not themselves independent, only exchangeable, there is an underlying family of i.i.d. random variables. That is, there are underlying, generally unobservable, quantities that are i.i.d. – exchangeable sequences are mixtures of i.i.d. sequences.",498605413459542127,related_concept,Conditional independence,2024-06-23 21:17:09.728,[],"[""De Finetti's theorem"", 'Bernoulli distribution']",set(),set(),0,1.0444444444444445,1.1108713218671875
7,21,Sufficient dimension reduction,http://dbpedia.org/resource/Sufficient_dimension_reduction,http://en.wikipedia.org/wiki/Sufficient_dimension_reduction,"In statistics, sufficient dimension reduction (SDR) is a paradigm for analyzing data that combines the ideas of dimension reduction with the concept of sufficiency. Dimension reduction has long been a primary goal of regression analysis. Given a response variable y and a p-dimensional predictor vector , regression analysis aims to study the distribution of , the conditional distribution of given . A dimension reduction is a function that maps to a subset of , k < p, thereby reducing the dimension of . For example, may be one or more linear combinations of . A dimension reduction is said to be sufficient if the distribution of is the same as that of . In other words, no information about the regression is lost in reducing the dimension of if the reduction is sufficient.",2115773200628473917,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,['Dimension'],['Dimension'],set(),set(),0,0.725,0.9826105559940778
8,23,Maximally informative dimensions,http://dbpedia.org/resource/Maximally_informative_dimensions,http://en.wikipedia.org/wiki/Maximally_informative_dimensions,"Maximally informative dimensions is a dimensionality reduction technique used in the statistical analyses of neural responses. Specifically, it is a way of projecting a stimulus onto a low-dimensional subspace so that as much information as possible about the stimulus is preserved in the neural response. It is motivated by the fact that natural stimuli are typically confined by their statistics to a lower-dimensional space than that spanned by white noise but correctly identifying this subspace using traditional techniques is complicated by the correlations that exist within natural images. Within this subspace, stimulus-response functions may be either linear or nonlinear. The idea was originally developed by Tatyana Sharpee, Nicole C. Rust, and William Bialek in 2003.",7278186280523159593,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,['Maximally informative dimensions'],"['Maximally informative dimensions', ""Bayes' theorem""]",set(),set(),0,0.3333333333333333,0.9304377597520492
9,24,Multidimensional scaling,http://dbpedia.org/resource/Multidimensional_scaling,http://en.wikipedia.org/wiki/Multidimensional_scaling,"Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. MDS is used to translate ""information about the pairwise 'distances' among a set of objects or individuals"" into a configuration of points mapped into an abstract Cartesian space. More technically, MDS refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix. It is a form of non-linear dimensionality reduction. Given a distance matrix with the distances between each pair of objects in a set, and a chosen number of dimensions, N, an MDS algorithm places each object into N-dimensional space (a lower-dimensional representation) such that the between-object distances are preserved as well as possible. For N = 1, 2, and 3, the resulting points can be visualized on a scatter plot. Core theoretical contributions to MDS were made by James O. Ramsay of McGill University, who is also regarded as the founder of functional data analysis.",8529093133326969461,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,['Multidimensional scaling'],"['Multidimensional scaling', 'Euclidean distance']",set(),set(),0,3.3698630136986303,1.2953811231785448
10,26,Data transformation (statistics),http://dbpedia.org/resource/Data_transformation_(statistics),http://en.wikipedia.org/wiki/Data_transformation_(statistics),"In statistics, data transformation is the application of a deterministic mathematical function to each point in a data set—that is, each data point zi is replaced with the transformed value yi = f(zi), where f is a function. Transforms are usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that is to be applied, or to improve the interpretability or appearance of graphs. Nearly always, the function that is used to transform the data is invertible, and generally is continuous. The transformation is usually applied to a collection of comparable measurements. For example, if we are working with data on peoples' incomes in some currency unit, it would be common to transform each person's income value by the logarithm function.",7446337797626734528,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,[],"['Data', 'Generalized linear models', 'Fisher transformation']",set(),set(),0,0.7766990291262136,1.3656593170523605
11,28,Feature extraction,http://dbpedia.org/resource/Feature_extraction,http://en.wikipedia.org/wiki/Feature_extraction,"In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction. When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data.",6787355161359361537,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,['Feature extraction'],"['Classification', 'Feature selection', 'Machine learning']",set(),set(),0,1.242603550295858,1.3938442348018096
12,34,Random projection,http://dbpedia.org/resource/Random_projection,http://en.wikipedia.org/wiki/Random_projection,"In mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are known for their power, simplicity, and low error rates when compared to other methods. According to experimental results, random projection preserves distances well, but empirical results are sparse. They have been applied to many natural language tasks under the name random indexing.",3884200958028964701,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,['Random projection'],"['Dimension', 'Dimensionality reduction', 'Random projection']",set(),set(),0,0.8292682926829268,1.3861062783656533
13,36,Multifactor dimensionality reduction,http://dbpedia.org/resource/Multifactor_dimensionality_reduction,http://en.wikipedia.org/wiki/Multifactor_dimensionality_reduction,"Multifactor dimensionality reduction (MDR) is a statistical approach, also used in machine learning automatic approaches, for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify nonadditive interactions among discrete variables that influence a binary outcome and is considered a nonparametric and model-free alternative to traditional statistical methods such as logistic regression. The basis of the MDR method is a constructive induction or feature engineering algorithm that converts two or more variables or attributes to a single attribute. This process of constructing a new attribute changes the representation space of the data. The end goal is to create or discover a representation that facilitates the detection of nonlinear or nonadditive interactions among the attributes such that prediction of the class variable is improved over that of the original representation of the data.",3479555230011076775,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,['Multifactor dimensionality reduction'],"['Multifactor dimensionality reduction', 'Bayes classifier', 'Decision tree']",set(),set(),0,1.2115384615384615,1.3988092572397373
14,38,Diffusion map,http://dbpedia.org/resource/Diffusion_map,http://en.wikipedia.org/wiki/Diffusion_map,"Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by Coifman and Lafon which computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the ""diffusion distance"" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps give a global description of the data-set. Compared with other methods, the diffusion map algorithm is robust to noise perturbation and computationally inexpensive.",1710433260055931238,related_concept,Dimensionality reduction,2024-06-23 21:17:09.728,"['Diffusion map', 'Euclidean distance']","['Diffusion map', 'Euclidean distance', 'Laplacian matrix']",set(),set(),0,2.48,0.8419058228884485
15,45,BrownBoost,http://dbpedia.org/resource/BrownBoost,http://en.wikipedia.org/wiki/BrownBoost,"BrownBoost is a boosting algorithm that may be robust to noisy datasets. BrownBoost is an adaptive version of the algorithm. As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. BrownBoost was introduced by Yoav Freund in 2001.",5073433692347092788,related_concept,AdaBoost,2024-06-23 21:17:09.728,['BrownBoost'],"['BrownBoost', 'AdaBoost', ""Newton's method""]",set(),set(),0,4.0,4.743345309768311e-304
16,48,LPBoost,http://dbpedia.org/resource/LPBoost,http://en.wikipedia.org/wiki/LPBoost,"Linear Programming Boosting (LPBoost) is a supervised classifier from the boosting family of classifiers. LPBoost maximizes a margin between training samples of different classes and hence also belongs to the class of margin-maximizing supervised classification algorithms. Consider a classification function which classifies samples from a space into one of two classes, labelled 1 and -1, respectively. LPBoost is an algorithm to learn such a classification function given a set of training examples with known class labels. LPBoost is a machine learning technique and especially suited for applications of joint classification and feature selection in structured domains.",1142379263170350381,related_concept,AdaBoost,2024-06-23 21:17:09.728,['LPBoost'],"['LPBoost', 'Iterative', 'AdaBoost']",set(),set(),0,3.6666666666666665,4.826787849257205e-304
17,51,Loss functions for classification,http://dbpedia.org/resource/Loss_functions_for_classification,http://en.wikipedia.org/wiki/Loss_functions_for_classification,"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). Given as the space of all possible inputs (usually ), and as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function which best predicts a label for a given input . However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same to generate different . As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as where is a given loss function, and is the probability density function of the process that generated the data, which can equivalently be written as Within classification, several commonly used loss functions are written solely in terms of the product of the true label and the predicted label . Therefore, they can be defined as functions of only one variable , so that with a suitably chosen function . These are called margin-based loss functions. Choosing a margin-based loss function amounts to choosing . Selection of a loss function within this framework impacts the optimal which minimizes the expected risk. In the case of binary classification, it is possible to simplify the calculation of expected risk from the integral specified above. Specifically, The second equality follows from the properties described above. The third equality follows from the fact that 1 and −1 are the only possible values for , and the fourth because . The term within brackets is known as the conditional risk. One can solve for the minimizer of by taking the functional derivative of the last equality with respect to and setting the derivative equal to 0. This will result in the following equation which is also equivalent to setting the derivative of the conditional risk equal to zero. Given the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0-1 loss function (0–1 indicator function), which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by where indicates the Heaviside step function.However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem. As a result, it is better to substitute loss function surrogates which are tractable for commonly used learning algorithms, as they have convenient properties such as being convex and smooth. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allow for the recovery of the actual solution to the original classification problem. Some of these surrogates are described below. In practice, the probability distribution is unknown. Consequently, utilizing a training set of independently and identically distributed sample points drawn from the data sample space, one seeks to minimize empirical risk as a proxy for expected risk. (See statistical learning theory for a more detailed description.)",5451176408267040688,related_concept,AdaBoost,2024-06-23 21:17:09.728,[],"[""Bayes' theorem"", 'Kullback–Leibler divergence', 'AdaBoost']",set(),set(),0,0.8080808080808081,1.3856877315917426
18,53,CoBoosting,http://dbpedia.org/resource/CoBoosting,http://en.wikipedia.org/wiki/CoBoosting,"CoBoost is a semi-supervised training algorithm proposed by Collins and Singer in 1999. The original application for the algorithm was the task of Named Entity Classification using very weak learners. It can be used for performing semi-supervised learning in cases in which there exist redundancy in features. It may be seen as a combination of co-training and boosting. Each example is available in two views (subsections of the feature set), and boosting is applied iteratively in alternation with each view using predicted labels produced in the alternate view on the previous iteration. CoBoosting is not a valid boosting algorithm in the PAC learning sense.",5518329965023410091,related_concept,AdaBoost,2024-06-23 21:17:09.728,"['CoBoosting', 'Classification']","['Classification', 'CoBoosting', 'AdaBoost']",set(),set(),0,8.2,0.670769862867764
19,57,Decision stump,http://dbpedia.org/resource/Decision_stump,http://en.wikipedia.org/wiki/Decision_stump,"A decision stump is a machine learning model consisting of a one-level decision tree. That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules. Depending on the type of the input feature, several variations are possible. For nominal features, one may build a stump which contains a leaf for each possible feature value or a stump with the two leaves, one of which corresponds to some chosen category, and the other leaf to all the other categories. For binary features these two schemes are identical. A missing value may be treated as a yet another category. For continuous features, usually, some threshold feature value is selected, and the stump contains two leaves — for values below and above the threshold. However, rarely, multiple thresholds may be chosen and the stump therefore contains three or more leaves. Decision stumps are often used as components (called ""weak learners"" or ""base learners"") in machine learning ensemble techniques such as bagging and boosting. For example, a Viola–Jones face detection algorithm employs AdaBoost with decision stumps as weak learners. The term ""decision stump"" was coined in a 1992 ICML paper by Wayne Iba and Pat Langley.",5281654181857615600,related_concept,AdaBoost,2024-06-23 21:17:09.728,"['Decision stump', 'AdaBoost']","['Decision stump', 'AdaBoost']",set(),set(),0,3.235294117647059,1.2549044635256292
20,59,Numerical instability,http://dbpedia.org/resource/Numerical_instability,http://en.wikipedia.org/wiki/Numerical_instability,,2280579012603423332,related_concept,AdaBoost,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.09009009009009009,0.29078121941256097
21,67,Nonlinear regression,http://dbpedia.org/resource/Nonlinear_regression,http://en.wikipedia.org/wiki/Nonlinear_regression,"In statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.",4555765725814206431,related_concept,Statistics,2024-06-23 21:17:09.728,[],"['Systematic error', 'Linearization']",set(),set(),0,1.71990171990172,0.9741371931951536
22,71,History of statistics,http://dbpedia.org/resource/History_of_statistics,http://en.wikipedia.org/wiki/History_of_statistics,"Statistics, in the modern sense of the word, began evolving in the 18th century in response to the novel needs of industrializing sovereign states. In early times, the meaning was restricted to information about states, particularly demographics such as population. This was later extended to include all collections of information of all types, and later still it was extended to include the analysis and interpretation of such data. In modern terms, ""statistics"" means both sets of collected information, as in national accounts and temperature record, and analytical work which requires statistical inference. Statistical activities are often associated with models expressed using probabilities, hence the connection with probability theory. The large requirements of data processing have made statistics a key application of computing. A number of statistical concepts have an important impact on a wide range of sciences. These include the design of experiments and approaches to statistical inference such as Bayesian inference, each of which can be considered to have their own sequence in the development of the ideas underlying modern statistics.",2721593523366773273,related_concept,Statistics,2024-06-23 21:17:09.728,"['Bayesian inference', 'Statistics']","['Statistics', 'Bayesian inference', 'Probability', 'Boot', 'P-value', ""Pearson's chi-squared test"", 'The Genetical Theory of Natural Selection', 'Statistical Methods for Research Workers', 'The Design of Experiments', 'Fisher information', ""Student's t-distribution"", 'Pearson correlation coefficient', 'Inference', ""Bayes' theorem"", 'Bayesianism', 'Mathematics']",set(),set(),0,0.3206751054852321,1.3571005111744148
23,75,Mathematical statistics,http://dbpedia.org/resource/Mathematical_statistics,http://en.wikipedia.org/wiki/Mathematical_statistics,"Mathematical statistics is the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data. Specific mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure theory.",6729812652307379439,related_concept,Statistics,2024-06-23 21:17:09.728,['Mathematical statistics'],"['Mathematical statistics', 'Data', 'Data analysis', 'Statistical inference', 'Nonparametric regression', 'Nonparametric statistics', 'Neyman–Pearson lemma', 'Likelihood-ratio test']",set(),set(),0,1.5605749486652978,1.3720132128112152
24,78,Glossary of probability and statistics,http://dbpedia.org/resource/Glossary_of_probability_and_statistics,http://en.wikipedia.org/wiki/Glossary_of_probability_and_statistics,"This glossary of statistics and probability is a list of definitions of terms and concepts used in the mathematical sciences of statistics and probability, their sub-disciplines, and related fields. For additional related terms, see Glossary of mathematics and Glossary of experimental design.",8759969240081616013,related_concept,Statistics,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.3747534516765286,1.2827417720550123
25,85,Abstract data type,http://dbpedia.org/resource/Abstract_data_type,http://en.wikipedia.org/wiki/Abstract_data_type,"In computer science, an abstract data type (ADT) is a mathematical model for data types. An abstract data type is defined by its behavior (semantics) from the point of view of a user, of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This mathematical model contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user. Formally, an ADT may be defined as a ""class of objects whose logical behavior is defined by a set of values and a set of operations""; this is analogous to an algebraic structure in mathematics. What is meant by ""behaviour"" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model; these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity (""cost""), both in terms of time (for computing operations) and space (for representing values). In practice, many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed-width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded. ADTs are a theoretical concept, in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.",8806866366427754406,related_concept,Data type,2024-06-23 21:17:09.728,[],"['Algebraic specification', 'Abstract data type']",set(),set(),0,1.7725321888412018,4.073507240976776e-304
26,86,Type constructor,http://dbpedia.org/resource/Type_constructor,http://en.wikipedia.org/wiki/Type_constructor,"In the area of mathematical logic and computer science known as type theory, a type constructor is a feature of a typed formal language that builds new types from old ones. Basic types are considered to be built using nullary type constructors. Some type constructors take another type as an argument, e.g., the constructors for product types, function types, power types and list types. New types can be defined by recursively composing type constructors. For example, simply typed lambda calculus can be seen as a language with a single non-basic type constructor—the function type constructor. Product types can generally be considered ""built-in"" in typed lambda calculi via currying. Abstractly, a type constructor is an n-ary type operator taking as argument zero or more types, and returning another type. Making use of currying, n-ary type operators can be (re)written as a sequence of applications of unary type operators. Therefore, we can view the type operators as a simply typed lambda calculus, which has only one basic type, usually denoted , and pronounced ""type"", which is the type of all types in the underlying language, which are now called proper types in order to distinguish them from the types of the type operators in their own calculus, which are called kinds. Type operators may bind type variables. For example, giving the structure of the simply-typed λ-calculus at the type level requires binding, or higher-order, type operators. These binding type operators correspond to the 2nd axis of the λ-cube, and type theories such as the simply-typed λ-calculus with type operators, λω. Combining type operators with the polymorphic λ-calculus (System F) yields System Fω.",8212473286971933236,related_concept,Data type,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2718446601941749,4.023118108950873e-304
27,87,Set (abstract data type),http://dbpedia.org/resource/Set_(abstract_data_type),http://en.wikipedia.org/wiki/Set_(abstract_data_type),"In computer science, a set is an abstract data type that can store unique values, without any particular order. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set. Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements — such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets, allow also the insertion and deletion of elements from the set. A multiset is a special kind of set in which an element can appear multiple times in the set.",621640098167630409,related_concept,Data type,2024-06-23 21:17:09.728,[],['SQL'],set(),set(),0,1.5115207373271888,1.3767976628402203
28,88,Enumerated type,http://dbpedia.org/resource/Enumerated_type,http://en.wikipedia.org/wiki/Enumerated_type,"In computer programming, an enumerated type (also called enumeration, enum, or factor in the R programming language, and a categorical variable in statistics) is a data type consisting of a set of named values called elements, members, enumeral, or enumerators of the type. The enumerator names are usually identifiers that behave as constants in the language. An enumerated type can be seen as a degenerate tagged union of unit type. A variable that has been declared as having an enumerated type can be assigned any of the enumerators as a value. In other words, an enumerated type has values that are different from each other, and that can be compared and assigned, but are not specified by the programmer as having any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named Club, Diamond, Heart, and Spade, belonging to an enumerated type named suit. If a variable V is declared having suit as its data type, one can assign any of those four values to it. Although the enumerators are usually distinct, some languages may allow the same enumerator to be listed twice in the type's declaration. The names of enumerators need not be semantically complete or compatible in any sense. For example, an enumerated type called color may be defined to consist of the enumerators Red, Green, Zebra, Missing, and Bacon. In some languages, the declaration of an enumerated type also intentionally defines an ordering of its members; in others, the enumerators are unordered; in others still, an implicit ordering arises from the compiler concretely representing enumerators as integers. Some enumerator types may be built into the language. The Boolean type, for example is often a pre-defined enumeration of the values False and True. Many languages allow users to define new enumerated types. Values and variables of an enumerated type are usually implemented with some integer type as the underlying representation. Some languages, especially system programming languages, allow the user to specify the bit combination to be used for each enumerator, which can be useful to efficiently represent sets of enumerators as fixed-length bit strings. In type theory, enumerated types are often regarded as tagged unions of unit types. Since such types are of the form , they may also be written as natural numbers.",4324545345911582817,related_concept,Data type,2024-06-23 21:17:09.728,[],"['Enumerated type', 'SQL']",set(),set(),0,1.6524390243902438,4.0479177149519345e-304
29,89,Variable (computer science),http://dbpedia.org/resource/Variable_(computer_science),http://en.wikipedia.org/wiki/Variable_(computer_science),"In computer programming, a variable is an abstract storage location paired with an associated symbolic name, which contains some known or unknown quantity of information referred to as a value; or in simpler terms, a variable is a named container for a particular set of bits or type of data (like integer, float, string etc...). A variable can eventually be associated with or identified by a memory address. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution. Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation. A variable's storage location may be referenced by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers. Compilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.",3288807209449611244,related_concept,Data type,2024-06-23 21:17:09.728,['Compiler'],['Compiler'],set(),set(),0,2.1373626373626373,4.083055283473802e-304
30,91,Algebraic specification,http://dbpedia.org/resource/Algebraic_specification,http://en.wikipedia.org/wiki/Algebraic_specification,Algebraic specification is a software engineering technique for formally specifying system behavior. It was a very active subject of computer science research around 1980.,803121979920687694,related_concept,Data type,2024-06-23 21:17:09.728,['Algebraic specification'],['Algebraic specification'],set(),set(),0,0.92,0.9275364072670641
31,92,Kind (type theory),http://dbpedia.org/resource/Kind_(type_theory),http://en.wikipedia.org/wiki/Kind_(type_theory),"In the area of mathematical logic and computer science known as type theory, a kind is the type of a type constructor or, less commonly, the type of a higher-order type operator. A kind system is essentially a simply typed lambda calculus ""one level up"", endowed with a primitive type, denoted and called ""type"", which is the kind of any data type which does not need any type parameters. A kind is sometimes confusingly described as the ""type of a (data) type"", but it is actually more of an arity specifier. Syntactically, it is natural to consider polymorphic types to be type constructors, thus non-polymorphic types to be nullary type constructors. But all nullary constructors, thus all monomorphic types, have the same, simplest kind; namely . Since higher-order type operators are uncommon in programming languages, in most programming practice, kinds are used to distinguish between data types and the types of constructors which are used to implement parametric polymorphism. Kinds appear, either explicitly or implicitly, in languages whose type systems account for parametric polymorphism in a programatically accessible way, such as C++, Haskell and Scala.",2821129628020558155,related_concept,Data type,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1559633027522935,4.0663902806843044e-304
32,93,List (abstract data type),http://dbpedia.org/resource/List_(abstract_data_type),http://en.wikipedia.org/wiki/List_(abstract_data_type),"In computer science, a list or sequence is an abstract data type that represents a finite number of ordered values, where the same value may occur more than once. An instance of a list is a computer representation of the mathematical concept of a tuple or finite sequence; the (potentially) infinite analog of a list is a stream. Lists are a basic example of containers, as they contain other values. If the same value occurs multiple times, each occurrence is considered a distinct item. The name list is also used for several concrete data structures that can be used to implement abstract lists, especially linked lists and arrays. In some contexts, such as in Lisp programming, the term list may refer specifically to a linked list rather than an array. In class-based programming, lists are usually provided as instances of subclasses of a generic ""list"" class, and traversed via separate iterators. Many programming languages provide support for list data types, and have special syntax and semantics for lists and list operations. A list can often be constructed by writing the items in sequence, separated by commas, semicolons, and/or spaces, within a pair of delimiters such as parentheses '', brackets '[]', braces '{}', or angle brackets '<>'. Some languages may allow list types to be indexed or sliced like array types, in which case the data type is more accurately described as an array. In type theory and functional programming, abstract lists are usually defined inductively by two operations: nil that yields the empty list, and cons, which adds an item at the beginning of a list.",1663709645645846391,related_concept,Data type,2024-06-23 21:17:09.728,[],['Sorting'],set(),set(),0,2.114130434782609,0.9629124835842132
33,94,C data types,http://dbpedia.org/resource/C_data_types,http://en.wikipedia.org/wiki/C_data_types,"In the C programming language, data types constitute the semantics and characteristics of storage of data elements. They are expressed in the language syntax in form of declarations for memory locations or variables. Data types also determine the types of operations or methods of processing of data elements. The C language provides basic arithmetic types, such as integer and real number types, and syntax to build array and compound types. Headers for the C standard library, to be used via include directives, contain definitions of support types, that have additional properties, such as providing storage with an exact size, independent of the language implementation on specific hardware platforms.",930538601884350137,related_concept,Data type,2024-06-23 21:17:09.728,"['Data', 'Data type']","['Data', 'Data type']",set(),set(),0,1.0628019323671498,4.084746613546435e-304
34,95,Primitive data type,http://dbpedia.org/resource/Primitive_data_type,http://en.wikipedia.org/wiki/Primitive_data_type,"In computer science, primitive data types are a set of basic data types from which all other data types are constructed. Specifically it often refers to the limited set of data representations in use by a particular processor, which all compiled programs must use. Most processors support a similar set of primitive data types, although the specific representations vary. More generally, ""primitive data types"" may refer to the standard data types built into a programming language. Data types which are not primitive are referred to as derived or composite. Primitive types are almost always value types, but composite types may also be value types.",7994294780069842130,related_concept,Data type,2024-06-23 21:17:09.728,"['Data', 'Data type']","['Data', 'Data type', 'Primitive data type', 'C data types']",set(),set(),0,1.25,4.052216637419893e-304
35,96,Statistical data type,http://dbpedia.org/resource/Statistical_data_type,http://en.wikipedia.org/wiki/Statistical_data_type,"In statistics, groups of individual data points may be classified as belonging to any of various statistical data types, e.g. categorical (""red"", ""blue"", ""green""), real number (1.68, -5, 1.7e+6), odd number (1,3,5) etc. The data type is a fundamental component of the semantic content of the variable, and controls which sorts of probability distributions can logically be used to describe the variable, the permissible operations on the variable, the type of regression analysis used to predict the variable, etc. The concept of data type is similar to the concept of level of measurement, but more specific: For example, count data require a different distribution (e.g. a Poisson distribution or binomial distribution) than non-negative real-valued data require, but both fall under the same level of measurement (a ratio scale). Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in degree Celsius or degree Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation. Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented. Other categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991). The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. ""The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer"" (Hand, 2004, p. 82).",7193727435685720256,related_concept,Data type,2024-06-23 21:17:09.728,"['Boolean data type', 'Poisson distribution']","['Poisson distribution', 'Boolean data type', 'Data']",set(),set(),0,0.16981132075471697,1.3972955097237119
36,97,Algebraic data type,http://dbpedia.org/resource/Algebraic_data_type,http://en.wikipedia.org/wiki/Algebraic_data_type,"In computer programming, especially functional programming and type theory, an algebraic data type (ADT) is a kind of composite type, i.e., a type formed by combining other types. Two common classes of algebraic types are product types (i.e., tuples and records) and sum types (i.e., tagged or disjoint unions, coproduct types or variant types). The values of a product type typically contain several values, called fields. All values of that type have the same combination of field types. The set of all possible values of a product type is the set-theoretic product, i.e., the Cartesian product, of the sets of all possible values of its field types. The values of a sum type are typically grouped into several classes, called variants. A value of a variant type is usually created with a quasi-functional entity called a constructor. Each variant has its own constructor, which takes a specified number of arguments with specified types. The set of all possible values of a sum type is the set-theoretic sum, i.e., the disjoint union, of the sets of all possible values of its variants. Enumerated types are a special case of sum types in which the constructors take no arguments, as exactly one value is defined for each constructor. Values of algebraic types are analyzed with pattern matching, which identifies a value by its constructor or field names and extracts the data it contains. Algebraic data types were introduced in Hope, a small functional programming language developed in the 1970s at the University of Edinburgh.",8068572642107858891,related_concept,Data type,2024-06-23 21:17:09.728,"['Enumerated type', 'Algebraic data type']","['Enumerated type', 'Algebraic data type']",set(),set(),0,1.2857142857142858,4.027312017242611e-304
37,98,Integer (computer science),http://dbpedia.org/resource/Integer_(computer_science),http://en.wikipedia.org/wiki/Integer_(computer_science),"In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware nearly always provides a way to represent a processor register or memory address as an integer.",762293474290467919,related_concept,Data type,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.269035532994924,0.8371172951807814
38,99,Boolean data type,http://dbpedia.org/resource/Boolean_data_type,http://en.wikipedia.org/wiki/Boolean_data_type,"In computer science, the Boolean (sometimes shortened to Bool) is a data type that has one of two possible values (usually denoted true and false) which is intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid 19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type—logic does not always need to be Boolean (see probabilistic logic).",4398918107039718122,related_concept,Data type,2024-06-23 21:17:09.728,['Boolean data type'],"['Boolean data type', 'SQL', 'Database', 'Data']",set(),set(),0,2.0491803278688523,1.4184148241749752
39,100,String (computer science),http://dbpedia.org/resource/String_(computer_science),http://en.wikipedia.org/wiki/String_(computer_science),"In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures. Depending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements. When a string appears literally in source code, it is known as a string literal or an anonymous string. In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.",4244637392058222696,related_concept,Data type,2024-06-23 21:17:09.728,[],"['Prolog', 'SQL']",set(),set(),0,2.8694516971279374,4.074954411309089e-304
40,101,Type system,http://dbpedia.org/resource/Type_system,http://en.wikipedia.org/wiki/Type_system,"In computer programming, a type system is a logical system comprising a set of rules that assigns a property called a type to every ""term"" (a word, phrase, or other set of symbols). Usually the terms are various constructs of a computer program, such as variables, expressions, functions, or modules. A type system dictates the operations that can be performed on a term. For variables, the type system determines the allowed values of that term. Type systems formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. ""string"", ""array of float"", ""function returning boolean""). Type systems are often specified as part of programming languages and built into interpreters and compilers, although the type system of a language can be extended by optional tools that perform added checks using the language's original type syntax and grammar. The main purpose of a type system in a programming language is to reduce possibilities for bugs in computer programs due to type errors. The given type system in question determines what constitutes a type error, but in general, the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (validity errors). Type systems allow defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of both. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, and providing a form of documentation.",2749931400430223675,related_concept,Data type,2024-06-23 21:17:09.728,['Type system'],['Type system'],set(),set(),0,3.5033557046979866,4.05959588402068e-304
41,102,Array data type,http://dbpedia.org/resource/Array_data_type,http://en.wikipedia.org/wiki/Array_data_type,,2075546538203531820,related_concept,Data type,2024-06-23 21:17:09.728,[],"['Array data type', 'Compiler']",set(),set(),0,1.6358695652173914,0.24015002789397485
42,103,Data structure,http://dbpedia.org/resource/Data_structure,http://en.wikipedia.org/wiki/Data_structure,"In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.",8632528596218553102,related_concept,Data type,2024-06-23 21:17:09.728,[],"['Data structure', 'Data', 'Trie']",set(),set(),0,4.761744966442953,1.1977848555507105
43,105,Hermann Minkowski,http://dbpedia.org/resource/Hermann_Minkowski,http://en.wikipedia.org/wiki/Hermann_Minkowski,"Hermann Minkowski (/mɪŋˈkɔːfski, -ˈkɒf-/; German: [mɪŋˈkɔfski]; 22 June 1864 – 12 January 1909) was a German mathematician and professor at Königsberg, Zürich and Göttingen. He created and developed the geometry of numbers and used geometrical methods to solve problems in number theory, mathematical physics, and the theory of relativity. Minkowski is perhaps best known for his foundational work describing space and time as a four-dimensional space, now known as ""Minkowski spacetime"", which facilitated geometric interpretations of Albert Einstein's special theory of relativity (1905).",1166677547754100824,related_concept,Minkowski distance,2024-06-23 21:17:09.728,['Hermann Minkowski'],"['Hermann Minkowski', 'Mathematics']",set(),set(),0,2.372340425531915,3.28066433349901e-304
44,107,Chebyshev distance,http://dbpedia.org/resource/Chebyshev_distance,http://en.wikipedia.org/wiki/Chebyshev_distance,"In mathematics, Chebyshev distance (or Tchebychev distance), maximum metric, or L∞ metric is a metric defined on a vector space where the distance between two vectors is the greatest of their differences along any coordinate dimension. It is named after Pafnuty Chebyshev. It is also known as chessboard distance, since in the game of chess the minimum number of moves needed by a king to go from one square on a chessboard to another equals the Chebyshev distance between the centers of the squares, if the squares have side length one, as represented in 2-D spatial coordinates with axes aligned to the edges of the board. For example, the Chebyshev distance between f6 and e2 equals 4.",5322347934240223072,related_concept,Minkowski distance,2024-06-23 21:17:09.728,['Chebyshev distance'],"['Chebyshev distance', 'Manhattan distance', 'Minkowski distance']",set(),set(),0,1.2277227722772277,1.2103093840876267
45,108,F-space,http://dbpedia.org/resource/F-space,http://en.wikipedia.org/wiki/F-space,"In functional analysis, an F-space is a vector space over the real or complex numbers together with a metric such that 1. 
* Scalar multiplication in is continuous with respect to and the standard metric on or 2. 
* Addition in is continuous with respect to 3. 
* The metric is translation-invariant; that is, for all 4. 
* The metric space is complete. The operation is called an F-norm, although in general an F-norm is not required to be homogeneous. By translation-invariance, the metric is recoverable from the F-norm. Thus, a real or complex F-space is equivalently a real or complex vector space equipped with a complete F-norm. Some authors use the term Fréchet space rather than F-space, but usually the term ""Fréchet space"" is reserved for locally convex F-spaces. Some other authors use the term ""F-space"" as a synonym of ""Fréchet space"", by which they mean a locally convex complete metrizable topological vector space. The metric may or may not necessarily be part of the structure on an F-space; many authors only require that such a space be metrizable in a manner that satisfies the above properties.",25922642266844647,related_concept,Minkowski distance,2024-06-23 21:17:09.728,['F-space'],"['F-space', 'Theorem']",set(),set(),0,1.0319148936170213,1.2155866252286294
46,109,Minkowski inequality,http://dbpedia.org/resource/Minkowski_inequality,http://en.wikipedia.org/wiki/Minkowski_inequality,"In mathematical analysis, the Minkowski inequality establishes that the Lp spaces are normed vector spaces. Let S be a measure space, let 1 ≤ p < ∞ and let f and g be elements of Lp(S). Then f + g is in Lp(S), and we have the triangle inequality with equality for 1 < p < ∞ if and only if f and g are positively linearly dependent, i.e., f = λg for some λ ≥ 0 or g = 0. Here, the norm is given by: if p < ∞, or in the case p = ∞ by the essential supremum The Minkowski inequality is the triangle inequality in Lp(S). In fact, it is a special case of the more general fact where it is easy to see that the right-hand side satisfies the triangular inequality. Like Hölder's inequality, the Minkowski inequality can be specialized to sequences and vectors by using the counting measure: for all real (or complex) numbers x1, ..., xn, y1, ..., yn and where n is the cardinality of S (the number of elements in S). The inequality is named after the German mathematician Hermann Minkowski.",7632435922231073784,related_concept,Minkowski distance,2024-06-23 21:17:09.728,"['Hermann Minkowski', 'Minkowski inequality']","['Minkowski inequality', 'Hermann Minkowski']",set(),set(),0,0.9769585253456221,0.8177527943262685
47,110,Normed vector space,http://dbpedia.org/resource/Normed_vector_space,http://en.wikipedia.org/wiki/Normed_vector_space,"In mathematics, a normed vector space or normed space is a vector space over the real or complex numbers, on which a norm is defined. A norm is the formalization and the generalization to real vector spaces of the intuitive notion of ""length"" in the real (physical) world. A norm is a real-valued function defined on the vector space that is commonly denoted and has the following properties: 1. 
* It is nonnegative, meaning that for every vector 2. 
* It is positive on nonzero vectors, that is, 3. 
* For every vector and every scalar 4. 
* The triangle inequality holds; that is, for every vectors and A norm induces a distance, called its (norm) induced metric, by the formula which makes any normed vector space into a metric space and a topological vector space. If this metric is complete then the normed space is a Banach space. Every normed vector space can be ""uniquely extended"" to a Banach space, which makes normed spaces intimately related to Banach spaces. Every Banach space is a normed space but converse is not true. For example, the set of the finite sequences of real numbers can be normed with the Euclidean norm, but it is not complete for this norm. An inner product space is a normed vector space whose norm is the square root of the inner product of a vector and itself. The Euclidean norm of a Euclidean vector space is a special case that allows defining Euclidean distance by the formula The study of normed spaces and Banach spaces is a fundamental part of functional analysis, which is a major subfield of mathematics.",7431379244732902399,related_concept,Minkowski distance,2024-06-23 21:17:09.728,['Euclidean distance'],"['Euclidean distance', 'Hahn–Banach theorem']",set(),set(),0,0.8363095238095238,1.2387031024613306
48,112,Manhattan distance,http://dbpedia.org/resource/Manhattan_distance,http://en.wikipedia.org/wiki/Manhattan_distance,,510981640105002928,related_concept,Minkowski distance,2024-06-23 21:17:09.728,[],"['Manhattan distance', 'Euclidean distance', 'Hermann Minkowski', 'Euclidean geometry', 'Minkowski inequality', 'Geometry', 'Chebyshev distance']",set(),set(),0,0.5620915032679739,0.31088510367057565
49,113,Triangle inequality,http://dbpedia.org/resource/Triangle_inequality,http://en.wikipedia.org/wiki/Triangle_inequality,"In mathematics, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side. This statement permits the inclusion of degenerate triangles, but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality. If x, y, and z are the lengths of the sides of the triangle, with no side being greater than z, then the triangle inequality states that with equality only in the degenerate case of a triangle with zero area.In Euclidean geometry and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths (norms): where the length z of the third side has been replaced by the vector sum x + y. When x and y are real numbers, they can be viewed as vectors in R1, and the triangle inequality expresses a relationship between absolute values. In Euclidean geometry, for right triangles the triangle inequality is a consequence of the Pythagorean theorem, and for general triangles, a consequence of the law of cosines, although it may be proven without these theorems. The inequality can be viewed intuitively in either R2 or R3. The figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). In the Euclidean case, equality occurs only if the triangle has a 180° angle and two 0° angles, making the three vertices collinear, as shown in the bottom example. Thus, in Euclidean geometry, the shortest distance between two points is a straight line. In spherical geometry, the shortest distance between two points is an arc of a great circle, but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in [0, π]) with those endpoints. The triangle inequality is a defining property of norms and measures of distance. This property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the real numbers, Euclidean spaces, the Lp spaces (p ≥ 1), and inner product spaces.",4084454345520059413,related_concept,Minkowski distance,2024-06-23 21:17:09.728,['Euclidean geometry'],"['Euclidean geometry', 'Euclidean plane', 'Manhattan distance', 'Cauchy sequence']",set(),set(),0,4.397435897435898,0.7387408013812182
50,114,Minkowski metric,http://dbpedia.org/resource/Minkowski_metric,http://en.wikipedia.org/wiki/Minkowski_metric,,5024983298546954040,related_concept,Minkowski distance,2024-06-23 21:17:09.728,[],"['Hermann Minkowski', 'Euclidean distance', 'Minkowski metric', 'Euclidean geometry', 'Theorem', 'Heuristic']",set(),set(),0,0.4176470588235294,0.27465701289799843
51,115,Level set,http://dbpedia.org/resource/Level_set,http://en.wikipedia.org/wiki/Level_set,"In mathematics, a level set of a real-valued function f of n real variables is a set where the function takes on a given constant value c, that is: When the number of independent variables is two, a level set is called a level curve, also known as contour line or isoline; so a level curve is the set of all real-valued solutions of an equation in two variables x1 and x2. When n = 3, a level set is called a level surface (or isosurface); so a level surface is the set of all real-valued roots of an equation in three variables x1, x2 and x3. For higher values of n, the level set is a level hypersurface, the set of all real-valued roots of an equation in n > 3 variables. A level set is a special case of a fiber.",8252450699214265246,related_concept,Minkowski distance,2024-06-23 21:17:09.728,[],"['Level set', 'Euclidean distance']",set(),set(),0,3.3469387755102042,1.3740106512957215
52,116,Power mean,http://dbpedia.org/resource/Power_mean,http://en.wikipedia.org/wiki/Power_mean,,4455621146180886669,related_concept,Minkowski distance,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.37037037037037035,4.2088336424779847e-305
53,117,Minkowski distance,http://dbpedia.org/resource/Minkowski_distance,http://en.wikipedia.org/wiki/Minkowski_distance,The Minkowski distance or Minkowski metric is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance. It is named after the German mathematician Hermann Minkowski.,4123743794131967992,main_concept,,2024-06-23 21:17:09.728,"['Hermann Minkowski', 'Manhattan distance', 'Minkowski metric', 'Minkowski distance', 'Euclidean distance']","['Hermann Minkowski', 'Manhattan distance', 'Minkowski metric', 'Minkowski distance', 'Euclidean distance', 'Minkowski inequality', 'Chebyshev distance']",set(),set(),0,1.1395348837209303,1.057547219667496
54,119,Transposition table,http://dbpedia.org/resource/Transposition_table,http://en.wikipedia.org/wiki/Transposition_table,"A transposition table is a cache of previously seen positions, and associated evaluations, in a game tree generated by a computer game playing program. If a position recurs via a different sequence of moves, the value of the position is retrieved from the table, avoiding re-searching the game tree below that position. Transposition tables are primarily useful in perfect-information games (where the entire state of the game is known to all players at all times). The usage of transposition tables is essentially memoization applied to the tree search and is a form of dynamic programming. Transposition tables are typically implemented as hash tables encoding the current board position as the hash index. The number of possible positions that may occur in a game tree is an exponential function of depth of search, and can be thousands to millions or even much greater. Transposition tables may therefore consume most of available system memory and are usually most of the memory footprint of game playing programs.",1284834909701464603,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,['Transposition table'],['Transposition table'],set(),set(),0,2.16,0.9982480404920016
55,120,Negamax,http://dbpedia.org/resource/Negamax,http://en.wikipedia.org/wiki/Negamax,"Negamax search is a variant form of minimax search that relies on the zero-sum property of a two-player game. This algorithm relies on the fact that to simplify the implementation of the minimax algorithm. More precisely, the value of a position to player A in such a game is the negation of the value to player B. Thus, the player on move looks for a move that maximizes the negation of the value resulting from the move: this successor position must by definition have been valued by the opponent. The reasoning of the previous sentence works regardless of whether A or B is on move. This means that a single procedure can be used to value both positions. This is a coding simplification over minimax, which requires that A selects the move with the maximum-valued successor while B selects the move with the minimum-valued successor. It should not be confused with negascout, an algorithm to compute the minimax or negamax value quickly by clever use of alpha-beta pruning discovered in the 1980s. Note that alpha-beta pruning is itself a way to compute the minimax or negamax value of a position quickly by avoiding the search of certain uninteresting positions. Most adversarial search engines are coded using some form of negamax search.",6121261303124729217,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,['Negamax'],"['Negamax', 'Alpha–beta pruning', 'Algorithm', 'MTD(f)', 'Transposition table']",set(),set(),0,1.25,4.418942733847308e-304
56,121,MTD(f),http://dbpedia.org/resource/MTD(f),http://en.wikipedia.org/wiki/MTD(f),"MTD(f) is an alpha-beta game tree search algorithm modified to use ‘zero-window’ initial search bounds, and memory (usually a transposition table) to reuse intermediate search results. MTD(f) is a shortened form of MTD(n,f) which stands for Memory-enhanced Test Driver with node ‘n’ and value ‘f’. The efficacy of this paradigm depends on a good initial guess, and the supposition that the final minimax value lies in a narrow window around the guess (which becomes an upper/lower bound for the search from root). The memory structure is used to save an initial guess determined elsewhere. MTD(f) was introduced in 1994 and largely supplanted NegaScout (PVS), the previously dominant search paradigm for chess, checkers, othello and other game automatons.",1278029998864569407,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,['MTD(f)'],"['MTD(f)', 'SSS*']",set(),set(),0,0.65,4.357326232037399e-304
57,122,Best first search,http://dbpedia.org/resource/Best_first_search,http://en.wikipedia.org/wiki/Best_first_search,,3830618643584990371,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.04081632653061224,0.3870682299308375
58,124,Killer heuristic,http://dbpedia.org/resource/Killer_heuristic,http://en.wikipedia.org/wiki/Killer_heuristic,"In competitive two-player games, the killer heuristic is a move-ordering method based on the observation that a strong move or small set of such moves in a particular position may be equally strong in similar positions at the same move (ply) in the game tree.Retaining such moves obviates the effort of rediscovering them in sibling nodes. This technique improves the efficiency of alpha–beta pruning, which in turn improves the efficiency of the minimax algorithm. Alpha–beta pruning works best when the best moves are considered first. This is because the best moves are the ones most likely to produce a cutoff, a condition where the game-playing program knows that the position it is considering could not possibly have resulted from best play by both sides and so need not be considered further. I.e. the game-playing program will always make its best available move for each position. It only needs to consider the other player's possible responses to that best move, and can skip evaluation of responses to (worse) moves it will not make. The killer heuristic attempts to produce a cutoff by assuming that a move that produced a cutoff in another branch of the game tree at the same depth is likely to produce a cutoff in the present position, that is to say that a move that was a very good move from a different (but possibly similar) position might also be a good move in the present position. By trying the killer move before other moves, a game-playing program can often produce an early cutoff, saving itself the effort of considering or even generating all legal moves from a position. In practical implementation, game-playing programs frequently keep track of two killer moves for each depth of the game tree (greater than depth of 1) and see if either of these moves, if legal, produces a cutoff before the program generates and considers the rest of the possible moves. If a non-killer move produces a cutoff, it replaces one of the two killer moves at its depth. This idea can be generalized into a set of refutation tables. A generalization of the killer heuristic is the history heuristic. The history heuristic can be implemented as a table that is indexed by some characteristic of the move, for example ""from"" and ""to"" squares or piece moving and the ""to"" square. When there is a cutoff, the appropriate entry in the table is incremented, such as by adding d² or 2d where d is the current search depth. Beyond an incremental depth of about 2, history information rapidly degenerates into ""noise"".",3402408082351728868,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],['Alpha–beta pruning'],set(),set(),0,3.0,4.3772541870747526e-304
59,125,Alexander Brudno,http://dbpedia.org/resource/Alexander_Brudno,http://en.wikipedia.org/wiki/Alexander_Brudno,"Alexander L'vovich Brudno (Russian: Александр Львович Брудно) (10 January 1918 – 1 December 2009) was a Russian computer scientist, best known for fully describing the alpha-beta pruning algorithm. From 1991 until his death he lived in Israel.",2631812181341302501,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],"['Pattern recognition', 'Minimax']",set(),set(),0,0.4090909090909091,1.2822581892803042
60,126,Expectiminimax,http://dbpedia.org/resource/Expectiminimax,http://en.wikipedia.org/wiki/Expectiminimax,"The expectiminimax algorithm is a variation of the minimax algorithm, for use in artificial intelligence systems that play two-player zero-sum games, such as backgammon, in which the outcome depends on a combination of the player's skill and chance elements such as dice rolls. In addition to ""min"" and ""max"" nodes of the traditional minimax tree, this variant has ""chance"" (""move by nature"") nodes, which take the expected value of a random event occurring. In game theory terms, an expectiminimax tree is the game tree of an extensive-form game of perfect, but incomplete information. In the traditional minimax method, the levels of the tree alternate from max to min until the depth limit of the tree has been reached. In an expectiminimax tree, the ""chance"" nodes are interleaved with the max and min nodes. Instead of taking the max or min of the utility values of their children, chance nodes take a weighted average, with the weight being the probability that child is reached. The interleaving depends on the game. Each ""turn"" of the game is evaluated as a ""max"" node (representing the AI player's turn), a ""min"" node (representing a potentially-optimal opponent's turn), or a ""chance"" node (representing a random effect or player). For example, consider a game in which each round consists of a single die throw, and then decisions made by first the AI player, and then another intelligent opponent. The order of nodes in this game would alternate between ""chance"", ""max"" and then ""min"".",7099461916616540755,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,['AI'],"['AI', 'Probability', 'Algorithm']",set(),set(),0,0.6086956521739131,4.375920596097495e-304
61,128,Game tree,http://dbpedia.org/resource/Game_tree,http://en.wikipedia.org/wiki/Game_tree,"In the context of Combinatorial game theory, which typically studies sequential games with perfect information, a game tree is a graph representing all possible game states within such a game. Such games include well-known ones such as chess, checkers, Go, and tic-tac-toe. This can be used to measure the complexity of a game, as it represents all the possible ways a game can pan out. Due to the large game trees of complex games such as chess, algorithms that are designed to play this class of games will use partial game trees, which makes computation feasible on modern computers. Various methods exist to solve game trees. If a complete game tree can be generated, a deterministic algorithm, such as backward induction or retrograde analysis can be used. Randomized algorithms and minimax algorithms such as MCTS can be used in cases where a complete game tree is not feasible.",5827799624606175955,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],['Game tree'],set(),set(),0,3.7142857142857144,4.310945863222061e-304
62,130,Principal variation,http://dbpedia.org/resource/Principal_variation,http://en.wikipedia.org/wiki/Principal_variation,,3154752461140115653,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5,0.42829039884373377
63,131,Minimax,http://dbpedia.org/resource/Minimax,http://en.wikipedia.org/wiki/Minimax,"Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. When dealing with gains, it is referred to as ""maximin"" – to maximize the minimum gain. Originally formulated for several-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.",668925713677148485,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,['Minimax'],['Minimax'],set(),set(),0,1.8703703703703705,1.2990322693877623
64,132,Branching factor,http://dbpedia.org/resource/Branching_factor,http://en.wikipedia.org/wiki/Branching_factor,"In computing, tree data structures, and game theory, the branching factor is the number of children at each node, the outdegree. If this value is not uniform, an average branching factor can be calculated. For example, in chess, if a ""node"" is considered to be a legal position, the average branching factor has been said to be about 35, and a statistical analysis of over 2.5 million games revealed an average of 31. This means that, on average, a player has about 31 to 35 legal moves at their disposal at each turn. By comparison, the average branching factor for the game Go is 250. Higher branching factors make algorithms that follow every branch at every node, such as exhaustive brute force searches, computationally more expensive due to the exponentially increasing number of nodes, leading to combinatorial explosion. For example, if the branching factor is 10, then there will be 10 nodes one level down from the current position, 102 (or 100) nodes two levels down, 103 (or 1,000) nodes three levels down, and so on. The higher the branching factor, the faster this ""explosion"" occurs. The branching factor can be cut down by a pruning algorithm. The average branching factor can be quickly calculated as the number of non-root nodes (the size of the tree, minus one; or the number of edges) divided by the number of non-leaf nodes (the number of nodes with children).",1200914030714693114,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.2222222222222223,4.302870133358681e-304
65,133,Branch and bound,http://dbpedia.org/resource/Branch_and_bound,http://en.wikipedia.org/wiki/Branch_and_bound,"Branch and bound (BB, B&B, or BnB) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as mathematical optimization. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm. The algorithm depends on efficient estimation of the lower and upper bounds of regions/branches of the search space. If no bounds are available, the algorithm degenerates to an exhaustive search. The method was first proposed by Ailsa Land and Alison Doig whilst carrying out research at the London School of Economics sponsored by British Petroleum in 1960 for discrete programming, and has become the most commonly used tool for solving NP-hard optimization problems. The name ""branch and bound"" first occurred in the work of Little et al. on the traveling salesman problem. Branch and bound methods do not go deep like Depth-first search; the first direction is lateral movement in the tree similar to Breadth-first search (BFS).",5876878083626410370,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,"['Branch and bound', 'Breadth-first search', 'Depth-first search']","['Branch and bound', ""Dijkstra's algorithm""]",set(),set(),0,1.6369047619047619,4.375519036295643e-304
66,134,Subtrees,http://dbpedia.org/resource/Subtrees,http://en.wikipedia.org/wiki/Subtrees,,2016429926377927949,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.012145748987854251,0.44773585461940224
67,135,SSS*,http://dbpedia.org/resource/SSS*,http://en.wikipedia.org/wiki/SSS*,"SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm. SSS* is based on the notion of . Informally, a solution tree can be formed from any arbitrary game tree by pruning the number of branches at each MAX node to one. Such a tree represents a complete strategy for MAX, since it specifies exactly one MAX action for every possible sequence of moves made by the opponent. Given a game tree, SSS* searches through the space of partial solution trees, gradually analyzing larger and larger subtrees, eventually producing a single solution tree with the same root and Minimax value as the original game tree. SSS* never examines a node that alpha-beta pruning would prune, and may prune some branches that alpha-beta would not. Stockman speculated that SSS* may therefore be a better general algorithm than alpha-beta. However, and Judea Pearl have shown that the savings in the number of positions that SSS* evaluates relative to alpha/beta is limited and generally not enough to compensate for the increase in other resources (e.g., the storing and sorting of a list of nodes made necessary by the best-first nature of the algorithm). However, , Jonathan Schaeffer, Wim Pijls and Arie de Bruin have shown that a sequence of null-window alpha-beta calls is equivalent to SSS* (i.e., it expands the same nodes in the same order) when alpha-beta is used with a transposition table, as is the case in all game-playing programs for chess, checkers, etc. Now the storing and sorting of the OPEN list were no longer necessary. This allowed the implementation of (an algorithm equivalent to) SSS* in tournament quality game-playing programs. Experiments showed that it did indeed perform better than Alpha-Beta in practice, but that it did not beat NegaScout. The reformulation of a best-first algorithm as a sequence of depth-first calls prompted the formulation of a class of null-window alpha-beta algorithms, of which MTD(f) is the best known example.",1330601846307974475,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,"['MTD(f)', 'Minimax', 'SSS*']","['SSS*', 'Minimax', 'MTD(f)']",set(),set(),0,1.2222222222222223,4.30478961202653e-304
68,136,Pruning (algorithm),http://dbpedia.org/resource/Pruning_(algorithm),http://en.wikipedia.org/wiki/Pruning_(algorithm),,1047454282601795564,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.9411764705882353,0.5694959265288903
69,137,Principal variation search,http://dbpedia.org/resource/Principal_variation_search,http://en.wikipedia.org/wiki/Principal_variation_search,"Principal variation search (sometimes equated with the practically identical NegaScout) is a negamax algorithm that can be faster than alpha-beta pruning. Like alpha-beta pruning, NegaScout is a directional search algorithm for computing the minimax value of a node in a tree. It dominates alpha-beta pruning in the sense that it will never examine a node that can be pruned by alpha-beta; however, it relies on accurate node ordering to capitalize on this advantage. NegaScout works best when there is a good move ordering. In practice, the move ordering is often determined by previous shallower searches. It produces more cutoffs than alpha-beta by assuming that the first explored node is the best. In other words, it supposes the first node is in the principal variation. Then, it can check whether that is true by searching the remaining nodes with a null window (also known as a scout window; when alpha and beta are equal), which is faster than searching with the regular alpha-beta window. If the proof fails, then the first node was not in the principal variation, and the search continues as normal alpha-beta. Hence, NegaScout works best when the move ordering is good. With a random move ordering, NegaScout will take more time than regular alpha-beta; although it will not explore any nodes alpha-beta did not, it will have to re-search many nodes. Alexander Reinefeld invented NegaScout several decades after the invention of alpha-beta pruning. He gives a proof of correctness of NegaScout in his book. Another search algorithm called SSS* can theoretically result in fewer nodes searched. However, its original formulation has practical issues (in particular, it relies heavily on an OPEN list for storage) and nowadays most chess engines still use a form of NegaScout in their search. Most chess engines use a transposition table in which the relevant part of the search tree is stored. This part of the tree has the same size as SSS*'s OPEN list would have. A reformulation called MT-SSS* allowed it to be implemented as a series of null window calls to Alpha-Beta (or NegaScout) that use a transposition table, and direct comparisons using game playing programs could be made. It did not outperform NegaScout in practice. Yet another search algorithm, which does tend to do better than NegaScout in practice, is the best-first algorithm called MTD(f), although neither algorithm dominates the other. There are trees in which NegaScout searches fewer nodes than SSS* or MTD(f) and vice versa. NegaScout takes after SCOUT, invented by Judea Pearl in 1980, which was the first algorithm to outperform alpha-beta and to be proven asymptotically optimal. Null windows, with β=α+1 in a negamax setting, were invented independently by J.P. Fishburn and used in an algorithm similar to SCOUT in an appendix to his Ph.D. thesis, in a parallel alpha-beta algorithm, and on the last subtree of a search tree root node.",8589749959749369645,related_concept,Alpha–beta pruning,2024-06-23 21:17:09.728,"['MTD(f)', 'Principal variation', 'SSS*', 'Principal variation search']","['Principal variation', 'Principal variation search', 'MTD(f)', 'SSS*']",set(),set(),0,4.25,4.3324165471865105e-304
70,138,Alpha–beta pruning,http://dbpedia.org/resource/Alpha–beta_pruning,http://en.wikipedia.org/wiki/Alpha–beta_pruning,"Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Connect 4, etc.). It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.",1162369215235500239,main_concept,,2024-06-23 21:17:09.728,[],"['Alpha–beta pruning', 'Alexander Brudno', 'MTD(f)']",set(),set(),0,1.367816091954023,4.2999404688318855e-304
71,141,Hotelling's t-squared statistic,http://dbpedia.org/resource/Hotelling's_t-squared_statistic,http://en.wikipedia.org/wiki/Hotelling's_t-squared_statistic,,4010362292191120068,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,[],"[""Hotelling's t-squared statistic"", ""Student's t-statistic"", ""Student's t-distribution"", 'Wishart distribution', ""Welch's t-test""]",set(),set(),0,0.016129032258064516,0.3644400317610601
72,144,Z-test,http://dbpedia.org/resource/Z-test,http://en.wikipedia.org/wiki/Z-test,"A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. Z-tests test the mean of a distribution. For each significance level in the confidence interval, the Z-test has a single critical value (for example, 1.96 for 5% two tailed) which makes it more convenient than the Student's t-test whose critical values are defined by the sample size (through the corresponding degrees of freedom). Both the Z test and Student's t-test have similarities in that they both help determine the significance of a set of data. However, the z-test is rarely used in practice because the population deviation is difficult to determine.",1942011344415444051,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,"['Z-test', ""Student's t-test""]","['Z-test', ""Student's t-test"", 'Null hypothesis', 'Fisher information']",set(),set(),0,1.562624254473161,1.1960212383684807
73,145,Student's t-statistic,http://dbpedia.org/resource/Student's_t-statistic,http://en.wikipedia.org/wiki/Student's_t-statistic,,6752024760931435335,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,[],"[""Student's t-test"", ""Student's t-distribution"", 'Mean']",set(),set(),0,0.2571428571428571,0.43586519418076275
74,146,Noncentral t-distribution,http://dbpedia.org/resource/Noncentral_t-distribution,http://en.wikipedia.org/wiki/Noncentral_t-distribution,"The noncentral t-distribution generalizes Student's t-distribution using a noncentrality parameter. Whereas the central probability distribution describes how a test statistic t is distributed when the difference tested is null, the noncentral distribution describes how t is distributed when the null is false. This leads to its use in statistics, especially calculating statistical power. The noncentral t-distribution is also known as the singly noncentral t-distribution, and in addition to its primary use in statistical inference, is also used in robust modeling for data.",6581847329575947159,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,"[""Student's t-distribution""]","[""Student's t-distribution""]",set(),set(),0,0.5526315789473685,1.3381837420960851
75,148,Welch's t-test,http://dbpedia.org/resource/Welch's_t-test,http://en.wikipedia.org/wiki/Welch's_t-test,"In statistics, Welch's t-test, or unequal variances t-test, is a two-sample location test which is used to test the hypothesis that two populations have equal means. It is named for its creator, Bernard Lewis Welch, is an adaptation of Student's t-test, and is more reliable when the two samples have unequal variances and possibly unequal sample sizes. These tests are often referred to as ""unpaired"" or ""independent samples"" t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping. Given that Welch's t-test has been less popular than Student's t-test and may be less familiar to readers, a more informative name is ""Welch's unequal variances t-test"" — or ""unequal variances t-test"" for brevity.",9174084773445414006,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,"[""Welch's t-test"", ""Student's t-test""]","[""Welch's t-test"", ""Student's t-test"", 'ANOVA']",set(),set(),0,0.6181818181818182,1.2822915402044357
76,150,Pearson distribution,http://dbpedia.org/resource/Pearson_distribution,http://en.wikipedia.org/wiki/Pearson_distribution,The Pearson distribution is a family of continuous probability distributions. It was first published by Karl Pearson in 1895 and subsequently extended by him in 1901 and 1916 in a series of articles on biostatistics.,9153596626256308026,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,['Pearson distribution'],"['Pearson distribution', ""Student's t-distribution"", 'Bernoulli distribution', 'Beta distribution', 'Cauchy distribution']",set(),set(),0,1.251937984496124,4.721723356118704e-304
77,151,Nuisance parameter,http://dbpedia.org/resource/Nuisance_parameter,http://en.wikipedia.org/wiki/Nuisance_parameter,"In statistics, a nuisance parameter is any parameter which is unspecified but which must be accounted for in the hypothesis testing of the parameters which are of interest. The classic example of a nuisance parameter comes from the normal distribution, a member of the location–scale family. For at least one normal distributions, the variance(s), σ2 is often not specified or known, but one desires to hypothesis test on the mean(s). Another example might be linear regression with unknown variance in the explanatory variable (the independent variable): its variance is a nuisance parameter that must be accounted for to derive an accurate interval estimate of the regression slope, calculate p-values, hypothesis test on the slope's value; see regression dilution. Nuisance parameters are often scale parameter, but not always; for example in errors-in-variables models, the unknown true location of each observation is a nuisance parameter. A parameter may also cease to be a ""nuisance"" if it becomes the object of study, is estimated from data, or known.",3391606587453128704,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,['Nuisance parameter'],['Nuisance parameter'],set(),set(),0,1.9310344827586208,1.335164816311138
78,152,Degrees of freedom (statistics),http://dbpedia.org/resource/Degrees_of_freedom_(statistics),http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics),"In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself. For example, if the variance is to be estimated from a random sample of N independent scores, then the degrees of freedom is equal to the number of independent scores (N) minus the number of parameters estimated as intermediate steps (one, namely, the sample mean) and is therefore equal to N − 1. Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of ""free"" components (how many components need to be known before the vector is fully determined). The term is most often used in the context of linear models (linear regression, analysis of variance), where certain random vectors are constrained to lie in linear subspaces, and the number of degrees of freedom is the dimension of the subspace. The degrees of freedom are also commonly associated with the squared lengths (or ""sum of squares"" of the coordinates) of such vectors, and the parameters of chi-squared and other distributions that arise in associated statistical testing problems. While introductory textbooks may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept.",4058898187963798004,related_concept,Student\'s t-test,2024-06-23 21:17:09.728,[],"[""Student's t-distribution"", 'Mean', 'Variance', 'ANOVA', 'F-test']",set(),set(),0,1.967914438502674,1.347285118059592
79,159,Student's t-test,http://dbpedia.org/resource/Student's_t-test,http://en.wikipedia.org/wiki/Student's_t-test,"A t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. It is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known (typically, the scaling term is unknown and therefore a nuisance parameter). When the scaling term is estimated based on the data, the test statistic—under certain conditions—follows a Student's t distribution. The t-test's most common application is to test whether the means of two populations are different.",4571663211516399089,main_concept,,2024-06-23 21:17:09.728,"[""Student's t-distribution""]","['Z-test', ""Student's t-test"", ""Student's t-distribution"", ""Welch's t-test"", 'Pearson correlation coefficient', 'Null hypothesis', 'ANOVA', ""Hotelling's t-squared statistic"", 'Data']",set(),set(),0,1.6751269035532994,1.2862088481555658
80,163,Informal inferential reasoning,http://dbpedia.org/resource/Informal_inferential_reasoning,http://en.wikipedia.org/wiki/Informal_inferential_reasoning,"In statistics education, informal inferential reasoning (also called informal inference) refers to the process of making a generalization based on data (samples) about a wider universe (population/process) while taking into account uncertainty without using the formal statistical procedure or methods (e.g. P-values, t-test, hypothesis testing, significance test). Like formal statistical inference, the purpose of informal inferential reasoning is to draw conclusions about a wider universe (population/process) from data (sample). However, in contrast with formal statistical inference, formal statistical procedure or methods are not necessarily used. In statistics education literature, the term ""informal"" is used to distinguish informal inferential reasoning from a formal method of statistical inference.",352583676129510154,related_concept,Statistical inference,2024-06-23 21:17:09.728,['P-value'],"['P-value', 'Informal inferential reasoning']",set(),set(),0,2.125,1.3793153615247766
81,170,Algorithmic inference,http://dbpedia.org/resource/Algorithmic_inference,http://en.wikipedia.org/wiki/Algorithmic_inference,"Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability.The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.",7908224556726231303,related_concept,Statistical inference,2024-06-23 21:17:09.728,"['Algorithmic inference', 'Algorithm']","['Algorithmic inference', 'Algorithm']",set(),set(),0,1.9047619047619047,1.3575385287887294
82,201,T-distributed stochastic neighbor embedding,http://dbpedia.org/resource/T-distributed_stochastic_neighbor_embedding,http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding,"t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton, where proposed the t-distributed variant. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability. The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate. t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research, natural language processing, music analysis, cancer research, bioinformatics, geological domain interpretation, and biomedical signal processing. While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such ""clusters"" can be shown to even appear in non-clustered data, and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results. It has been demonstrated that t-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.",3146106767621928971,related_concept,Clustering high-dimensional data,2024-06-23 21:17:09.728,"['Stochastic', 'Euclidean distance']","['Stochastic', 'Kullback–Leibler divergence', 'Euclidean distance', 'Cauchy distribution']",set(),set(),0,2.78,1.3373685018600214
83,210,Dijkstra's algorithm,http://dbpedia.org/resource/Dijkstra's_algorithm,http://en.wikipedia.org/wiki/Dijkstra's_algorithm,"Dijkstra's algorithm (/ˈdaɪkstrəz/ DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. The algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree. For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and OSPF (Open Shortest Path First). It is also employed as a subroutine in other algorithms such as Johnson's. The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered. It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing. This generalization is called the generic Dijkstra shortest-path algorithm. Dijkstra's algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. While the original algorithm uses a min-priority queue and runs in time (where is the number of nodes and is the number of edges), it can also be implemented in using an array. The idea of this algorithm is also given in . propose using a Fibonacci heap min-priority queue to optimize the running time complexity to . This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can indeed be improved further as detailed in Specialized variants. Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster. In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.",1931702083525107063,related_concept,Clustering high-dimensional data,2024-06-23 21:17:09.728,"[""Dijkstra's algorithm""]","[""Dijkstra's algorithm"", ""Prim's algorithm"", 'Breadth-first search']",set(),set(),0,2.6511627906976742,1.0741391939691158
84,213,Distance function,http://dbpedia.org/resource/Distance_function,http://en.wikipedia.org/wiki/Distance_function,,8884977009317584742,related_concept,Clustering high-dimensional data,2024-06-23 21:17:09.728,[],"['Euclidean plane', 'Euclidean distance', 'Manhattan distance', 'Chebyshev distance', 'Cauchy sequence']",set(),set(),0,0.12427745664739884,0.3367810794405995
85,218,Heaps' law,http://dbpedia.org/resource/Heaps'_law,http://en.wikipedia.org/wiki/Heaps'_law,"In linguistics, Heaps' law (also called Herdan's law) is an empirical law which describes the number of distinct words in a document (or set of documents) as a function of the document length (so called type-token relation). It can be formulated as where VR is the number of distinct words in an instance text of size n. K and β are free parameters determined empirically. With English text corpora, typically K is between 10 and 100, and β is between 0.4 and 0.6. The law is frequently attributed to , but was originally discovered by Gustav Herdan. Under mild assumptions, the Herdan–Heaps law is asymptotically equivalent to Zipf's law concerning the frequencies of individual words within a text. This is a consequence of the fact that the type-token relation (in general) of a homogenous text can be derived from the distribution of its types. Heaps' law means that as more instance text is gathered, there will be diminishing returns in terms of discovery of the full vocabulary from which the distinct terms are drawn. Heaps' law also applies to situations in which the ""vocabulary"" is just some set of distinct types which are attributes of some collection of objects. For example, the objects could be people, and the types could be country of origin of the person. If persons are selected randomly (that is, we are not selecting based on country of origin), then Heaps' law says we will quickly have representatives from most countries (in proportion to their population) but it will become increasingly difficult to cover the entire set of countries by continuing this method of sampling.Heaps' law has been observed also in single-cell transcriptomes considering genes as the distinct objects in the ""vocabulary"".",8428108892040224319,related_concept,Clustering high-dimensional data,2024-06-23 21:17:09.728,"[""Heaps' law""]","[""Heaps' law""]",set(),set(),0,1.1785714285714286,1.3937123088192407
86,219,DNA microarray,http://dbpedia.org/resource/DNA_microarray,http://en.wikipedia.org/wiki/DNA_microarray,"A DNA microarray (also commonly known as DNA chip or biochip) is a collection of microscopic DNA spots attached to a solid surface. Scientists use DNA microarrays to measure the expression levels of large numbers of genes simultaneously or to genotype multiple regions of a genome. Each DNA spot contains picomoles (10−12 moles) of a specific DNA sequence, known as probes (or reporters or oligos). These can be a short section of a gene or other DNA element that are used to hybridize a cDNA or cRNA (also called anti-sense RNA) sample (called target) under high-stringency conditions. Probe-target hybridization is usually detected and quantified by detection of fluorophore-, silver-, or chemiluminescence-labeled targets to determine relative abundance of nucleic acid sequences in the target. The original nucleic acid arrays were macro arrays approximately 9 cm × 12 cm and the first computerized image based analysis was published in 1981. It was invented by Patrick O. Brown. An example of its application is in SNPs arrays for polymorphisms in cardiovascular diseases, cancer, pathogens and GWAS analysis. It is also used for the identification of structural variations and the measurement of gene expression.",4090551185176128418,related_concept,Clustering high-dimensional data,2024-06-23 21:17:09.728,['DNA microarray'],"['DNA microarray', 'Data', 'Data analysis', 'Experimental design', 'Algorithm']",{'Bioinformatics'},set(),0,1.90990990990991,1.2591982127440815
87,224,Computation,http://dbpedia.org/resource/Computation,http://en.wikipedia.org/wiki/Computation,"Computation is any type of arithmetic or non-arithmetic calculation that follows a well-defined model (e.g., an algorithm). Mechanical or electronic devices (or, historically, people) that perform computations are known as computers. An especially well-known discipline of the study of computation is computer science.",1746898430729522457,related_concept,Data processing,2024-06-23 21:17:09.728,['Computation'],"['Computer science', 'Computation']",set(),set(),0,8.323076923076924,1.4185738738002032
88,225,SPSS,http://dbpedia.org/resource/SPSS,http://en.wikipedia.org/wiki/SPSS,"SPSS Statistics is a statistical software suite developed by IBM for data management, advanced analytics, multivariate analysis, business intelligence, and criminal investigation. Long produced by SPSS Inc., it was acquired by IBM in 2009. Current versions (post 2015) have the brand name: IBM SPSS Statistics. The software name originally stood for Statistical Package for the Social Sciences (SPSS), reflecting the original market, then later changed to Statistical Product and Service Solutions.",8030026037062259834,related_concept,Data processing,2024-06-23 21:17:09.728,"['SPSS', 'Statistics']","['SPSS', 'Statistics', 'Data', 'SQL', 'PDF', 'ANOVA', 'Bayesian statistics', 'Analytics', 'Algorithm', 'Dimension', 'Regression analysis']",set(),set(),0,1.5994475138121547,1.3055190963137053
89,226,Information processing,http://dbpedia.org/resource/Information_processing,http://en.wikipedia.org/wiki/Information_processing,"Information processing is the change (processing) of information in any manner detectable by an observer. As such, it is a process that describes everything that happens (changes) in the universe, from the falling of a rock (a change in position) to the printing of a text file from a digital computer system. In the latter case, an information processor (the printer) is changing the form of presentation of that text file (from bytes to glyphs). The computers up to this period function on the basis of programs saved in the memory, having no intelligence of their own.",1559517196794586323,related_concept,Data processing,2024-06-23 21:17:09.728,['Information processing'],[],set(),set(),0,36.2,1.300102157851509
90,228,SAS (software),http://dbpedia.org/resource/SAS_(software),http://en.wikipedia.org/wiki/SAS_(software),"SAS (previously ""Statistical Analysis System"") is a statistical software suite developed by SAS Institute for data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics. SAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.",3666164514693316298,related_concept,Data processing,2024-06-23 21:17:09.728,[],"['AI', 'Data', 'Data set', 'PDF', 'SQL', 'Analytics', 'SPSS']",set(),set(),0,1.6157407407407407,1.3117108646956355
91,230,DAP (software),http://dbpedia.org/resource/DAP_(software),http://en.wikipedia.org/wiki/DAP_(software),"Dap is a statistics and graphics program based on the C programming language that performs data management, analysis, and C-style graphical visualization tasks without requiring complex syntax. Its name is an acronym for Data Analysis and Presentation. Dap was written to be a free replacement for SAS, but users are assumed to have a basic familiarity with the C programming language in order to permit greater flexibility. It has been designed to be used on large data sets and is primarily used in statistical consulting practices. However, even with its clear benefits, Dap hasn't been updated since 2014 and hasn't seen widespread use when compared to other statistical analysis programs.",2591422532722169797,related_concept,Data processing,2024-06-23 21:17:09.728,['Data'],"['Data', 'ANOVA']",set(),set(),0,0.6363636363636364,1.3178400223044513
92,231,Automatic summarization,http://dbpedia.org/resource/Automatic_summarization,http://en.wikipedia.org/wiki/Automatic_summarization,"Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data. Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document. On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection. Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion. Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.",6132976574814455249,related_concept,Data processing,2024-06-23 21:17:09.728,['Automatic summarization'],"['Automatic summarization', 'PageRank', 'F-score', 'Algorithm', 'Bayes classifier', 'Naive Bayes classifier']",set(),set(),0,1.2938388625592416,1.3408169086599095
93,232,Data classification (business intelligence),http://dbpedia.org/resource/Data_classification_(business_intelligence),http://en.wikipedia.org/wiki/Data_classification_(business_intelligence),"In business intelligence, data classification has close ties to data clustering, but where data clustering is descriptive, data classification is predictive. In essence data classification consists of using variables with known values to predict the unknown or future values of other variables. It can be used in e.g. direct marketing, insurance fraud detection or medical diagnosis. The first step in doing a data classification is to cluster the data set used for category training, to create the wanted number of categories. An algorithm, called the classifier, is then used on the categories, creating a descriptive model for each. These models can then be used to categorize new items in the created classification system.",6939897585353420776,related_concept,Data processing,2024-06-23 21:17:09.728,[],"['Data', 'Classification']",set(),set(),0,2.75,1.2981677024914096
94,238,Data collection,http://dbpedia.org/resource/Data_collection,http://en.wikipedia.org/wiki/Data_collection,"Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities, and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that allows analysis to lead to the formulation of convincing and credible answers to the questions that have been posed. Data collection and validation consists of four steps when it involves taking a census and seven steps when it involves sampling. Regardless of the field of or preference for defining data (quantitative or qualitative), accurate data collection is essential to maintain research integrity. The selection of appropriate data collection instruments (existing, modified, or newly developed) and delineated instructions for their correct use reduce the likelihood of errors. A formal data collection process is necessary as it ensures that the data gathered are both defined and accurate. This way, subsequent decisions based on arguments embodied in the findings are made using valid data. The process provides both a baseline from which to measure and in certain cases an indication of what to improve. There are 5 common data collection methods: 1. 
* closed-ended surveys and quizzes, 2. 
* open-ended surveys and questionnaires, 3. 
* 1-on-1 interviews, 4. 
* focus groups, and 5. 
* direct observation.",4017435832575253082,related_concept,Data processing,2024-06-23 21:17:09.728,"['Data collection', 'Data']","['Data collection', 'Data', 'Information processing']",set(),set(),0,2.610215053763441,5.018847717222608e-304
95,239,Data validation,http://dbpedia.org/resource/Data_validation,http://en.wikipedia.org/wiki/Data_validation,"In computer science, data validation is the process of ensuring data has undergone data cleansing to ensure they have data quality, that is, that they are both correct and useful. It uses routines, often called ""validation rules"", ""validation constraints"", or ""check routines"", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic of the computer and its application. This is distinct from formal verification, which attempts to prove or disprove the correctness of algorithms for implementing a specification or property.",2833357680463229165,related_concept,Data processing,2024-06-23 21:17:09.728,[],"['Data validation', 'Data', 'Data type', 'Consistency']",set(),set(),0,3.4358974358974357,5.040751173725901e-304
96,241,Data processing,http://dbpedia.org/resource/Data_processing,http://en.wikipedia.org/wiki/Data_processing,"Data processing is the collection and manipulation of digital data to produce meaningful information. Data processing is a form of information processing, which is the modification (processing) of information in any manner detectable by an observer. The term ""Data Processing"", or ""DP"" has also been used to refer to a department within an organization responsible for the operation of data processing programs.",6310559231829300966,main_concept,,2024-06-23 21:17:09.728,"['Data processing', 'Data']","['Data processing', 'Data', 'SPSS', 'Data analysis']",set(),set(),0,5.2342342342342345,5.262499812604003e-304
97,242,Inquisitive learning,http://dbpedia.org/resource/Inquisitive_learning,http://en.wikipedia.org/wiki/Inquisitive_learning,,4866188564610023376,related_concept,Active learning,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.00998003992015968,0.4734048571247948
98,243,Student-centered,http://dbpedia.org/resource/Student-centered,http://en.wikipedia.org/wiki/Student-centered,,4040912057167780007,related_concept,Active learning,2024-06-23 21:17:09.728,[],['Student-centered'],set(),set(),0,0.01583710407239819,0.509146350957507
99,244,Learning styles,http://dbpedia.org/resource/Learning_styles,http://en.wikipedia.org/wiki/Learning_styles,"Learning styles refer to a range of theories that aim to account for differences in individuals' learning. Although there is ample evidence that individuals express personal preferences for how they prefer to receive information, few studies have found any validity in using learning styles in education. Many theories share the proposition that humans can be classified according to their ""style"" of learning, but differ in how the proposed styles should be defined, categorized and assessed. A common concept is that individuals differ in how they learn. The idea of individualized learning styles became popular in the 1970s, and has greatly influenced education despite the criticism that the idea has received from some researchers. Proponents recommend that teachers have to run a needs analysis to assess the learning styles of their students and adapt their classroom methods to best fit each student's learning style. Critics say there is no consistent evidence that identifying an individual student's learning style and teaching for specific learning styles produces better student outcomes. Since 2012, learning styles have often been referred to as a ""neuromyth"" in education. There is evidence of empirical and pedagogical problems related to forcing learning tasks to ""correspond to differences in a one-to-one fashion"". Studies contradict the widespread ""meshing hypothesis"" that a student will learn best if taught in a method deemed appropriate for the student's learning style. However, a 2020 systematic review suggested that a majority (89%) of educators around the world continue to believe that the meshing hypothesis is correct. Studies further show that teachers cannot assess the learning style of their students accurately. In one study, students were asked to take an inventory on their learning style. After nearly 400 students completed the inventory, 70% didn't use study habits that matched their preferred learning method. Another piece of this study indicated that those students who used study methods that did match their preferred learning style didn't perform any better on tests.",7508467573324310202,related_concept,Active learning,2024-06-23 21:17:09.728,['Learning styles'],['Learning styles'],set(),set(),0,0.45454545454545453,0.5428458925889738
100,245,Cooperative learning,http://dbpedia.org/resource/Cooperative_learning,http://en.wikipedia.org/wiki/Cooperative_learning,"Cooperative learning is an educational approach which aims to organize classroom activities into academic and social learning experiences. There is much more to cooperative learning than merely arranging students into groups, and it has been described as ""structuring positive interdependence."" Students must work in groups to complete tasks collectively toward academic goals. Unlike individual learning, which can be competitive in nature, students learning cooperatively can capitalize on one another's resources and skills (asking one another for information, evaluating one another's ideas, monitoring one another's work, etc.). Furthermore, the teacher's role changes from giving information to facilitating students' learning. Everyone succeeds when the group succeeds. Ross and Smyth (1995) describe successful cooperative learning tasks as intellectually demanding, creative, open-ended, and involve higher-order thinking tasks. Cooperative learning has also been linked to increased levels of student satisfaction. Five essential elements are identified for the successful incorporation of cooperative learning in the classroom: 
* positive interdependence 
* individual and group accountability 
* promotive interaction (face to face) 
* teaching the students the required interpersonal and small group skills 
* group processing. According to Johnson and Johnson's meta-analysis, students in cooperative learning settings compared to those in individualistic or competitive learning settings, achieve more, reason better, gain higher self-esteem, like classmates and the learning tasks more and have more perceived social support.",8566200132965716206,related_concept,Active learning,2024-06-23 21:17:09.728,['Cooperative learning'],['Cooperative learning'],set(),set(),0,2.7115384615384617,2.2756587329118432e-304
101,246,Design-based learning,http://dbpedia.org/resource/Design-based_learning,http://en.wikipedia.org/wiki/Design-based_learning,"Design-based learning (DBL), also known as design-based instruction, is an inquiry-based form of learning, or pedagogy, that is based on integration of design thinking and the design process into the classroom at the K-12 and post-secondary levels. Design-based learning environments can be found across many disciplines, including those traditionally associated with design (e.g. art, architecture, engineering, interior design, graphic design), as well as others not normally considered to be design-related (science, technology, business, humanities). DBL, as well as project-based learning and problem-based learning, is used to teach 21st century skills such as communication and collaboration and foster deeper learning. Deeper learning is supported when students design and create an artifact that requires understanding and application of knowledge. DBL activity supports iteration as students create, assess, and redesign their projects. The work's complexity often requires collaboration and specialized roles, providing students with the opportunity to become ""experts"" in a particular area. Design projects require students to establish goals and constraints, generate ideas, and create prototypes through storyboarding or other representational practices. Robotics competitions in schools are popular design-based learning activities, wherein student teams design, build and then pilot their robots in competitive challenges. Design-based learning was developed in the 1980s by Doreen Nelson, a professor at California State Polytechnic University, Pomona and the Art Center College of Design. Her findings suggested that kinesthetic problem-solving helps students acquire, retain, and synthesize information in practical ways.",7463108548766136801,related_concept,Active learning,2024-06-23 21:17:09.728,['Design-based learning'],['Design-based learning'],set(),set(),0,1.12,2.28288924605959e-304
102,247,Intrinsic motivation,http://dbpedia.org/resource/Intrinsic_motivation,http://en.wikipedia.org/wiki/Intrinsic_motivation,,8802753256976014341,related_concept,Active learning,2024-06-23 21:17:09.728,[],['Intrinsic motivation'],set(),set(),0,0.2692307692307692,0.4393068691544536
103,248,Role-playing,http://dbpedia.org/resource/Role-playing,http://en.wikipedia.org/wiki/Role-playing,"Role-playing or roleplaying is the changing of one's behaviour to assume a role, either unconsciously to fill a social role, or consciously to act out an adopted role. While the Oxford English Dictionary offers a definition of role-playing as ""the changing of one's behaviour to fulfill a social role"", in the field of psychology, the term is used more loosely in four senses: 
* To refer to the playing of roles generally such as in a theatre, or educational setting; 
* To refer to taking a role of a character or person and acting it out with a partner taking someone else's role, often involving different genres of practice; 
* To refer to a wide range of games including role-playing video game (RPG), play-by-mail games and more; 
* To refer specifically to role-playing games.",8212059091294327650,related_concept,Active learning,2024-06-23 21:17:09.728,['Role-playing'],['Role-playing'],{'Role-playing'},set(),0,8.044776119402986,2.2457012930442685e-304
104,249,School organizational models,http://dbpedia.org/resource/School_organizational_models,http://en.wikipedia.org/wiki/School_organizational_models,"School organizational models are methods of structuring the curriculum, functions, and facilities for schools, colleges, and universities. The organizing of teaching and learning has been structured since the first educational institutions were established. With greater specialization and expertise in a particular field of knowledge, and a gathering of like-minded individuals, instructors clustered into specialized groups, schools, and eventually departments within larger institutions. This structure spread rapidly during the 19th and 20th centuries with factory model schools and their ""assembly-line"" method of standardized curriculum and instructional methods. Beginning with the progressive educational movement in the early-mid 20th century, and again with similar trends in the late 20th and early 21st century, alternative models structured towards deeper learning, higher retention, and 21st century skills developed. The organizational models of schools fall into several main categories, including: departmental, integrative, project-based, academy, small learning communities, and school-within-a-school.",1384843507105320948,related_concept,Active learning,2024-06-23 21:17:09.728,['School organizational models'],['School organizational models'],set(),set(),0,0.8181818181818182,0.6987887067083379
105,250,Problem-based learning,http://dbpedia.org/resource/Problem-based_learning,http://en.wikipedia.org/wiki/Problem-based_learning,"Problem-based learning (PBL) is a student-centered pedagogy in which students learn about a subject through the experience of solving an open-ended problem found in trigger material. The PBL process does not focus on problem solving with a defined solution, but it allows for the development of other desirable skills and attributes. This includes knowledge acquisition, enhanced group collaboration and communication. The PBL process was developed for medical education and has since been broadened in applications for other programs of learning. The process allows for learners to develop skills used for their future practice. It enhances critical appraisal, literature retrieval and encourages ongoing learning within a team environment. The PBL tutorial process often involves working in small groups of learners. Each student takes on a role within the group that may be formal or informal and the role often alternates. It is focused on the student's reflection and reasoning to construct their own learning. The Maastricht seven-jump process involves clarifying terms, defining problem(s), brainstorming, structuring and hypothesis, learning objectives, independent study and synthesising. In short, it is identifying what they already know, what they need to know, and how and where to access new information that may lead to the resolution of the problem. The role of the tutor is to facilitate learning by supporting, guiding, and monitoring the learning process. The tutor aims to build students' confidence when addressing problems, while also expanding their understanding. This process is based on constructivism. PBL represents a paradigm shift from traditional teaching and learning philosophy, which is more often lecture-based. The constructs for teaching PBL are very different from traditional classroom or lecture teaching and often require more preparation time and resources to support small group learning.",7218671423920173360,related_concept,Active learning,2024-06-23 21:17:09.728,['Problem-based learning'],['Problem-based learning'],set(),set(),0,0.8879492600422833,2.2743877639556588e-304
106,251,Experiential learning,http://dbpedia.org/resource/Experiential_learning,http://en.wikipedia.org/wiki/Experiential_learning,"Experiential learning (ExL) is the process of learning through experience, and is more narrowly defined as ""learning through reflection on doing"". Hands-on learning can be a form of experiential learning, but does not necessarily involve students reflecting on their product. Experiential learning is distinct from rote or didactic learning, in which the learner plays a comparatively passive role. It is related to, but not synonymous with, other forms of active learning such as action learning, adventure learning, free-choice learning, cooperative learning, service-learning, and situated learning. Experiential learning is often used synonymously with the term ""experiential education"", but while experiential education is a broader philosophy of education, experiential learning considers the individual learning process. As such, compared to experiential education, experiential learning is concerned with more concrete issues related to the learner and the learning context. The general concept of learning through experience is ancient. Around 350 BC, Aristotle wrote in the Nicomachean Ethics ""for the things we have to learn before we can do them, we learn by doing them"". But as an articulated educational approach, experiential learning is of much more recent vintage. Beginning in the 1970s, David A. Kolb helped develop the modern theory of experiential learning, drawing heavily on the work of John Dewey, Kurt Lewin, and Jean Piaget. Experiential learning has significant teaching advantages. Peter Senge, author of The Fifth Discipline (1990), states that teaching is of utmost importance to motivate people. Learning only has good effects when learners have the desire to absorb the knowledge. Therefore, experiential learning requires the showing of directions for learners. Experiential learning entails a hands-on approach to learning that moves away from just the teacher at the front of the room imparting and transferring their knowledge to students. It makes learning an experience that moves beyond the classroom and strives to bring a more involved way of learning.",9029822308099042165,related_concept,Active learning,2024-06-23 21:17:09.728,['Experiential learning'],"['Experiential learning', 'Learning styles']",set(),set(),0,0.9770114942528736,2.2751806103795716e-304
107,252,Just-in-time teaching,http://dbpedia.org/resource/Just-in-time_teaching,http://en.wikipedia.org/wiki/Just-in-time_teaching,"Just-in-time teaching (often abbreviated as JiTT) is a pedagogical strategy that uses feedback between classroom activities and work that students do at home, in preparation for the classroom meeting. The goals are to increase learning during classroom time, to enhance student motivation, to encourage students to prepare for class, and to allow the instructor to fine-tune the classroom activities to best meet students' needs. This should not be confused with just-in-time learning, which itself focuses on immediate connections between learners and the content that is needed at that moment.",4101974816328091501,related_concept,Active learning,2024-06-23 21:17:09.728,['Just-in-time teaching'],['Just-in-time teaching'],set(),set(),0,0.14814814814814814,2.260076265672574e-304
108,253,Learning environment,http://dbpedia.org/resource/Learning_environment,http://en.wikipedia.org/wiki/Learning_environment,"The term learning environment can refer to an educational approach, cultural context, or physical setting in which teaching and learning occur. The term is commonly used as a more definitive alternative to ""classroom"", but it typically refers to the context of educational philosophy or knowledge experienced by the student and may also encompass a variety of learning cultures—its presiding ethos and characteristics, how individuals interact, governing structures, and philosophy. In a societal sense, learning environment may refer to the culture of the population it serves and of their location. Learning environments are highly diverse in use, learning styles, organization, and educational institution. The culture and context of a place or organization includes such factors as a way of thinking, behaving, or working, also known as organizational culture. For a learning environment such as an educational institution, it also includes such factors as operational characteristics of the instructors, instructional group, or institution; the philosophy or knowledge experienced by the student and may also encompass a variety of learning cultures—its presiding ethos and characteristics, how individuals interact, governing structures, and philosophy in learning styles and pedagogies used; and the societal culture of where the learning is occurring. Although physical environments do not determine educational activities, there is evidence of a relationship between school settings and the activities that take place there.",5276140651590369454,related_concept,Active learning,2024-06-23 21:17:09.728,['Learning environment'],"['Learning environment', 'Passive learning', 'Active learning']",set(),set(),0,1.1829268292682926,0.6869971278030622
109,254,Gallery walk,http://dbpedia.org/resource/Gallery_walk,http://en.wikipedia.org/wiki/Gallery_walk,"Gallery walk is a classroom-based active learning strategy where students are encouraged to build on their knowledge about a topic or content to promote higher-order thinking, interaction and cooperative learning. The students in groups move through different stations where a question is posted for them to answer and interact and share knowledge in the process.",795717449300729068,related_concept,Active learning,2024-06-23 21:17:09.728,['Gallery walk'],['Gallery walk'],set(),set(),0,0.45,2.2482462007547936e-304
110,255,Action teaching,http://dbpedia.org/resource/Action_teaching,http://en.wikipedia.org/wiki/Action_teaching,"Action teaching is a style of instruction that aims to teach students about subject material while also contributing to the betterment of society. The approach represents an educational counterpart to action research, a method first developed by Kurt Lewin in the 1940s to address racial prejudice, anti-Semitism, and other societal problems through the integration of social science and social action. Proponents of action teaching argue that by allowing students to take action on social issues as part of the learning process, action teaching deepens learning, heightens student engagement, and provides students with a ""scaffold"" for future prosocial civic action. Action teaching has been used in varied educational settings, including grade schools, high schools, colleges, universities, and online courses taken by student and post-graduate learners. Although action teaching was initially developed within the field of psychology, it later spread to other curricular areas such as business, law, and environmental science. The social issues that it addresses encompass diverse topics such as violence prevention, disaster relief, prejudice reduction, sustainable living, human health. animal protection, and the development of empathy and compassion.",5657195547765484858,related_concept,Active learning,2024-06-23 21:17:09.728,['Action teaching'],"['Action teaching', 'SPSS']",set(),set(),0,0.3488372093023256,2.261594802602504e-304
111,256,Learning space,http://dbpedia.org/resource/Learning_space,http://en.wikipedia.org/wiki/Learning_space,"Learning space or learning setting refers to a physical setting for a learning environment, a place in which teaching and learning occur. The term is commonly used as a more definitive alternative to ""classroom,"" but it may also refer to an indoor or outdoor location, either actual or virtual. Learning spaces are highly diverse in use, configuration, location, and educational institution. They support a variety of pedagogies, including quiet study, passive or active learning, kinesthetic or physical learning, vocational learning, experiential learning, and others. As the design of a learning space impacts the learning process, it is deemed important to design a learning space with the learning process in mind.",5544983872769462421,related_concept,Active learning,2024-06-23 21:17:09.728,['Learning space'],"['Learning space', 'Learning environment', 'Active learning', 'Passive learning']",set(),set(),0,0.4090909090909091,0.5674483936812089
112,257,Case study,http://dbpedia.org/resource/Case_study,http://en.wikipedia.org/wiki/Case_study,"A case study is an in-depth, detailed examination of a particular case (or cases) within a real-world context. For example, case studies in medicine may focus on an individual patient or ailment; case studies in business might cover a particular firm's strategy or a broader market; similarly, case studies in politics can range from a narrow happening over time (e.g., a specific political campaign) to an enormous undertaking (e.g., a world war). Generally, a case study can highlight nearly any individual, group, organization, event, belief system, or action. A case study does not necessarily have to be one observation (N=1), but may include many observations (one or multiple individuals and entities across multiple time periods, all within the same case study). Research projects involving numerous cases are frequently called cross-case research, whereas a study of a single case is called within-case research. Case study research has been extensively practiced in both the social and natural sciences.",1047219908220163588,related_concept,Active learning,2024-06-23 21:17:09.728,['Case study'],['Case study'],set(),set(),0,4.844936708860759,1.3935485040825186
113,258,Learning by teaching,http://dbpedia.org/resource/Learning_by_teaching,http://en.wikipedia.org/wiki/Learning_by_teaching,"In the field of pedagogy, learning by teaching (German: Lernen durch Lehren, short LdL) is a method of teaching in which students are made to learn material and prepare lessons to teach it to the other students. There is a strong emphasis on acquisition of life skills along with the subject matter. This method was originally defined by Jean-Pol Martin in the 1980s.",4617149068047999266,related_concept,Active learning,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.6857142857142855,2.271081882247352e-304
114,259,Passive learning,http://dbpedia.org/resource/Passive_learning,http://en.wikipedia.org/wiki/Passive_learning,"Passive learning is a method of learning or instruction where students receive information from the instructor and internalize it. It is a method ""where the learner receives no feedback from the instructor"". The term is often used together with direct instruction and lecturing, with passive learning being the result or intended outcome of the instruction. This style of learning is teacher-centered and contrasts to active learning, which is student-centered, whereby students take an active or participatory role in the learning process, and to the Socratic method where students and instructors engage in cooperative argumentative dialogue. Passive learning is a traditional method utilized in factory model schools and modern schools, as well as historic and contemporary religious services in churches (sermons), mosques, and synagogues. Passive learning is not simply the outcome of an educational model. Passive learners may quietly absorb information and knowledge without typically engaging with the information received or the learning experience. They may not interact with others, share insights, or contribute to a dialogue. An estimated 60 percent of people are passive learners.",7319668335577788988,related_concept,Active learning,2024-06-23 21:17:09.728,['Passive learning'],['Passive learning'],set(),set(),0,0.5560859188544153,2.2704439077868566e-304
115,260,Organizational learning,http://dbpedia.org/resource/Organizational_learning,http://en.wikipedia.org/wiki/Organizational_learning,"Organizational learning is the process of creating, retaining, and transferring knowledge within an organization. An organization improves over time as it gains experience. From this experience, it is able to create knowledge. This knowledge is broad, covering any topic that could better an organization. Examples may include ways to increase production efficiency or to develop beneficial investor relations. Knowledge is created at four different units: individual, group, organizational, and inter organizational. The most common way to measure organizational learning is a learning curve. Learning curves are a relationship showing how as an organization produces more of a product or service, it increases its productivity, efficiency, reliability and/or quality of production with diminishing returns. Learning curves vary due to organizational learning rates. Organizational learning rates are affected by individual proficiency, improvements in an organization's technology, and improvements in the structures, routines and methods of coordination.",1398908238025859393,related_concept,Active learning,2024-06-23 21:17:09.728,['Organizational learning'],"['Organizational learning', 'Knowledge management', 'Data', 'Dimension']",set(),set(),0,1.6369863013698631,0.9705633321398978
116,261,Learning cell,http://dbpedia.org/resource/Learning_cell,http://en.wikipedia.org/wiki/Learning_cell,A learning cell is a learning strategy for a pair of students to learn together. It is an active learning style. A learning cell is a process of learning where two students alternate asking and answering questions on commonly read materials.,5483792928913196425,related_concept,Active learning,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.42857142857142855,2.271480682943437e-304
117,262,Active learning,http://dbpedia.org/resource/Active_learning,http://en.wikipedia.org/wiki/Active_learning,"Active learning is ""a method of learning in which students are actively or experientially involved in the learning process and where there are different levels of active learning, depending on student involvement."" states that ""students participate [in active learning] when they are doing something besides passively listening."" According to Hanson and Moser (2003) using active teaching techniques in the classroom create better academic outcomes for students. Scheyvens, Griffin, Jocoy, Liu, & Bradford (2008) further noted that “by utilizing learning strategies that can include small-group work, role-play and simulations, data collection and analysis, active learning is purported to increase student interest and motivation and to build students ‘critical thinking, problem-solving and social skills”. In a report from the Association for the Study of Higher Education (ASHE), authors discuss a variety of methodologies for promoting active learning. They cite literature that indicates students must do more than just listen in order to learn. They must read, write, discuss, and be engaged in solving problems. This process relates to the three learning domains referred to as knowledge, skills and attitudes (KSA). This taxonomy of learning behaviors can be thought of as ""the goals of the learning process."" In particular, students must engage in such higher-order thinking tasks as analysis, synthesis, and evaluation.",6831167033031937311,main_concept,,2024-06-23 21:17:09.728,['Active learning'],['Active learning'],set(),set(),0,0.8895463510848126,0.5157952997253936
118,264,Probit,http://dbpedia.org/resource/Probit,http://en.wikipedia.org/wiki/Probit,"In probability theory and statistics, the probit function is the quantile function associated with the standard normal distribution. It has applications in data analysis and machine learning, in particular exploratory statistical graphics and specialized regression modeling of binary response variables. Mathematically, the probit is the inverse of the cumulative distribution function of the standard normal distribution, which is denoted as , so the probit is defined as . Largely because of the central limit theorem, the standard normal distribution plays a fundamental role in probability theory and statistics. If we consider the familiar fact that the standard normal distribution places 95% of probability between −1.96 and 1.96, and is symmetric around zero, it follows that The probit function gives the 'inverse' computation, generating a value of a standard normal random variable, associated with specified cumulative probability. Continuing the example, . In general, and",7395690063291052434,related_concept,Histogram,2024-06-23 21:17:09.728,[],"['Probit', 'Q–Q plot', 'MATLAB']",set(),set(),0,1.6938775510204083,1.33633257599096
119,266,Binomial distribution,http://dbpedia.org/resource/Binomial_distribution,http://en.wikipedia.org/wiki/Binomial_distribution,"In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability ). A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance. The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for N much larger than n, the binomial distribution remains a good approximation, and is widely used.",8388110037448034139,related_concept,Histogram,2024-06-23 21:17:09.728,['Bernoulli distribution'],"['Bernoulli distribution', 'Beta distribution', 'Jeffreys prior', 'Poisson distribution', 'Bayesian inference']",set(),set(),0,2.6218487394957983,1.314539391805263
120,267,Image histogram,http://dbpedia.org/resource/Image_histogram,http://en.wikipedia.org/wiki/Image_histogram,"An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance. Image histograms are present on many modern services. Photographers can use them as an aid to show the distribution of tones captured, and whether image detail has been lost to blown-out highlights or blacked-out shadows. This is less useful when using a raw image format, as the dynamic range of the displayed image may only be an approximation to that in the raw file. The horizontal axis of the graph represents the tonal variations, while the vertical axis represents the total number of pixels in that particular tone. The left side of the horizontal axis represents the dark areas, the middle represents mid-tone values and the right hand side represents light areas. The vertical axis represents the size of the area (total number of pixels) that is captured in each one of these zones. Thus, the histogram for a very dark image will have most of its data points on the left side and center of the graph. Conversely, the histogram for a very bright image with few dark areas and/or shadows will have most of its data points on the right side and center of the graph.",5560321177751904406,related_concept,Histogram,2024-06-23 21:17:09.728,['Image histogram'],"['Image histogram', 'Histogram', 'Algorithm']",set(),set(),0,2.875,1.426936893913796
121,269,Data binning,http://dbpedia.org/resource/Data_binning,http://en.wikipedia.org/wiki/Data_binning,"Data binning, also called data discrete binning or data bucketing, is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall into a given small interval, a bin, are replaced by a value representative of that interval, often a central value (mean or median). It is related to quantization: data binning operates on the abscissa axis while quantization operates on the ordinate axis. Binning is a generalization of rounding. Statistical data binning is a way to group numbers of more-or-less continuous values into a smaller number of ""bins"". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals (for example, grouping every five years together). It can also be used in multivariate statistics, binning in several dimensions at once. In digital image processing, ""binning"" has a very different meaning. Pixel binning is the process of combining blocks of adjacent pixels throughout an image, by summing or averaging their values, during or after readout. It reduces the amount of data; also the relative noise level in the result is lower.",4577566165176932353,related_concept,Histogram,2024-06-23 21:17:09.728,"['Data binning', 'Data']","['Data binning', 'Data', 'Histogram', 'Gradient', 'Classification']",set(),set(),0,1.5853658536585367,1.420682834470634
122,270,Color histogram,http://dbpedia.org/resource/Color_histogram,http://en.wikipedia.org/wiki/Color_histogram,"In image processing and photography, a color histogram is a representation of the distribution of colors in an image. For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image's color space, the set of all possible colors. The color histogram can be built for any kind of color space, although the term is more often used for three-dimensional spaces like RGB or HSV. For monochromatic images, the term intensity histogram may be used instead. For multi-spectral images, where each pixel is represented by an arbitrary number of measurements (for example, beyond the three measurements in RGB), the color histogram is N-dimensional, with N being the number of measurements taken. Each measurement has its own wavelength range of the light spectrum, some of which may be outside the visible spectrum. If the set of possible color values is sufficiently small, each of those colors may be placed on a range by itself; then the histogram is merely the count of pixels that have each possible color. Most often, the space is divided into an appropriate number of ranges, often arranged as a regular grid, each containing many similar color values. The color histogram may also be represented and displayed as a smooth function defined over the color space that approximates the pixel counts. Like other kinds of histograms, the color histogram is a statistic that can be viewed as an approximation of an underlying continuous distribution of colors values.",6160232415863931973,related_concept,Histogram,2024-06-23 21:17:09.728,[],"['Color histogram', 'Histogram', 'RGB color space', 'Euclidean distance']",set(),set(),0,1.206896551724138,1.4261992825296872
123,271,Entropy estimation,http://dbpedia.org/resource/Entropy_estimation,http://en.wikipedia.org/wiki/Entropy_estimation,"In various science/engineering applications, such as independent component analysis, image analysis, genetic analysis, speech recognition, manifold learning, and time delay estimation it is useful to estimate the differential entropy of a system or process, given some observations. The simplest and most common approach uses histogram-based estimation, but other approaches have been developed and used, each with its own benefits and drawbacks. The main factor in choosing a method is often a trade-off between the bias and the variance of the estimate, although the nature of the (suspected) distribution of the data may also be a factor.",8668472659972753151,related_concept,Histogram,2024-06-23 21:17:09.728,[],"['Histogram', 'Bayesian estimator']",set(),set(),0,1.0,1.2993911486728187
124,276,Frequency distribution,http://dbpedia.org/resource/Frequency_distribution,http://en.wikipedia.org/wiki/Frequency_distribution,,4617926927073673171,related_concept,Histogram,2024-06-23 21:17:09.728,[],"['Frequency distribution', 'Statistical hypothesis testing']",set(),set(),0,1.7849162011173185,0.4366809487311694
125,278,Normalization (statistics),http://dbpedia.org/resource/Normalization_(statistics),http://en.wikipedia.org/wiki/Normalization_(statistics),"In statistics and applications of statistics, normalization can have a range of meanings. In the simplest cases, normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted values into alignment. In the case of normalization of scores in educational assessment, there may be an intention to align distributions to a normal distribution. A different approach to normalization of probability distributions is quantile normalization, where the quantiles of the different measures are brought into alignment. In another usage in statistics, normalization refers to the creation of shifted and scaled versions of statistics, where the intention is that these normalized values allow the comparison of corresponding normalized values for different datasets in a way that eliminates the effects of certain gross influences, as in an anomaly time series. Some types of normalization involve only a rescaling, to arrive at values relative to some size variable. In terms of levels of measurement, such ratios only make sense for ratio measurements (where ratios of measurements are meaningful), not interval measurements (where only distances are meaningful, but not ratios). In theoretical statistics, parametric normalization can often lead to pivotal quantities – functions whose sampling distribution does not depend on the parameters – and to ancillary statistics – pivotal quantities that can be computed from observations, without knowing parameters.",1541411853763246694,related_concept,Histogram,2024-06-23 21:17:09.728,[],[],set(),set(),0,7.71875,1.2925982118994737
126,280,Freedman–Diaconis rule,http://dbpedia.org/resource/Freedman–Diaconis_rule,http://en.wikipedia.org/wiki/Freedman–Diaconis_rule,"In statistics, the Freedman–Diaconis rule can be used to select the width of the bins to be used in a histogram. It is named after David A. Freedman and Persi Diaconis. For a set of empirical measurements sampled from some probability distribution, the Freedman-Diaconis rule is designed roughly to minimize the integral of the squared difference between the histogram (i.e., relative frequency density) and the density of the theoretical probability distribution. The general equation for the rule is: where is the interquartile range of the data and is the number of observations in the sample",3293572732983242261,related_concept,Histogram,2024-06-23 21:17:09.728,[],"['Freedman–Diaconis rule', 'Mean', 'Normal distribution', 'Theorem']",set(),set(),0,1.3043478260869565,1.4312226707745217
127,281,Relative frequency,http://dbpedia.org/resource/Relative_frequency,http://en.wikipedia.org/wiki/Relative_frequency,,6582409216298001424,related_concept,Histogram,2024-06-23 21:17:09.728,[],"['Bayesian statistics', 'Bayesian inference']",set(),set(),0,0.6451612903225806,0.22206421890823688
128,283,V-optimal histograms,http://dbpedia.org/resource/V-optimal_histograms,http://en.wikipedia.org/wiki/V-optimal_histograms,"Histograms are most commonly used as visual representations of data. However, Database systems use histograms to summarize data internally and provide size estimates for queries. These histograms are not presented to users or displayed visually, so a wider range of options are available for their construction. Simple or exotic histograms are defined by four parameters, Sort Value, Source Value, Partition Class and Partition Rule. The most basic histogram is the equi-width histogram, where each bucket represents the same range of values. That histogram would be defined as having a Sort Value of Value, a Source Value of Frequency, be in the Serial Partition Class and have a Partition Rule stating that all buckets have the same range. V-optimal histograms are an example of a more ""exotic"" histogram. V-optimality is a Partition Rule which states that the bucket boundaries are to be placed as to minimize the cumulative weighted variance of the buckets. Implementation of this rule is a complex problem and construction of these histograms is also a complex process.",1859887881805892264,related_concept,Histogram,2024-06-23 21:17:09.728,"['V-optimal histograms', 'Histogram', 'Database', 'Data']","['Histogram', 'Database', 'Data', 'V-optimal histograms', 'Variance', 'Iterative', 'Algorithm']",set(),set(),0,2.75,1.4274043940265657
129,284,Histogram,http://dbpedia.org/resource/Histogram,http://en.wikipedia.org/wiki/Histogram,"A histogram is an approximate representation of the distribution of numerical data. The term was first introduced by Karl Pearson. To construct a histogram, the first step is to ""bin"" (or ""bucket"") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent and are often (but not required to be) of equal size. If the bins are of equal size, a bar is drawn over the bin with height proportional to the frequency—the number of cases in each bin. A histogram may also be normalized to display ""relative"" frequencies showing the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density—the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.",1852080233997018285,main_concept,,2024-06-23 21:17:09.728,['Histogram'],"['Histogram', 'Freedman–Diaconis rule']",set(),set(),0,2.631868131868132,1.4294602340368316
130,285,Domain of a function,http://dbpedia.org/resource/Domain_of_a_function,http://en.wikipedia.org/wiki/Domain_of_a_function,"In mathematics, the domain of a function is the set of inputs accepted by the function. It is sometimes denoted by or , where f is the function. More precisely, given a function , the domain of f is X. Note that in modern mathematical language, the domain is part of the definition of a function rather than a property of it. In the special case that X and Y are both subsets of , the function f can be graphed in the Cartesian coordinate system. In this case, the domain is represented on the x-axis of the graph, as the projection of the graph of the function onto the x-axis. For a function , the set Y is called the codomain, and the set of values attained by the function (which is a subset of Y) is called its range or image. Any function can be restricted to a subset of its domain. The restriction of to , where , is written as .",8504744642213294939,related_concept,Local optimum,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.718543046357616,1.397940573482469
131,286,Neighbourhood (mathematics),http://dbpedia.org/resource/Neighbourhood_(mathematics),http://en.wikipedia.org/wiki/Neighbourhood_(mathematics),"In topology and related areas of mathematics, a neighbourhood (or neighborhood) is one of the basic concepts in a topological space. It is closely related to the concepts of open set and interior. Intuitively speaking, a neighbourhood of a point is a set of points containing that point where one can move some amount in any direction away from that point without leaving the set.",1569496216680510551,related_concept,Local optimum,2024-06-23 21:17:09.728,[],[],set(),set(),0,8.714285714285714,1.3932034662969
132,287,Applied mathematics,http://dbpedia.org/resource/Applied_mathematics,http://en.wikipedia.org/wiki/Applied_mathematics,"Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, finance, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term ""applied mathematics"" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models. In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.",8819586693319232707,related_concept,Local optimum,2024-06-23 21:17:09.728,['Applied mathematics'],"['Applied mathematics', 'Mathematics', 'Statistics', 'Computer science', 'Operations research', 'Statistical theory', 'Classification']",set(),set(),0,7.359430604982206,1.3998026800349754
133,288,First derivative test,http://dbpedia.org/resource/First_derivative_test,http://en.wikipedia.org/wiki/First_derivative_test,,399252057760946826,related_concept,Local optimum,2024-06-23 21:17:09.728,[],['Theorem'],set(),set(),0,2.8627450980392157,4.236699139027965e-305
134,290,Necessary and sufficient conditions,http://dbpedia.org/resource/Necessary_and_sufficient_conditions,http://en.wikipedia.org/wiki/Necessary_and_sufficient_conditions,,3776407397853367791,related_concept,Local optimum,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1260504201680672,4.2100730621744944e-305
135,291,Bounded set,http://dbpedia.org/resource/Bounded_set,http://en.wikipedia.org/wiki/Bounded_set,"In mathematical analysis and related areas of mathematics, a set is called bounded if it is, in a certain sense, of finite measure. Conversely, a set which is not bounded is called unbounded. The word 'bounded' makes no sense in a general topological space without a corresponding metric.",7961048527307868580,related_concept,Local optimum,2024-06-23 21:17:09.728,[],['Euclidean distance'],set(),set(),0,4.6,1.3769296074908286
136,292,Optimization problem,http://dbpedia.org/resource/Optimization_problem,http://en.wikipedia.org/wiki/Optimization_problem,"In mathematics, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions. Optimization problems can be divided into two categories, depending on whether the variables are continuous or discrete: 
* An optimization problem with discrete variables is known as a discrete optimization, in which an object such as an integer, permutation or graph must be found from a countable set. 
* A problem with continuous variables is known as a continuous optimization, in which an optimal value from a continuous function must be found. They can include constrained problems and multimodal problems.",4780131999942813523,related_concept,Local optimum,2024-06-23 21:17:09.728,['Optimization problem'],['Optimization problem'],set(),set(),0,2.5961538461538463,4.260781728651473e-304
137,293,Calculus,http://dbpedia.org/resource/Calculus,http://en.wikipedia.org/wiki/Calculus,"Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus has widespread uses in science, engineering, and social science.",1069452184151785147,related_concept,Local optimum,2024-06-23 21:17:09.728,['Calculus'],"['Calculus', 'Theorem', 'Mathematics', 'Differential equation', ""Newton's method""]",set(),set(),0,3.8957528957528957,1.2010609624070847
138,294,Iterated local search,http://dbpedia.org/resource/Iterated_local_search,http://en.wikipedia.org/wiki/Iterated_local_search,"Iterated Local Search (ILS) is a term in applied mathematics and computer sciencedefining a modification of local search or hill climbing methods for solving discrete optimization problems. Local search methods can get stuck in a local minimum, where no improving neighbors are available. A simple modification consists of iterating calls to the local search routine, each time starting from a different initial configuration. This is called repeated local search, and implies that the knowledge obtained during the previous local search phases is not used. Learning implies that the previous history, for example the memory about the previously found local minima, is mined to produce better and better starting points for local search. The implicit assumption is that of a clustered distribution of local minima: when minimizing a function, determining good local minima is easier when starting from a local minimum with a low value than when starting from a random point. The only caveat is to avoid confinement in a given attraction basin, so that the kick to transform a local minimizer into the starting point for the next run has to be appropriately strong, but not too strong to avoid reverting to memory-less random restarts. Iterated Local Search is based on building a sequence of locally optimal solutions by: 1. 
* perturbing the current local minimum; 2. 
* applying local search after starting from the modified solution. The perturbation strength has to be sufficient to lead the trajectory to a different attraction basin leading to a different local optimum.",1288303849669170010,related_concept,Local optimum,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.3333333333333333,4.349723789885549e-304
139,295,Basin of attraction,http://dbpedia.org/resource/Basin_of_attraction,http://en.wikipedia.org/wiki/Basin_of_attraction,,1930022519978980796,related_concept,Local optimum,2024-06-23 21:17:09.728,[],"['Dynamical system', ""Newton's method""]",set(),set(),0,0.0771513353115727,0.26001389956297266
140,296,Simulated annealing,http://dbpedia.org/resource/Simulated_annealing,http://en.wikipedia.org/wiki/Simulated_annealing,"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. It is often used when the search space is discrete (for example the traveling salesman problem, the boolean satisfiability problem, protein structure prediction, and job-shop scheduling). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound. The name of the algorithm comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to alter its physical properties. Both are attributes of the material that depend on their thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy.Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail; even though it usually achieves an approximate solution to the global minimum, it could be enough for many practical problems. The problems solved by SA are currently formulated by an objective function of many variables, subject to several constraints. In practice, the constraint can be penalized as part of the objective function. Similar techniques have been independently introduced on several occasions, including Pincus (1970), Khachaturyan et al (1979, 1981), Kirkpatrick, Gelatt and Vecchi (1983), and Cerny (1985). In 1983, this approach was used by Kirkpatrick, Gelatt Jr., Vecchi, for a solution of the traveling salesman problem. They also proposed its current name, simulated annealing. This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions allows for a more extensive search for the global optimal solution. In general, simulated annealing algorithms work as follows. The temperature progressively decreases from an initial positive value to zero. At each time step, the algorithm randomly selects a solution close to the current one, measures its quality, and moves to it according to the temperature-dependent probabilities of selecting better or worse solutions, which during the search respectively remain at 1 (or positive) and decrease toward zero. The simulation can be performed either by a solution of kinetic equations for density functions or by using the stochastic sampling method. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, published by N. Metropolis et al. in 1953.",4839424249380278407,related_concept,Local optimum,2024-06-23 21:17:09.728,['Simulated annealing'],"['Simulated annealing', 'Metaheuristic']",set(),set(),0,4.412844036697248,4.2585018380281204e-304
141,297,Tabu search,http://dbpedia.org/resource/Tabu_search,http://en.wikipedia.org/wiki/Tabu_search,"Tabu search is a metaheuristic search method employing local search methods used for mathematical optimization. It was created by Fred W. Glover in 1986 and formalized in 1989. Local (neighborhood) searches take a potential solution to a problem and check its immediate neighbors (that is, solutions that are similar except for very few minor details) in the hope of finding an improved solution. Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit. Tabu search enhances the performance of local search by relaxing its basic rule. First, at each step worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local minimum). In addition, prohibitions (henceforth the term tabu) are introduced to discourage the search from coming back to previously-visited solutions. The implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, it is marked as ""tabu"" (forbidden) so that the algorithm does not consider that possibility repeatedly.",4824676294925459607,related_concept,Local optimum,2024-06-23 21:17:09.728,['Tabu search'],['Tabu search'],set(),set(),0,2.3333333333333335,0.4503226497621846
142,299,Plateau (mathematics),http://dbpedia.org/resource/Plateau_(mathematics),http://en.wikipedia.org/wiki/Plateau_(mathematics),"A plateau of a function is a part of its domain where the function has constant value. More formally, let U, V be topological spaces. A plateau for a function f: U → V is a path-connected set of points P of U such that for some y we have f (p) = y for all p in P.",1917561407227220166,related_concept,Local optimum,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.25,1.3886527050257338
143,301,Solution space,http://dbpedia.org/resource/Solution_space,http://en.wikipedia.org/wiki/Solution_space,,8259866034509900004,related_concept,Local optimum,2024-06-23 21:17:09.728,[],['Algorithm'],set(),set(),0,0.3541666666666667,0.6697391143869128
144,302,Local search (optimization),http://dbpedia.org/resource/Local_search_(optimization),http://en.wikipedia.org/wiki/Local_search_(optimization),"In computer science, local search is a heuristic method for solving computationally hard optimization problems. Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of candidate solutions. Local search algorithms move from solution to solution in the space of candidate solutions (the search space) by applying local changes, until a solution deemed optimal is found or a time bound is elapsed. Local search algorithms are widely applied to numerous hard computational problems, including problems from computer science (particularly artificial intelligence), mathematics, operations research, engineering, and bioinformatics. Examples of local search algorithms are WalkSAT, the 2-opt algorithm for the Traveling Salesman Problem and the Metropolis–Hastings algorithm.",2217125377928355079,related_concept,Local optimum,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.1780821917808217,4.334926160754662e-304
145,303,Local optimum,http://dbpedia.org/resource/Local_optimum,http://en.wikipedia.org/wiki/Local_optimum,"In applied mathematics and computer science, a local optimum of an optimization problem is a solution that is optimal (either maximal or minimal) within a neighboring set of candidate solutions. This is in contrast to a global optimum, which is the optimal solution among all possible solutions, not just those in a particular neighborhood of values.",9215596551601377764,main_concept,,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.26666666666666666,4.269341988356288e-304
146,304,Bootstrapping (business),http://dbpedia.org/resource/Bootstrapping_(business),http://en.wikipedia.org/wiki/Bootstrapping_(business),,1084339917458014033,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],"['Boot', 'Bootstrapping']",set(),set(),0,0.019696969696969695,0.6730584952637269
147,305,Rudolf Erich Raspe,http://dbpedia.org/resource/Rudolf_Erich_Raspe,http://en.wikipedia.org/wiki/Rudolf_Erich_Raspe,"Rudolf Erich Raspe (March 1736 – 16 November 1794) was a German librarian, writer, and scientist, called by his biographer John Patrick Carswell a ""rogue"". He is best known for his collection of tall tales The Surprising Adventures of Baron Munchausen, also known as Baron Munchausen's Narrative of his Marvellous Travels and Campaigns in Russia, originally a satirical work with political aims.",3999392297248444154,related_concept,Bootstrapping,2024-06-23 21:17:09.728,"['Rudolf Erich Raspe', 'Baron Munchausen']","['Rudolf Erich Raspe', 'Baron Munchausen']",set(),set(),0,0.7403846153846154,1.4345270709310483
148,307,Bootstrapping (finance),http://dbpedia.org/resource/Bootstrapping_(finance),http://en.wikipedia.org/wiki/Bootstrapping_(finance),"In finance, bootstrapping is a method for constructing a (zero-coupon) fixed-income yield curve from the prices of a set of coupon-bearing products, e.g. bonds and swaps. A bootstrapped curve, correspondingly, is one where the prices of the instruments used as an input to the curve, will be an exact output, when these same instruments are valued using this curve.Here, the term structure of spot returns is recovered from the bond yields by solving for them recursively, by forward substitution: this iterative process is called the bootstrap method. The usefulness of bootstrapping is that using only a few carefully selected zero-coupon products, it becomes possible to derive par swap rates (forward and spot) for all maturities given the solved curve.",1425399591034231020,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],['Boot'],set(),set(),0,0.42105263157894735,1.34937496989874
149,308,Compiler,http://dbpedia.org/resource/Compiler,http://en.wikipedia.org/wiki/Compiler,"In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name ""compiler"" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g. assembly language, object code, or machine code) to create an executable program. There are many different types of compilers which produce output in different useful forms. A cross-compiler produces code for a different CPU or operating system than the one on which the cross-compiler itself runs. A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimised compiler for a language. Related software include, a program that translates from a low-level language to a higher level one is a decompiler ; a program that translates between high-level languages, usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. A compiler-compiler is a compiler that produces a compiler (or part of one), often in a generic and reusable way so as to be able to produce many differing compilers. A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness. Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In theory, a programming language can have both a compiler and an interpreter. In practice, programming languages tend to be associated with just one (a compiler or an interpreter).",2544413444538748558,related_concept,Bootstrapping,2024-06-23 21:17:09.728,['Compiler'],"['Compiler', 'Calculus', 'AI']",set(),set(),0,5.9825327510917035,1.3775076000665598
150,309,Boot jack,http://dbpedia.org/resource/Boot_jack,http://en.wikipedia.org/wiki/Boot_jack,"A boot jack, sometimes known as a boot pull, is a small tool that aids in the removal of boots. It consists of a U-shaped mouth that grips the heel of the boot, and a flat area to which weight can be applied with the opposite foot. To operate it, the user places the heel of the boot in the mouth of the jack, stands on the back of the device with the other foot, and pulls his foot free of the front boot. The process is then repeated to remove the other boot. The boot jack has several advantages over the removal of boots by hand. By allowing the wearer to pull his foot straight up and out of the boot, and by using his full body weight to hold the boot in place, far greater leverage and a much more secure grip are possible than can be achieved with the hands. In addition, the wearer is spared the inconvenience of having to bend over or sit down to remove the boots, or directly handle them if they are dirty. The function of the boot jack can be approximated with a variety of other objects that may be on hand, ranging from a convenient piece of furniture to a rifle butt, but these generally cannot remove the boot as easily as a proper boot jack. An adequate naturally occurring bootjack is formed by the base of cabbage palm Sabal palmetto leaf and these leaf bases are consequently called bootjacks. Additionally, the sole of a boot still being worn can also function as an improvised jack, but the wearer using one foot to remove the opposite boot often lacks proper leverage to successfully remove a snug-fitting boot, particularly a tall boot. In addition to simple, utilitarian models made of wood or a synthetic material, representational cast iron boot jacks are also available. The U shape of the jack is formed by artistic elements, such as the horns of a steer, antennae of an insect or snail, or other, often humorous or whimsical, designs.",8934673276238747100,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.0,1.361868335298063
151,310,Intelligence explosion,http://dbpedia.org/resource/Intelligence_explosion,http://en.wikipedia.org/wiki/Intelligence_explosion,,7604794697855641421,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],"['AI', 'Evolution']",set(),set(),0,0.22410865874363328,4.2243258790720065e-305
152,311,Econometric,http://dbpedia.org/resource/Econometric,http://en.wikipedia.org/wiki/Econometric,,7075160377800957546,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],"['Econometric', 'Estimator', 'Bayesian statistics', 'Ordinary least squares']",set(),set(),0,0.30793650793650795,0.1365479170874286
153,312,Baron Munchausen,http://dbpedia.org/resource/Baron_Munchausen,http://en.wikipedia.org/wiki/Baron_Munchausen,"Baron Munchausen (/ˈmʌntʃaʊzən, ˈmʊntʃ-/; German: [ˈmʏnçˌhaʊzn̩]) is a fictional German nobleman created by the German writer Rudolf Erich Raspe in his 1785 book Baron Munchausen's Narrative of his Marvellous Travels and Campaigns in Russia. The character is loosely based on a real baron, Hieronymus Karl Friedrich, Freiherr von Münchhausen. Born in Bodenwerder, Electorate of Hanover, the real-life Münchhausen fought for the Russian Empire in the Russo-Turkish War of 1735–1739. Upon retiring in 1760, he became a minor celebrity within German aristocratic circles for telling outrageous tall tales based on his military career. After hearing some of Münchhausen's stories, Raspe adapted them anonymously into literary form, first in German as ephemeral magazine pieces and then in English as the 1785 book, which was first published in Oxford by a bookseller named Smith. The book was soon translated into other European languages, including a German version expanded by the poet Gottfried August Bürger. The real-life Münchhausen was deeply upset at the development of a fictional character bearing his name, and threatened legal proceedings against the book's publisher. Perhaps fearing a libel suit, Raspe never acknowledged his authorship of the work, which was only established posthumously. The fictional Baron's exploits, narrated in the first person, focus on his impossible achievements as a sportsman, soldier, and traveller; for instance: riding on a cannonball, fighting a forty-foot crocodile, and travelling to the Moon. Intentionally comedic, the stories play on the absurdity and inconsistency of Munchausen's claims, and contain an undercurrent of social satire. The earliest illustrations of the character, perhaps created by Raspe himself, depict Munchausen as slim and youthful, although later illustrators have depicted him as an older man, and have added the sharply beaked nose and twirled moustache that have become part of the character's definitive visual representation. Raspe's book was a major international success, becoming the core text for numerous English, continental European, and American editions that were expanded and rewritten by other writers. The book in its various revised forms remained widely read throughout the 19th century, especially in editions for young readers. Versions of the fictional Baron have appeared on stage, screen, radio, and television, as well as in other literary works. Though the Baron Munchausen stories are no longer well-known in many English-speaking countries, they are still popular in continental Europe. The character has inspired numerous memorials and museums, and several medical conditions and other concepts are named after him.",4446680796011832188,related_concept,Bootstrapping,2024-06-23 21:17:09.728,"['Rudolf Erich Raspe', 'Baron Munchausen']","['Rudolf Erich Raspe', 'Baron Munchausen']",set(),set(),0,1.1654676258992807,1.4345387942700512
154,313,High-level programming language,http://dbpedia.org/resource/High-level_programming_language,http://en.wikipedia.org/wiki/High-level_programming_language,"In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language. The amount of abstraction provided defines how ""high-level"" a programming language is. In the 1960s, a high-level programming language using a compiler was commonly called an autocode.Examples of autocodes are COBOL and Fortran. The first high-level programming language designed for computers was Plankalkül, created by Konrad Zuse. However, it was not implemented in his time, and his original contributions were largely isolated from other developments due to World War II, aside from the language's influence on the ""Superplan"" language by Heinz Rutishauser and also to some degree ALGOL. The first significantly widespread high-level language was Fortran, a machine-independent development of IBM's earlier Autocode systems. The ALGOL family, with ALGOL 58 defined in 1958 and ALGOL 60 defined in 1960 by committees of European and American computer scientists, introduced recursion as well as nested functions under lexical scope. ALGOL 60 was also the first language with a clear distinction between value and name-parameters and their corresponding semantics. ALGOL also introduced several structured programming concepts, such as the while-do and if-then-else constructs and its syntax was the first to be described in formal notation – Backus–Naur form (BNF). During roughly the same period, COBOL introduced records (also called structs) and Lisp introduced a fully general lambda abstraction in a programming language for the first time.",884494554217314368,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.811475409836065,1.3730119138377763
155,314,Pseudorandom number generator,http://dbpedia.org/resource/Pseudorandom_number_generator,http://en.wikipedia.org/wiki/Pseudorandom_number_generator,"A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by an initial value, called the PRNG's seed (which may include truly random values). Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility. PRNGs are central in applications such as simulations (e.g. for the Monte Carlo method), electronic games (e.g. for procedural generation), and cryptography. Cryptographic applications require the output not to be predictable from earlier outputs, and more elaborate algorithms, which do not inherit the linearity of simpler PRNGs, are needed. Good statistical properties are a central requirement for the output of a PRNG. In general, careful mathematical analysis is required to have any confidence that a PRNG generates numbers that are sufficiently close to random to suit the intended use. John von Neumann cautioned about the misinterpretation of a PRNG as a truly random generator, joking that ""Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.""",5243854083774325274,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.5546218487394956,1.09000216369341
156,315,Language acquisition,http://dbpedia.org/resource/Language_acquisition,http://en.wikipedia.org/wiki/Language_acquisition,"Language acquisition is the process by which humans acquire the capacity to perceive and comprehend language (in other words, gain the ability to be aware of language and to understand it), as well as to produce and use words and sentences to communicate. Language acquisition involves structures, rules and representation. The capacity to use language successfully requires one to acquire a range of tools including phonology, morphology, syntax, semantics, and an extensive vocabulary. Language can be vocalized as in speech, or manual as in sign. Human language capacity is represented in the brain. Even though human language capacity is finite, one can say and understand an infinite number of sentences, which is based on a syntactic principle called recursion. Evidence suggests that every individual has three recursive mechanisms that allow sentences to go indeterminately. These three mechanisms are: relativization, complementation and coordination. There are two main guiding principles in first-language acquisition: speech perception always precedes speech production, and the gradually evolving system by which a child learns a language is built up one step at a time, beginning with the distinction between individual phonemes. Linguists who are interested in child language acquisition have for many years questioned how language is acquired. Lidz et al. state ""The question of how these structures are acquired, then, is more properly understood as the question of how a learner takes the surface forms in the input and converts them into abstract linguistic rules and representations."" Language acquisition usually refers to first-language acquisition, which studies infants' acquisition of their native language, whether that be spoken language or signed language, though it can also refer to bilingual first language acquisition (BFLA), which refers to an infant's simultaneous acquisition of two native languages. This is distinguished from second-language acquisition, which deals with the acquisition (in both children and adults) of additional languages. In addition to speech, reading and writing a language with an entirely different script compounds the complexities of true foreign language literacy. Language acquisition is one of the quintessential human traits.",3139548911613268875,related_concept,Bootstrapping,2024-06-23 21:17:09.728,['Language acquisition'],"['Language acquisition', 'Evolution']",set(),set(),0,2.389090909090909,1.41982850287518
157,317,Phylogenetic tree,http://dbpedia.org/resource/Phylogenetic_tree,http://en.wikipedia.org/wiki/Phylogenetic_tree,"A phylogenetic tree (also phylogeny or evolutionary tree ) is a branching diagram or a tree showing the evolutionary relationships among various biological species or other entities based upon similarities and differences in their physical or genetic characteristics. All life on Earth is part of a single phylogenetic tree, indicating common ancestry. In a rooted phylogenetic tree, each node with descendants represents the inferred most recent common ancestor of those descendants, and the edge lengths in some trees may be interpreted as time estimates. Each node is called a taxonomic unit. Internal nodes are generally called hypothetical taxonomic units, as they cannot be directly observed. Trees are useful in fields of biology such as bioinformatics, systematics, and phylogenetics. Unrooted trees illustrate only the relatedness of the leaf nodes and do not require the ancestral root to be known or inferred.",1865157892933378620,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],"['Computation', 'Phylogenetic tree', 'UPGMA']",set(),set(),0,6.913725490196079,1.3681119239287378
158,318,Integrated development environment,http://dbpedia.org/resource/Integrated_development_environment,http://en.wikipedia.org/wiki/Integrated_development_environment,"An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools and a debugger. Some IDEs, such as NetBeans and Eclipse, contain the necessary compiler, interpreter, or both; others, such as SharpDevelop and Lazarus, do not. The boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development.",3676178032768868088,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],"['Compiler', 'Integrated development environment', 'Google Search', 'SQL', 'Data']",set(),set(),0,4.6553191489361705,0.4736510015607589
159,319,Idiom,http://dbpedia.org/resource/Idiom,http://en.wikipedia.org/wiki/Idiom,"An idiom is a phrase or expression that typically presents a figurative, non-literal meaning attached to the phrase; but some phrases become figurative idioms while retaining the literal meaning of the phrase. Categorized as formulaic language, an idiom's figurative meaning is different from the literal meaning. Idioms occur frequently in all languages; in English alone there are an estimated twenty-five million idiomatic expressions.",6094015423316324256,related_concept,Bootstrapping,2024-06-23 21:17:09.728,['Idiom'],['Idiom'],set(),set(),0,12.731481481481481,3.6594538358485054e-304
160,321,Computer simulation,http://dbpedia.org/resource/Computer_simulation,http://en.wikipedia.org/wiki/Computer_simulation,"Computer simulation is the process of mathematical modelling, performed on a computer, which is designed to predict the behaviour of, or the outcome of, a real-world or physical system. The reliability of some mathematical models can be determined by comparing their results to the real-world outcomes they aim to predict. Computer simulations have become a useful tool for the mathematical modeling of many natural systems in physics (computational physics), astrophysics, climatology, chemistry, biology and manufacturing, as well as human systems in economics, psychology, social science, health care and engineering. Simulation of a system is represented as the running of the system's model. It can be used to explore and gain new insights into new technology and to estimate the performance of systems too complex for analytical solutions. Computer simulations are realized by running computer programs that can be either small, running almost instantly on small devices, or large-scale programs that run for hours or days on network-based groups of computers. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. In 1997, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around Kuwait, using multiple supercomputers in the DoD High Performance Computer Modernization Program.Other examples include a 1-billion-atom model of material deformation; a 2.64-million-atom model of the complex protein-producing organelle of all living organisms, the ribosome, in 2005;a complete simulation of the life cycle of Mycoplasma genitalium in 2012; and the Blue Brain project at EPFL (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level. Because of the computational cost of simulation, computer experiments are used to perform inference such as uncertainty quantification.",955040853125169833,related_concept,Bootstrapping,2024-06-23 21:17:09.728,['Computer simulation'],['Computer simulation'],set(),set(),0,4.7991967871485945,1.3937239907238397
161,322,Computer technology,http://dbpedia.org/resource/Computer_technology,http://en.wikipedia.org/wiki/Computer_technology,,7156673304245391422,related_concept,Bootstrapping,2024-06-23 21:17:09.728,[],"['Computation', 'Computer science', 'Data science', 'Data', 'Data mining', 'Information system', 'Information technology']",set(),set(),0,0.2802197802197802,0.3678364155152777
162,323,Boot,http://dbpedia.org/resource/Boot,http://en.wikipedia.org/wiki/Boot,"A boot is a type of footwear. Most boots mainly cover the foot and the ankle, while some also cover some part of the lower calf. Some boots extend up the leg, sometimes as far as the knee or even the hip. Most boots have a heel that is clearly distinguishable from the rest of the sole, even if the two are made of one piece. Traditionally made of leather or rubber, modern boots are made from a variety of materials. Boots are worn both for their functionality and for reasons of style and fashion. Functional concerns include: protection of the foot and leg from water, mud, pestilence (infectious disease, insect bites and stings, snake bites), extreme temperatures, sharp or blunt hazards (e.g. work boots may provide steel toes), physical abrasion, corrosive agents, or damaging radiation; ankle support and traction for strenuous activities such as hiking; and durability in harsh conditions (e.g. the underside of combat boots may be reinforced with hobnails). In some cases, the wearing of boots may be required by laws or regulations, such as the regulations in some jurisdictions requiring workers on construction sites to wear steel-toed safety boots. Some uniforms include boots as the regulated footwear. Boots are recommended as well for motorcycle riders. High-top athletic shoes are generally not considered boots, even though they do cover the ankle, primarily due to the absence of a distinct heel. In Britain football (soccer) cleats are also called boots.",4739431754729412649,related_concept,Bootstrapping,2024-06-23 21:17:09.728,['Boot'],['Boot'],set(),set(),0,1.7120253164556962,1.376163067485344
163,324,Bootstrapping,http://dbpedia.org/resource/Bootstrapping,http://en.wikipedia.org/wiki/Bootstrapping,"In general, bootstrapping usually refers to a self-starting process that is supposed to continue or grow without external input.",471724695519075272,main_concept,,2024-06-23 21:17:09.728,[],"['Boot', 'Rudolf Erich Raspe', 'Baron Munchausen', 'Bootstrapping', 'Compiler', 'AI', 'Evolution']",set(),set(),0,2.672566371681416,1.3449416048127
164,325,Optimal design,http://dbpedia.org/resource/Optimal_design,http://en.wikipedia.org/wiki/Optimal_design,"In the design of experiments, optimal designs (or optimum designs) are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith. In the design of experiments for estimating statistical models, optimal designs allow parameters to be estimated without bias and with minimum variance. A non-optimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design. In practical terms, optimal experiments can reduce the costs of experimentation. The optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion, which is related to the variance-matrix of the estimator. Specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments.",1726505065308793566,related_concept,Model selection,2024-06-23 21:17:09.728,[],"['Optimal design', 'Experimental design', 'Fisher information']",set(),set(),0,1.3213530655391121,1.3724728985696286
165,327,Curve fitting,http://dbpedia.org/resource/Curve_fitting,http://en.wikipedia.org/wiki/Curve_fitting,"Curve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a ""smooth"" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data. For linear-algebraic analysis of data, ""fitting"" usually means trying to find the curve that minimizes the vertical (y-axis) displacement of a point from the curve (e.g., ordinary least squares). However, for graphical and image applications, geometric fitting seeks to provide the best visual fit; which usually means trying to minimize the orthogonal distance to the curve (e.g., total least squares), or to otherwise include both axes of displacement of a point from the curve. Geometric fits are not popular because they usually require non-linear and/or iterative calculations, although they have the advantage of a more aesthetic and geometrically accurate result.",1415040518935805042,related_concept,Model selection,2024-06-23 21:17:09.728,['Curve fitting'],['Curve fitting'],{'Interpolation'},set(),0,5.640449438202247,1.2318719892432792
166,330,Consistency (statistics),http://dbpedia.org/resource/Consistency_(statistics),http://en.wikipedia.org/wiki/Consistency_(statistics),"In statistics, consistency of procedures, such as computing confidence intervals or conducting hypothesis tests, is a desired property of their behaviour as the number of items in the data set to which they are applied increases indefinitely. In particular, consistency requires that the outcome of the procedure with unlimited data should identify the underlying truth.Use of the term in statistics derives from Sir Ronald Fisher in 1922. Use of the terms consistency and consistent in statistics is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow.",4119249125344027866,related_concept,Model selection,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.9444444444444446,1.340607491420563
167,331,Decision theory,http://dbpedia.org/resource/Decision_theory,http://en.wikipedia.org/wiki/Decision_theory,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome. There are three branches of decision theory: 1. 
* Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational. 2. 
* Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules. 3. 
* Descriptive decision theory: Analyzes how individuals actually make the decisions that they do. Decision theory is closely related to the field of game theory and is an interdisciplinary topic, studied by economists, mathematicians, data scientists, psychologists, biologists, political and other social scientists, philosophers and computer scientists. Empirical applications of this theory are usually done with the help of statistical and econometric methods.",4247003274828160421,related_concept,Model selection,2024-06-23 21:17:09.728,['Decision theory'],"['Decision theory', 'Heuristic']",set(),set(),0,2.0737327188940093,1.4380240256564212
168,333,Automated machine learning,http://dbpedia.org/resource/Automated_machine_learning,http://en.wikipedia.org/wiki/Automated_machine_learning,"Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.",75448148039473632,related_concept,Model selection,2024-06-23 21:17:09.728,['Automated machine learning'],['Automated machine learning'],set(),set(),0,1.0179856115107915,4.772318432718516e-304
169,338,Identifiability Analysis,http://dbpedia.org/resource/Identifiability_Analysis,http://en.wikipedia.org/wiki/Identifiability_Analysis,,1333159364713261406,related_concept,Model selection,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.14285714285714285,0.5041269860200869
170,339,Grid search,http://dbpedia.org/resource/Grid_search,http://en.wikipedia.org/wiki/Grid_search,,1149459645297702470,related_concept,Model selection,2024-06-23 21:17:09.728,[],"['Hyperparameter', 'Grid search', 'Bayesian optimization', 'Evolution']",set(),set(),0,0.2011173184357542,0.5175565023978405
171,341,Focused information criterion,http://dbpedia.org/resource/Focused_information_criterion,http://en.wikipedia.org/wiki/Focused_information_criterion,"In statistics, the focused information criterion (FIC) is a method for selecting the most appropriate model among a set of competitors for a given data set. Unlike most other model selection strategies, like the Akaike information criterion (AIC), the Bayesian information criterion (BIC) and the deviance information criterion (DIC), the FIC does not attempt to assess the overall fit of candidate models but focuses attention directly on the parameter of primary interest with the statistical analysis, say , for which competing models lead to different estimates, say for model . The FIC method consists in first developing an exact or approximate expression for the precision or quality of each estimator, say for , and then use data to estimate these precision measures, say . In the end the model with best estimated precision is selected. The FIC methodology was developed by Gerda Claeskens and Nils Lid Hjort, first in two 2003 discussion articles in Journal of the American Statistical Association and later on in other papers and in their 2008 book. The concrete formulae and implementation for FIC depend firstly on the particular parameter of interest, the choice of which does not depend on mathematics but on the scientific and statistical context. Thus the FIC apparatus may be selecting one model as most appropriate for estimating a quantile of a distribution but preferring another model as best for estimating the mean value. Secondly, the FIC formulae depend on the specifics of the models used for the observed data and also on how precision is to be measured. The clearest case is where precision is taken to be mean squared error, say in terms of squared bias and variance for the estimator associated with model . FIC formulae are then available in a variety of situations, both for handling parametric, semiparametric and nonparametric situations, involving separate estimation of squared bias and variance, leading to estimated precision . In the end the FIC selects the model with smallest estimated mean squared error. Associated with the use of the FIC for selecting a good model is the FIC plot, designed to give a clear and informative picture of all estimates, across all candidate models, and their merit. It displays estimates on the axis along with FIC scores on the axis; thus estimates found to the left in the plot are associated with the better models and those found in the middle and to the right stem from models less or not adequate for the purpose of estimating the focus parameter in question. Generally speaking, complex models (with many parameters relative to sample size) tend to lead to estimators with small bias but high variance; more parsimonious models (with fewer parameters) typically yield estimators with larger bias but smaller variance. The FIC method balances the two desired data of having small bias and small variance in an optimal fashion. The main difficulty lies with the bias , as it involves the distance from the expected value of the estimator to the true underlying quantity to be estimated, and the true data generating mechanism may lie outside each of the candidate models. In situations where there is not a unique focus parameter, but rather a family of such, there are versions of average FIC (AFIC or wFIC) that find the best model in terms of suitably weighted performance measures, e.g. when searching for a regression model to perform particularly well in a portion of the covariate space. It is also possible to keep several of the best models on board, ending the statistical analysis with a data-dicated weighted average of the estimators of the best FIC scores, typically giving highest weight to estimators associated with the best FIC scores. Such schemes of model averaging extend the direct FIC selection method. The FIC methodology applies in particular to selection of variables in different forms of regression analysis, including the framework of generalised linear models and the semiparametric proportional hazards models (i.e. Cox regression).",6688705584395975139,related_concept,Model selection,2024-06-23 21:17:09.728,"['Akaike information criterion', 'AI']","['Akaike information criterion', 'AI']",set(),set(),0,0.6571428571428571,1.3979966632014242
172,346,Binary variable,http://dbpedia.org/resource/Binary_variable,http://en.wikipedia.org/wiki/Binary_variable,,4261070315512943526,related_concept,Discretization,2024-06-23 21:17:09.728,[],"['Boolean function', 'Bernoulli distribution', 'Regression analysis', 'Overdispersion']",set(),set(),0,0.25,0.5140579221119386
173,347,Discrete mathematics,http://dbpedia.org/resource/Discrete_mathematics,http://en.wikipedia.org/wiki/Discrete_mathematics,"Discrete mathematics is the study of mathematical structures that can be considered ""discrete"" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than ""continuous"" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic. By contrast, discrete mathematics excludes topics in ""continuous mathematics"" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term ""discrete mathematics"". The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business. Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in ""discrete"" steps and store data in ""discrete"" bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems. Although the main objects of study in discrete mathematics are discrete objects, analytic methods from ""continuous"" mathematics are often employed as well. In university curricula, ""Discrete Mathematics"" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well. Some high-school-level discrete mathematics textbooks have appeared as well. At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect. The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.",6598965981542010493,related_concept,Discretization,2024-06-23 21:17:09.728,"['Discrete mathematics', 'Euclidean geometry', 'Mathematics']","['Discrete mathematics', 'Euclidean geometry', 'Computation', 'Numerical analysis', 'Discretization']",set(),set(),0,2.4473684210526314,1.3325623542441227
174,348,Numerical analysis,http://dbpedia.org/resource/Numerical_analysis,http://en.wikipedia.org/wiki/Numerical_analysis,"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). It is the study of numerical methods that attempt at finding approximate solutions of problems rather than the exact ones. Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences, medicine, business and even the arts. Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology. Before modern computers, numerical methods often relied on hand interpolation formulas, using data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas continue to be used in software algorithms. The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Numerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used.",2953594974049746677,related_concept,Discretization,2024-06-23 21:17:09.728,['Numerical analysis'],"['Numerical analysis', ""Euler's method"", ""Newton's method"", 'Mathematics', 'Iterative', 'Iterative method', 'Interpolation', 'Differential equation', 'Root-finding algorithm', 'Linearization', 'Optimization problem', 'Partial differential equation', 'Statistics', 'MATLAB']",set(),set(),0,3.7142857142857144,1.115178283276038
175,349,Finite difference method,http://dbpedia.org/resource/Finite_difference_method,http://en.wikipedia.org/wiki/Finite_difference_method,"In numerical analysis, finite-difference methods (FDM) are a class of numerical techniques for solving differential equations by approximating derivatives with finite differences. Both the spatial domain and time interval (if applicable) are discretized, or broken into a finite number of steps, and the value of the solution at these discrete points is approximated by solving algebraic equations containing finite differences and values from nearby points. Finite difference methods convert ordinary differential equations (ODE) or partial differential equations (PDE), which may be nonlinear, into a system of linear equations that can be solved by matrix algebra techniques. Modern computers can perform these linear algebra computations efficiently which, along with their relative ease of implementation, has led to the widespread use of FDM in modern numerical analysis.Today, FDM are one of the most common approaches to the numerical solution of PDE, along with finite element methods.",8644295684312400736,related_concept,Discretization,2024-06-23 21:17:09.728,['Finite difference method'],"['Finite difference method', 'Consistency']",set(),set(),0,1.755868544600939,1.2203658845712033
176,350,Smoothness,http://dbpedia.org/resource/Smoothness,http://en.wikipedia.org/wiki/Smoothness,"In mathematical analysis, the smoothness of a function is a property measured by the number of continuous derivatives it has over some domain, called differentiability class. At the very minimum, a function could be considered smooth if it is differentiable everywhere (hence continuous). At the other end, it might also possess derivatives of all orders in its domain, in which case it is said to be infinitely differentiable and referred to as a C-infinity function (or function).",5879214891823598895,related_concept,Discretization,2024-06-23 21:17:09.728,[],['Smoothness'],set(),set(),0,1.7201834862385321,1.2555815557459464
177,351,Bandlimiting,http://dbpedia.org/resource/Bandlimiting,http://en.wikipedia.org/wiki/Bandlimiting,"Bandlimiting is the limiting of a signal's frequency domain representation or spectral density to zero above a certain finite frequency. A band-limited signal is one whose Fourier transform or spectral density has bounded support. A bandlimited signal may be either random (stochastic) or non-random (deterministic). In general, infinitely many terms are required in a continuous Fourier series representation of a signal, but if a finite number of Fourier series terms can be calculated from that signal, that signal is considered to be band-limited.",2902779931934155772,related_concept,Discretization,2024-06-23 21:17:09.728,['Bandlimiting'],['Bandlimiting'],set(),set(),0,0.94,1.1561179291813573
178,352,State space (controls),http://dbpedia.org/resource/State_space_(controls),http://en.wikipedia.org/wiki/State_space_(controls),,2042634181759199126,related_concept,Discretization,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.3561643835616438,0.46239967313631825
179,353,White noise,http://dbpedia.org/resource/White_noise,http://en.wikipedia.org/wiki/White_noise,"In signal processing, white noise is a random signal having equal intensity at different frequencies, giving it a constant power spectral density. The term is used, with this or similar meanings, in many scientific and technical disciplines, including physics, acoustical engineering, telecommunications, and statistical forecasting. White noise refers to a statistical model for signals and signal sources, rather than to any specific signal. White noise draws its name from white light, although light that appears white generally does not have a flat power spectral density over the visible band. In discrete time, white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance; a single realization of white noise is a random shock. Depending on the context, one may also require that the samples be independent and have identical probability distribution (in other words independent and identically distributed random variables are the simplest representation of white noise). In particular, if each sample has a normal distribution with zero mean, the signal is said to be additive white Gaussian noise. The samples of a white noise signal may be sequential in time, or arranged along one or more spatial dimensions. In digital image processing, the pixels of a white noise image are typically arranged in a rectangular grid, and are assumed to be independent random variables with uniform probability distribution over some interval. The concept can be defined also for signals spread over more complicated domains, such as a sphere or a torus. An infinite-bandwidth white noise signal is a purely theoretical construction. The bandwidth of white noise is limited in practice by the mechanism of noise generation, by the transmission medium and by finite observation capabilities. Thus, random signals are considered ""white noise"" if they are observed to have a flat spectrum over the range of frequencies that are relevant to the context. For an audio signal, the relevant range is the band of audible sound frequencies (between 20 and 20,000 Hz). Such a signal is heard by the human ear as a hissing sound, resembling the /h/ sound in a sustained aspiration. On the other hand, the ""sh"" sound /ʃ/ in ""ash"" is a colored noise because it has a formant structure. In music and acoustics, the term ""white noise"" may be used for any signal that has a similar hissing sound. The term white noise is sometimes used in the context of phylogenetically based statistical methods to refer to a lack of phylogenetic pattern in comparative data. It is sometimes used analogously in nontechnical contexts to mean ""random talk without meaningful contents"".",5551121385576764086,related_concept,Discretization,2024-06-23 21:17:09.728,['White noise'],"['White noise', 'Hypothesis']",set(),set(),0,2.046979865771812,3.93707457577554e-304
180,354,Granular computing,http://dbpedia.org/resource/Granular_computing,http://en.wikipedia.org/wiki/Granular_computing,"Granular computing (GrC) is an emerging computing paradigm of information processing that concerns the processing of complex information entities called ""information granules"", which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like. At present, granular computing is more a theoretical perspective than a coherent set of methods or principles. As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales. In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented.",6024656842910267916,related_concept,Discretization,2024-06-23 21:17:09.728,['Granular computing'],"['Granular computing', 'SQL', 'Database', 'Data', 'AI']",set(),set(),0,0.7272727272727273,1.2390114613651644
181,356,Discretization error,http://dbpedia.org/resource/Discretization_error,http://en.wikipedia.org/wiki/Discretization_error,"In numerical analysis, computational physics, and simulation, discretization error is the error resulting from the fact that a function of a continuous variable is represented in the computer by a finite number of evaluations, for example, on a lattice. Discretization error can usually be reduced by using a more finely spaced lattice, with an increased computational cost.",831062355394904610,related_concept,Discretization,2024-06-23 21:17:09.728,"['Discretization error', 'Discretization']","['Discretization error', 'Discretization']",set(),set(),0,2.0,1.2272859899545034
182,357,Discrete time and continuous time,http://dbpedia.org/resource/Discrete_time_and_continuous_time,http://en.wikipedia.org/wiki/Discrete_time_and_continuous_time,"In mathematical dynamics, discrete time and continuous time are two alternative frameworks within which variables that evolve over time are modeled.",6222351602515962134,related_concept,Discretization,2024-06-23 21:17:09.728,[],[],set(),set(),0,5.25,4.525086748945973e-304
183,358,Distribution (mathematics),http://dbpedia.org/resource/Distribution_(mathematics),http://en.wikipedia.org/wiki/Distribution_(mathematics),"Distributions, also known as Schwartz distributions or generalized functions, are objects that generalize the classical notion of functions in mathematical analysis. Distributions make it possible to differentiate functions whose derivatives do not exist in the classical sense. In particular, any locally integrable function has a distributional derivative. Distributions are widely used in the theory of partial differential equations, where it may be easier to establish the existence of distributional solutions than classical solutions, or where appropriate classical solutions may not exist. Distributions are also important in physics and engineering where many problems naturally lead to differential equations whose solutions or initial conditions are singular, such as the Dirac delta function. A function is normally thought of as acting on the points in the function domain by ""sending"" a point in its domain to the point Instead of acting on points, distribution theory reinterprets functions such as as acting on test functions in a certain way. In applications to physics and engineering, test functions are usually infinitely differentiable complex-valued (or real-valued) functions with compact support that are defined on some given non-empty open subset . (Bump functions are examples of test functions.) The set of all such test functions forms a vector space that is denoted by or Most commonly encountered functions, including all continuous maps if using can be canonically reinterpreted as acting via ""integration against a test function."" Explicitly, this means that ""acts on"" a test function by ""sending"" it to the number which is often denoted by This new action of is a scalar-valued map, denoted by whose domain is the space of test functions This functional turns out to have the two defining properties of what is known as a distribution on : it is linear and also continuous when is given a certain topology called the canonical LF topology. The action of this distribution on a test function can be interpreted as a weighted average of the distribution on the support of the test function, even if the values of the distribution at a single point are not well-defined. Distributions like that arise from functions in this way are prototypical examples of distributions, but many cannot be defined by integration against any function. Examples of the latter include the Dirac delta function and distributions defined to act by integration of test functions against certain measures. It is nonetheless still possible to down to a simpler family of related distributions that do arise via such actions of integration. More generally, a distribution on is by definition a linear functional on that is continuous when is given a topology called the canonical LF topology. This leads to the space of (all) distributions on , usually denoted by (note the prime), which by definition is the space of all distributions on (that is, it is the continuous dual space of ); it is these distributions that are the main focus of this article. Definitions of the appropriate topologies on spaces of test functions and distributions are given in the article on spaces of test functions and distributions. This article is primarily concerned with the definition of distributions, together with their properties and some important examples.",2311427244631135769,related_concept,Discretization,2024-06-23 21:17:09.728,[],"['Theorem', 'Minkowski sum', 'Differential equation']",set(),set(),0,2.290909090909091,4.697381270446865e-304
184,359,Quantization (signal processing),http://dbpedia.org/resource/Quantization_(signal_processing),http://en.wikipedia.org/wiki/Quantization_(signal_processing),"Quantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements. Rounding and truncation are typical examples of quantization processes. Quantization is involved to some degree in nearly all digital signal processing, as the process of representing a signal in digital form ordinarily involves rounding. Quantization also forms the core of essentially all lossy compression algorithms. The difference between an input value and its quantized value (such as round-off error) is referred to as quantization error. A device or algorithmic function that performs quantization is called a quantizer. An analog-to-digital converter is an example of a quantizer.",7467793857183798926,related_concept,Discretization,2024-06-23 21:17:09.728,[],"['Mean', 'PDF', 'Iterative']",set(),set(),0,1.1684782608695652,1.3646634882390443
185,363,Continuous function,http://dbpedia.org/resource/Continuous_function,http://en.wikipedia.org/wiki/Continuous_function,"In mathematics, a continuous function is a function such that a continuous variation (that is a change without jump) of the argument induces a continuous variation of the value of the function. This means that there are no abrupt changes in value, known as discontinuities. More precisely, a function is continuous if arbitrarily small changes in its value can be assured by restricting to sufficiently small changes of its argument. A discontinuous function is a function that is not continuous. Up until the 19th century, mathematicians largely relied on intuitive notions of continuity, and considered only continuous functions. The epsilon–delta definition of a limit was introduced to formalize the definition of continuity. Continuity is one of the core concepts of calculus and mathematical analysis, where arguments and values of functions are real and complex numbers. The concept has been generalized to functions and . The latter are the most general continuous functions, and their definition is the basis of topology. A stronger form of continuity is uniform continuity. In order theory, especially in domain theory, a related concept of continuity is Scott continuity. As an example, the function H(t) denoting the height of a growing flower at time t would be considered continuous. In contrast, the function M(t) denoting the amount of money in a bank account at time t would be considered discontinuous, since it ""jumps"" at each point in time when money is deposited or withdrawn.",4038783235881308467,related_concept,Discretization,2024-06-23 21:17:09.728,[],"['Smoothness', 'Cauchy sequence', 'Continuous function', 'Theorem', 'Hahn–Banach theorem']",set(),set(),0,3.796766743648961,1.2777840128646627
186,364,Time-scale calculus,http://dbpedia.org/resource/Time-scale_calculus,http://en.wikipedia.org/wiki/Time-scale_calculus,"In mathematics, time-scale calculus is a unification of the theory of difference equations with that of differential equations, unifying integral and differential calculus with the calculus of finite differences, offering a formalism for studying hybrid systems. It has applications in any field that requires simultaneous modelling of discrete and continuous data. It gives a new definition of a derivative such that if one differentiates a function defined on the real numbers then the definition is equivalent to standard differentiation, but if one uses a function defined on the integers then it is equivalent to the forward difference operator.",5818443826177756965,related_concept,Discretization,2024-06-23 21:17:09.728,[],"['Time-scale calculus', 'Partial differential equation', 'Stochastic']",set(),set(),0,1.2307692307692308,4.568999288956456e-304
187,365,Stochastic simulation,http://dbpedia.org/resource/Stochastic_simulation,http://en.wikipedia.org/wiki/Stochastic_simulation,"A stochastic simulation is a simulation of a system that has variables that can change stochastically (randomly) with individual probabilities. Realizations of these random variables are generated and inserted into a model of the system. Outputs of the model are recorded, and then the process is repeated with a new set of random values. These steps are repeated until a sufficient amount of data is gathered. In the end, the distribution of the outputs shows the most probable estimates as well as a frame of expectations regarding what ranges of values the variables are more or less likely to fall in. Often random variables inserted into the model are created on a computer with a random number generator (RNG). The U(0,1) uniform distribution outputs of the random number generator are then transformed into random variables with probability distributions that are used in the system model.",8658211257430692091,related_concept,Discretization,2024-06-23 21:17:09.728,[],"['Stochastic', 'Bernoulli distribution', 'Algorithm', ""Student's t-distribution""]",set(),set(),0,0.953125,1.2156530065662208
188,366,Discretization,http://dbpedia.org/resource/Discretization,http://en.wikipedia.org/wiki/Discretization,"In applied mathematics, discretization is the process of transferring continuous functions, models, variables, and equations into discrete counterparts. This process is usually carried out as a first step toward making them suitable for numerical evaluation and implementation on digital computers. Dichotomization is the special case of discretization in which the number of discrete classes is 2, which can approximate a continuous variable as a binary variable (creating a dichotomy for modeling purposes, as in binary classification). Discretization is also related to discrete mathematics, and is an important component of granular computing. In this context, discretization may also refer to modification of variable or category granularity, as when multiple discrete variables are aggregated or multiple discrete categories fused. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand. The terms discretization and quantization often have the same denotation but not always identical connotations. (Specifically, the two terms share a semantic field.) The same is true of discretization error and quantization error. Mathematical methods relating to discretization include the Euler–Maruyama method and the zero-order hold.",2958300957909270803,main_concept,,2024-06-23 21:17:09.728,['Discretization'],"['Discretization', 'Theorem']",set(),set(),0,3.3968253968253967,1.3587364674670068
189,372,Quantile function,http://dbpedia.org/resource/Quantile_function,http://en.wikipedia.org/wiki/Quantile_function,"In probability and statistics, the quantile function, associated with a probability distribution of a random variable, specifies the value of the random variable such that the probability of the variable being less than or equal to that value equals the given probability. Intuitively, the quantile function associates with a range at and below a probability input the likelihood that a random variable is realized in that range for some probability distribution. It is also called the percentile function, percent-point function or inverse cumulative distribution function.",3787766900884308565,related_concept,Quartile,2024-06-23 21:17:09.728,[],"['Quantile function', 'Quantile', 'Algorithm']",set(),set(),0,2.175675675675676,1.2096645002512376
190,376,Order statistic,http://dbpedia.org/resource/Order_statistic,http://en.wikipedia.org/wiki/Order_statistic,"In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases of the order statistics are the minimum and maximum value of a sample, and (with some qualifications discussed below) the sample median and other sample quantiles. When using probability theory to analyze order statistics of random samples from a continuous distribution, the cumulative distribution function is used to reduce the analysis to the case of order statistics of the uniform distribution.",856146299019518240,related_concept,Quartile,2024-06-23 21:17:09.728,[],"['PDF', 'Cauchy distribution']",set(),set(),0,1.485933503836317,4.31852808304302e-304
191,379,Quantile,http://dbpedia.org/resource/Quantile,http://en.wikipedia.org/wiki/Quantile,"In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups). The groups created are termed halves, thirds, quarters, etc., though sometimes the terms for the quantile are used for the groups created, rather than for the cut points. q-quantiles are values that partition a finite set of values into q subsets of (nearly) equal sizes. There are q − 1 partitions of the q-quantiles, one for each integer k satisfying 0 < k < q. In some cases the value of a quantile may not be uniquely determined, as can be the case for the median (2-quantile) of a uniform probability distribution on a set of even size. Quantiles can also be applied to continuous distributions, providing a way to generalize rank statistics to continuous variables (see percentile rank). When the cumulative distribution function of a random variable is known, the q-quantiles are the application of the quantile function (the inverse function of the cumulative distribution function) to the values {1/q, 2/q, …, (q − 1)/q}.",4390386835190380919,related_concept,Quartile,2024-06-23 21:17:09.728,['Quantile'],"['Quantile', 'Mathematics', 'Normal distribution', 'Algorithm']",set(),set(),0,2.643835616438356,1.1141952168149398
192,387,Quartile,http://dbpedia.org/resource/Quartile,http://en.wikipedia.org/wiki/Quartile,"In statistics, a quartile is a type of quantile which divides the number of data points into four parts, or quarters, of more-or-less equal size. The data must be ordered from smallest to largest to compute quartiles; as such, quartiles are a form of order statistic. The three main quartiles are as follows: 
* The first quartile (Q1) is defined as the middle number between the smallest number (minimum) and the median of the data set. It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point. 
* The second quartile (Q2) is the median of a data set; thus 50% of the data lies below this point. 
* The third quartile (Q3) is the middle value between the median and the highest value (maximum) of the data set. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point. Along with the minimum and maximum of the data (which are also quartiles), the three quartiles described above provide a five-number summary of the data. This summary is important in statistics because it provides information about both the center and the spread of the data. Knowing the lower and upper quartile provides information on how big the spread is and if the dataset is skewed toward one side. Since quartiles divide the number of data points evenly, the range is not the same between quartiles (i.e., Q3-Q2 ≠ Q2-Q1) and is instead known as the interquartile range (IQR). While the maximum and minimum also show the spread of the data, the upper and lower quartiles can provide more detailed information on the location of specific data points, the presence of outliers in the data, and the difference in spread between the middle 50% of the data and the outer data points.",5930085451063033657,main_concept,,2024-06-23 21:17:09.728,[],"['Interquartile range', 'Data', 'Outlier']",set(),set(),0,4.205128205128205,0.8069044388862836
193,388,Euclidean plane,http://dbpedia.org/resource/Euclidean_plane,http://en.wikipedia.org/wiki/Euclidean_plane,"In mathematics, the Euclidean plane is a Euclidean space of dimension two. That is, a geometric setting in which two real quantities are required to determine the position of each point (element of the plane), which includes affine notions of parallel lines, and also metrical notions of distance, circles, and angle measurement. The set of pairs of real numbers (the real coordinate plane) augmented by appropriate structure often serves as the canonical example.",6161769878192442955,related_concept,Linear separability,2024-06-23 21:17:09.728,['Euclidean plane'],"['Euclidean plane', 'Euclidean geometry']",{'Dimension'},set(),0,1.6007067137809188,3.321148449738463e-304
194,394,Boolean function,http://dbpedia.org/resource/Boolean_function,http://en.wikipedia.org/wiki/Boolean_function,"In mathematics, a Boolean function is a function whose arguments and result assume values from a two-element set (usually {true, false}, {0,1} or {-1,1}). Alternative names are switching function, used especially in older computer science literature, and truth function (or logical function), used in logic. Boolean functions are the subject of Boolean algebra and switching theory. A Boolean function takes the form , where is known as the Boolean domain and is a non-negative integer called the arity of the function. In the case where , the function is a constant element of . A Boolean function with multiple outputs, with is a vectorial or vector-valued Boolean function (an S-box in symmetric cryptography). There are different Boolean functions with arguments; equal to the number of different truth tables with entries. Every -ary Boolean function can be expressed as a propositional formula in variables , and two propositional formulas are logically equivalent if and only if they express the same Boolean function.",2479531333622251326,related_concept,Linear separability,2024-06-23 21:17:09.728,['Boolean function'],['Boolean function'],set(),set(),0,1.513157894736842,1.42352196841352
195,395,Collinear,http://dbpedia.org/resource/Collinear,http://en.wikipedia.org/wiki/Collinear,,9137815849898914350,related_concept,Linear separability,2024-06-23 21:17:09.728,[],['Euclidean geometry'],set(),set(),0,0.8879310344827587,0.5106867566594528
196,397,Margin classifier,http://dbpedia.org/resource/Margin_classifier,http://en.wikipedia.org/wiki/Margin_classifier,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example. The notion of margin is important in several machine learning classification algorithms, as it can be used to bound the generalization error of the classifier. These bounds are frequently shown using the VC dimension. Of particular prominence is the generalization error bound on boosting algorithms and support vector machines.",5715306174205428285,related_concept,Linear separability,2024-06-23 21:17:09.728,[],"['BrownBoost', 'AdaBoost']",set(),set(),0,2.789473684210526,1.3957893653307811
197,398,Real number,http://dbpedia.org/resource/Real_number,http://en.wikipedia.org/wiki/Real_number,"In mathematics, a real number is a number that can be used to measure a continuous one-dimensional quantity such as a distance, duration or temperature. Here, continuous means that values can have arbitrarily small variations. Every real number can be almost uniquely represented by an infinite decimal expansion. The real numbers are fundamental in calculus (and more generally in all mathematics), in particular by their role in the classical definitions of limits, continuity and derivatives. The set of real numbers is denoted R or and is sometimes called ""the reals"".The adjective real in this context was introduced in the 17th century by René Descartes to distinguish real numbers, associated with physical reality, from imaginary numbers (such as the square roots of −1), which seemed like a theoretical contrivance unrelated to physical reality. The real numbers include the rational numbers, such as the integer −5 and the fraction 4 / 3. The rest of the real numbers are called irrational numbers, and include algebraic numbers (such as the square root √2 = 1.414...) and transcendental numbers (such as π = 3.1415...). Real numbers can be thought of as all points on an infinitely long line called the number line or real line, where the points corresponding to integers (..., −2, −1, 0, 1, 2, ...) are equally spaced. Conversely, analytic geometry is the association of points on lines (especially axis lines) to real numbers such that geometric displacements are proportional to differences between corresponding numbers. The informal descriptions above of the real numbers are not sufficient for ensuring the correctness of proofs of theorems involving real numbers. The realization that a better definition was needed, and the elaboration of such a definition was a major development of 19th-century mathematics and is the foundation of real analysis, the study of real functions and real-valued sequences. A current axiomatic definition is that real numbers form the unique (up to an isomorphism) Dedekind-complete ordered field. Other common definitions of real numbers include equivalence classes of Cauchy sequences (of rational numbers), Dedekind cuts, and infinite decimal representations. All these definitions satisfy the axiomatic definition and are thus equivalent.",5456911877807935392,related_concept,Linear separability,2024-06-23 21:17:09.728,"['Real number', 'Cauchy sequence']","['Real number', 'Cauchy sequence', 'Axiom', 'Euclidean geometry']",set(),set(),0,8.091787439613526,1.3249812966774037
198,400,Vapnik–Chervonenkis dimension,http://dbpedia.org/resource/Vapnik–Chervonenkis_dimension,http://en.wikipedia.org/wiki/Vapnik–Chervonenkis_dimension,"In Vapnik–Chervonenkis theory, the Vapnik–Chervonenkis (VC) dimension is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a set of functions that can be learned by a statistical binary classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter, which means the algorithm can always learn a perfect classifier for any labeling of at least one configuration of those data points. It was originally defined by Vladimir Vapnik and Alexey Chervonenkis. Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the thresholding of a high-degree polynomial: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This function may not fit the training set well, because it has a low capacity. This notion of capacity is made rigorous below.",1479427039551553863,related_concept,Linear separability,2024-06-23 21:17:09.728,[],[],{'Dimension'},set(),0,0.5471698113207547,1.4090072339786488
199,401,Point (geometry),http://dbpedia.org/resource/Point_(geometry),http://en.wikipedia.org/wiki/Point_(geometry),"In classical Euclidean geometry, a point is a primitive notion that models an exact location in space, and has no length, width, or thickness. In modern mathematics, a point refers more generally to an element of some set called a space. Being a primitive notion means that a point cannot be defined in terms of previously defined objects. That is, a point is defined only by some properties, called axioms, that it must satisfy; for example, ""there is exactly one line that passes through two different points"".",4775740260179616853,related_concept,Linear separability,2024-06-23 21:17:09.728,['Euclidean geometry'],"['Euclidean geometry', 'Euclidean plane']",set(),set(),0,2.0323624595469254,3.288923939741355e-304
200,402,Line (geometry),http://dbpedia.org/resource/Line_(geometry),http://en.wikipedia.org/wiki/Line_(geometry),"In geometry, a line is an infinitely long object with no width, depth, or curvature. Thus, lines are one-dimensional objects, though they may exist in two, three, or higher dimension spaces. The word line may also refer to a line segment in everyday life, which has two points to denote its ends. Lines can be referred by two points that lay on it (e.g., ) or by a single letter (e.g., ). Euclid described a line as ""breadthless length"" which ""lies equally with respect to the points on itself""; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the 19th century (such as non-Euclidean, projective and affine geometry). In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it. When a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry, a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries, a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.",6065233956459166908,related_concept,Linear separability,2024-06-23 21:17:09.728,['Euclidean geometry'],"['Euclidean geometry', 'Euclidean plane', 'Euclidean distance', 'Manhattan distance']",set(),set(),0,2.6297709923664123,3.2956035703554103e-304
201,403,Euclidean geometry,http://dbpedia.org/resource/Euclidean_geometry,http://en.wikipedia.org/wiki/Euclidean_geometry,"Euclidean geometry is a mathematical system attributed to ancient Greek mathematician Euclid, which he described in his textbook on geometry: the Elements. Euclid's approach consists in assuming a small set of intuitively appealing axioms (postulates) and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated earlier, Euclid was the first to organize these propositions into a logical system in which each result is proved from axioms and previously proved theorems. The Elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of mathematical proofs. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory, explained in geometrical language. For more than two thousand years, the adjective ""Euclidean"" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only over short distances (relative to the strength of the gravitational field). Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects. This is in contrast to analytic geometry, introduced almost 2,000 years later by René Descartes, which uses coordinates to express geometric properties as algebraic formulas.",3437902874781564559,related_concept,Linear separability,2024-06-23 21:17:09.728,['Euclidean geometry'],"['Euclidean geometry', 'Geometry']",set(),set(),0,2.724812030075188,0.36695691268687836
202,405,Hypercube,http://dbpedia.org/resource/Hypercube,http://en.wikipedia.org/wiki/Hypercube,"In geometry, a hypercube is an n-dimensional analogue of a square (n = 2) and a cube (n = 3). It is a closed, compact, convex figure whose 1-skeleton consists of groups of opposite parallel line segments aligned in each of the space's dimensions, perpendicular to each other and of the same length. A unit hypercube's longest diagonal in n dimensions is equal to . An n-dimensional hypercube is more commonly referred to as an n-cube or sometimes as an n-dimensional cube. The term measure polytope (originally from Elte, 1912) is also used, notably in the work of H. S. M. Coxeter who also labels the hypercubes the γn polytopes. The hypercube is the special case of a hyperrectangle (also called an n-orthotope). A unit hypercube is a hypercube whose side has length one unit. Often, the hypercube whose corners (or vertices) are the 2n points in Rn with each coordinate equal to 0 or 1 is called the unit hypercube.",2250479655314639452,related_concept,Linear separability,2024-06-23 21:17:09.728,[],['Minkowski sum'],set(),set(),0,3.2857142857142856,0.8392949931169672
203,408,Linear separability,http://dbpedia.org/resource/Linear_separability,http://en.wikipedia.org/wiki/Linear_separability,"In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane. The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept.",838595120701943321,main_concept,,2024-06-23 21:17:09.728,"['Euclidean plane', 'Euclidean geometry']","['Euclidean plane', 'Euclidean geometry', 'Boolean function']",{'Geometry'},set(),0,1.0769230769230769,1.316439198518105
204,410,Knowledge graph,http://dbpedia.org/resource/Knowledge_graph,http://en.wikipedia.org/wiki/Knowledge_graph,"In knowledge representation and reasoning, knowledge graph is a knowledge base that uses a graph-structured data model or topology to integrate data. Knowledge graphs are often used to store interlinked descriptions of entities – objects, events, situations or abstract concepts – while also encoding the semantics underlying the used terminology. Since the development of the Semantic Web, knowledge graphs are often associated with linked open data projects, focusing on the connections between concepts and entities. They are also prominently associated with and used by search engines such as Google, Bing, Yext and Yahoo; knowledge-engines and question-answering services such as WolframAlpha, Apple's Siri, and Amazon Alexa; and social networks such as LinkedIn and Facebook.",8082374918826163528,related_concept,Knowledge base,2024-06-23 21:17:09.728,['Knowledge graph'],"['Knowledge graph', 'Data']",set(),set(),0,1.475,1.3259219176942192
205,411,Vadalog,http://dbpedia.org/resource/Vadalog,http://en.wikipedia.org/wiki/Vadalog,"The Vadalog system is a Knowledge Graph Management System (KGMS) that offers a language for performing complex logic reasoning tasks over knowledge graphs. At the same time, Vadalog delivers a platform to support the entire spectrum of data science tasks: data integration, pre-processing, statistical analysis, machine learning, algorithmic modeling, probabilistic reasoning and temporal reasoning. Its language is based on an extension of the rule-based language Datalog, Warded Datalog±, a high-performance language using an aggressive termination control strategy. Vadalog can support the entire spectrum of data science activities and tools. The system can read from and connect to multiple sources, from relational databases, such as PostgreSQL and MySQL, to graph databases, such as Neo4j, as well as make use of machine learning tools (e.g., Weka and Scikit-learn), and a web data extraction tool, OXPath. Additional Python libraries and extensions can also be easily integrated into the system. Vadalog is the result of a joint effort between University of Oxford, the Knowledge Graph Lab of Technische Universität Wien and Bank of Italy.",4807493161289039562,related_concept,Knowledge base,2024-06-23 21:17:09.728,"['Vadalog', 'Scikit-learn', 'Datalog', 'SQL', 'Data']","['Vadalog', 'Datalog', 'Data', 'SPARQL']",set(),set(),0,0.13333333333333333,1.3601826980675191
206,412,Semantic network,http://dbpedia.org/resource/Semantic_network,http://en.wikipedia.org/wiki/Semantic_network,"A semantic network, or frame network is a knowledge base that represents semantic relations between concepts in a network. This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between concepts, mapping or connecting semantic fields. A semantic network may be instantiated as, for example, a graph database or a concept map. Typical standardized semantic networks are expressed as semantic triples. Semantic networks are used in natural language processing applications such as semantic parsing and word-sense disambiguation. Semantic networks can also be used as a method to analyze large texts and identify the main themes and topics (e.g., of social media posts), to reveal biases (e.g., in news coverage), or even to map an entire research field.",6159429031569022542,related_concept,Knowledge base,2024-06-23 21:17:09.728,['Semantic network'],"['Semantic network', 'Computation']",set(),set(),0,1.3758099352051836,1.3183045897418928
207,413,Knowledge management,http://dbpedia.org/resource/Knowledge_management,http://en.wikipedia.org/wiki/Knowledge_management,"Knowledge management (KM) is the collection of methods relating to creating, sharing, using and managing the knowledge and information of an organization. It refers to a multidisciplinary approach to achieve organisational objectives by making the best use of knowledge. An established discipline since 1991, KM includes courses taught in the fields of business administration, information systems, management, library, and information science. Other fields may contribute to KM research, including information and media, computer science, public health and public policy. Several universities offer dedicated master's degrees in knowledge management. Many large companies, public institutions, and non-profit organisations have resources dedicated to internal KM efforts, often as a part of their business strategy, IT, or human resource management departments. Several consulting companies provide advice regarding KM to these organizations. Knowledge management efforts typically focus on organisational objectives such as improved performance, competitive advantage, innovation, the sharing of lessons learned, integration, and continuous improvement of the organisation. These efforts overlap with organisational learning and may be distinguished from that by a greater focus on the management of knowledge as a strategic asset and on encouraging the sharing of knowledge. KM is an enabler of organizational learning. The most complex scenario for knowledge management may be found in the context of supply chain as it involves multiple companies without an ownership relationship or hierarchy between them, being called by some authors as transorganizational or interorganizational knowledge. That complexity is additionally increased by industry 4.0 (or 4th industrial revolution) and digital transformation, as new challenges emerge from both the volume and speed of information flows and knowledge generation.",662409865829448252,related_concept,Knowledge base,2024-06-23 21:17:09.728,['Knowledge management'],"['Knowledge management', 'Data', 'Information technology']",set(),set(),0,2.8341584158415842,1.385686063480693
208,415,Attribute–value system,http://dbpedia.org/resource/Attribute–value_system,http://en.wikipedia.org/wiki/Attribute–value_system,"An attribute–value system is a basic knowledge representation framework comprising a table with columns designating ""attributes"" (also known as ""properties"", ""predicates"", ""features"", ""dimensions"", ""characteristics"", ""fields"", ""headers"" or ""independent variables"" depending on the context) and ""rows"" designating ""objects"" (also known as ""entities"", ""instances"", ""exemplars"", ""elements"", ""records"" or ""dependent variables""). Each table cell therefore designates the value (also known as ""state"") of a particular attribute of a particular object.",6741363615969664992,related_concept,Knowledge base,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.65,4.527652016192528e-304
209,417,Information technology,http://dbpedia.org/resource/Information_technology,http://en.wikipedia.org/wiki/Information_technology,"Information technology (IT) is the use of computers to create, process, store, retrieve, and exchange all kinds of data and information. IT forms part of information and communications technology (ICT). An information technology system (IT system) is generally an information system, a communications system, or, more specifically speaking, a computer system — including all hardware, software, and peripheral equipment — operated by a limited group of IT users. Although humans have been storing, retrieving, manipulating, and communicating information since the earliest writing systems were developed, the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that ""the new technology does not yet have a single established name. We shall call it information technology (IT)."" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs. The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce. Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC — 1450 AD), mechanical (1450—1840), electromechanical (1840—1940), and electronic (1940 to present). Information technology is also a branch of computer science, which can be defined as the overall study of procedure, structure, and the processing of various types of data. As this field continues to evolve across the world, the overall priority and importance has also grown, which is where we begin to see the introduction of computer science-related courses in K-12 education.",8514212912557100827,related_concept,Knowledge base,2024-06-23 21:17:09.728,['Information technology'],"['Information technology', 'Database', 'Data']",set(),set(),0,161.86238532110093,1.300413518751193
210,419,Object database,http://dbpedia.org/resource/Object_database,http://en.wikipedia.org/wiki/Object_database,"An object database or object-oriented database is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. A third type, object–relational databases, is a hybrid of both approaches. Object databases have been considered since the early 1980s.",551454650409949343,related_concept,Knowledge base,2024-06-23 21:17:09.728,['Object database'],"['Object database', 'Database', 'Data', 'Object Query Language', 'SQL', 'XQuery', 'Query language', 'JSONiq']",set(),set(),0,2.5447154471544717,0.5830478935110324
211,423,Ontology engineering,http://dbpedia.org/resource/Ontology_engineering,http://en.wikipedia.org/wiki/Ontology_engineering,"In computer science, information science and systems engineering, ontology engineering is a field which studies the methods and methodologies for building ontologies, which encompasses a representation, formal naming and definition of the categories, properties and relations between the concepts, data and entities. In a broader sense, this field also includes a knowledge construction of the domain using formal ontology representations such as OWL/RDF.A large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering. Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. Core ideas and objectives of ontology engineering are also central in conceptual modeling. Ontology engineering aims at making explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. Ontology engineering offers a direction towards solving the inter-operability problems brought about by semantic obstacles, i.e. the obstacles related to the definitions of business terms and software classes. Ontology engineering is a set of tasks related to the development of ontologies for a particular domain. Automated processing of information not interpretable by software agents can be improved by adding rich semantics to the corresponding resources, such as video files. One of the approaches for the formal conceptualization of represented knowledge domains is the use of machine-interpretable ontologies, which provide structured data in, or based on, RDF, RDFS, and OWL. Ontology engineering is the design and creation of such ontologies, which can contain more than just the list of terms (controlled vocabulary); they contain terminological, assertional, and relational axioms to define concepts (classes), individuals, and roles (properties) (TBox, ABox, and RBox, respectively). Ontology engineering is a relatively new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.A common way to provide the logical underpinning of ontologies is to formalize the axioms with description logics, which can then be translated to any serialization of RDF, such as RDF/XML or Turtle. Beyond the description logic axioms, ontologies might also contain SWRL rules. The concept definitions can be mapped to any kind of resource or resource segment in RDF, such as images, videos, and regions of interest, to annotate objects, persons, etc., and interlink them with related resources across knowledge bases, ontologies, and LOD datasets. This information, based on human experience and knowledge, is valuable for reasoners for the automated interpretation of sophisticated and ambiguous contents, such as the visual content of multimedia resources. Application areas of ontology-based reasoning include, but are not limited to, information retrieval, automated scene interpretation, and knowledge discovery.",6857820703920579850,related_concept,Knowledge base,2024-06-23 21:17:09.728,['Ontology engineering'],['Ontology engineering'],{'Bioinformatics'},set(),0,0.5787037037037037,1.194189064569205
212,424,Hierarchical database model,http://dbpedia.org/resource/Hierarchical_database_model,http://en.wikipedia.org/wiki/Hierarchical_database_model,"A hierarchical database model is a data model in which the data are organized into a tree-like structure. The data are stored as records which are connected to one another through links. A record is a collection of fields, with each field containing only one value. The type of a record defines which fields the record contains. The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have one or more child records. In order to retrieve data from a hierarchical database, the whole tree needs to be traversed starting from the root node. This model is recognized as the first database model created by IBM in the 1960s.",5911971650853514675,related_concept,Knowledge base,2024-06-23 21:17:09.728,[],['XML database'],set(),set(),0,2.3076923076923075,1.0892921169335434
213,428,Ontology (information science),http://dbpedia.org/resource/Ontology_(information_science),http://en.wikipedia.org/wiki/Ontology_(information_science),"In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject. Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. Each uses ontological assumptions to frame explicit theories, research and applications. New ontologies may improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages. For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management). What ontologies in both information science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence). Applied ontology is considered a successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.",2473233558467682805,related_concept,Knowledge base,2024-06-23 21:17:09.728,['AI'],"['AI', 'Ontology engineering', 'Data']",set(),set(),0,1.7578740157480315,1.240350503328399
214,430,Data wrangling,http://dbpedia.org/resource/Data_wrangling,http://en.wikipedia.org/wiki/Data_wrangling,"Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one ""raw"" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data. The process of data wrangling may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data wrangling typically follows a set of general steps which begin with extracting the data in a raw form from the data source, ""munging"" the raw data (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use.",6917667306939878027,related_concept,Data reduction,2024-06-23 21:17:09.728,"['Data wrangling', 'Data']","['Data wrangling', 'Data', 'SQL', 'KNIME', 'AI']",set(),set(),0,1.150943396226415,5.126731458535642e-304
215,432,Data pre-processing,http://dbpedia.org/resource/Data_pre-processing,http://en.wikipedia.org/wiki/Data_pre-processing,"Data preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), and missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running any analysis. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Examples of data preprocessing include cleaning, instance selection, normalization, one hot encoding, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set. Data preprocessing may affect the way in which outcomes of the final data processing can be interpreted. This aspect should be carefully considered when interpretation of the results is a key point, such in the multivariate processing of chemical data (chemometrics).",6694330029127859799,related_concept,Data reduction,2024-06-23 21:17:09.728,['Data'],"['Data collection', 'Data', 'Mean']",set(),set(),0,1.099009900990099,5.039099412492585e-304
216,433,Experimental data,http://dbpedia.org/resource/Experimental_data,http://en.wikipedia.org/wiki/Experimental_data,"Experimental data in science and engineering is data produced by a measurement, test method, experimental design or quasi-experimental design. In clinical research any data produced are the result of a clinical trial. Experimental data may be qualitative or quantitative, each being appropriate for different investigations. Generally speaking, qualitative data are considered more descriptive and can be subjective in comparison to having a continuous measurement scale that produces numbers. Whereas quantitative data are gathered in a manner that is normally experimentally repeatable, qualitative information is usually more closely related to phenomenal meaning and is, therefore, subject to interpretation by individual observers. Experimental data can be reproduced by a variety of different investigators and mathematical analysis may be performed on these data.",6193165062329930794,related_concept,Data reduction,2024-06-23 21:17:09.728,['Experimental data'],['Experimental data'],{'Data'},set(),0,1.3414634146341464,1.3105027042628035
217,434,Digital information,http://dbpedia.org/resource/Digital_information,http://en.wikipedia.org/wiki/Digital_information,,1219370059070730196,related_concept,Data reduction,2024-06-23 21:17:09.728,[],['Data'],set(),set(),0,0.175,0.39659134616550357
218,436,Conditionality principle,http://dbpedia.org/resource/Conditionality_principle,http://en.wikipedia.org/wiki/Conditionality_principle,"The conditionality principle is a Fisherian principle of statistical inference that Allan Birnbaum formally defined and studied in his 1962 JASA article. Informally, the conditionality principle can be taken as the claim that experiments which were not actually performed are statistically irrelevant. Together with the sufficiency principle, Birnbaum's version of the principle implies the famous likelihood principle. Although the relevance of the proof to data analysis remains controversial among statisticians, many Bayesians and likelihoodists consider the likelihood principle foundational for statistical inference.",5392987908883221060,related_concept,Data reduction,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2105263157894737,1.2533145882652363
219,438,Image compression,http://dbpedia.org/resource/Image_compression,http://en.wikipedia.org/wiki/Image_compression,"Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.",3518422297978027266,related_concept,Data reduction,2024-06-23 21:17:09.728,"['Image compression', 'Algorithm']","['Image compression', 'Algorithm']",set(),set(),0,4.266666666666667,1.3713921434815275
220,439,Data editing,http://dbpedia.org/resource/Data_editing,http://en.wikipedia.org/wiki/Data_editing,"Data editing is defined as the process involving the review and adjustment of collected survey data. Data editing helps define guidelines that will reduce potential bias and ensure consistent estimates leading to a clear analysis of the data set by correct inconsistent data using the methods later in this article. The purpose is to control the quality of the collected data. Data editing can be performed manually, with the assistance of a computer or a combination of both.",789201522513701496,related_concept,Data reduction,2024-06-23 21:17:09.728,"['Data editing', 'Data']","['Data editing', 'Data']",set(),set(),0,1.3275862068965518,5.035748524737996e-304
221,440,Data cleansing,http://dbpedia.org/resource/Data_cleansing,http://en.wikipedia.org/wiki/Data_cleansing,"Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall. After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data. The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of ""varying file formats, naming conventions, and columns"", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (""st, rd, etc."" to ""street, road, etcetera"").",2675662507173926928,related_concept,Data reduction,2024-06-23 21:17:09.728,"['Data cleansing', 'Data']","['Data cleansing', 'Data']",set(),set(),0,2.7916666666666665,0.8376481542196877
222,441,Encoding,http://dbpedia.org/resource/Encoding,http://en.wikipedia.org/wiki/Encoding,,4856755316881805682,related_concept,Data reduction,2024-06-23 21:17:09.728,[],['Mean'],set(),set(),0,0.8774193548387097,0.34432063125550766
223,442,Scaling (geometry),http://dbpedia.org/resource/Scaling_(geometry),http://en.wikipedia.org/wiki/Scaling_(geometry),"In affine geometry, uniform scaling (or isotropic scaling) is a linear transformation that enlarges (increases) or shrinks (diminishes) objects by a scale factor that is the same in all directions. The result of uniform scaling is similar (in the geometric sense) to the original. A scale factor of 1 is normally allowed, so that congruent shapes are also classed as similar. Uniform scaling happens, for example, when enlarging or reducing a photograph, or when creating a scale model of a building, car, airplane, etc. More general is scaling with a separate scale factor for each axis direction. Non-uniform scaling (anisotropic scaling) is obtained when at least one of the scaling factors is different from the others; a special case is directional scaling or stretching (in one direction). Non-uniform scaling changes the shape of the object; e.g. a square may change into a rectangle, or into a parallelogram if the sides of the square are not parallel to the scaling axes (the angles between lines parallel to the axes are preserved, but not all angles). It occurs, for example, when a faraway billboard is viewed from an oblique angle, or when the shadow of a flat object falls on a surface that is not parallel to it. When the scale factor is larger than 1, (uniform or non-uniform) scaling is sometimes also called dilation or enlargement. When the scale factor is a positive number smaller than 1, scaling is sometimes also called contraction or reduction. In the most general sense, a scaling includes the case in which the directions of scaling are not perpendicular. It also includes the case in which one or more scale factors are equal to zero (projection), and the case of one or more negative scale factors (a directional scaling by -1 is equivalent to a reflection). Scaling is a linear transformation, and a special case of homothetic transformation (scaling about a point). In most cases, the homothetic transformations are non-linear transformations.",4176044504037483559,related_concept,Data reduction,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.5371900826446283,1.3608853323115129
224,444,Interpolation,http://dbpedia.org/resource/Interpolation,http://en.wikipedia.org/wiki/Interpolation,"In the mathematical field of numerical analysis, interpolation is a type of estimation, a method of constructing (finding) new data points based on the range of a discrete set of known data points. In engineering and science, one often has a number of data points, obtained by sampling or experimentation, which represent the values of a function for a limited number of values of the independent variable. It is often required to interpolate; that is, estimate the value of that function for an intermediate value of the independent variable. A closely related problem is the approximation of a complicated function by a simple function. Suppose the formula for some given function is known, but too complicated to evaluate efficiently. A few data points from the original function can be interpolated to produce a simpler function which is still fairly close to the original. The resulting gain in simplicity may outweigh the loss from interpolation error and give better performance in calculation process.",8035254067362835204,related_concept,Data reduction,2024-06-23 21:17:09.728,[],"['Interpolation', 'Polynomial', 'Kriging']",{'Interpolation'},set(),0,7.08080808080808,1.3931838536161831
225,445,Epilepsy,http://dbpedia.org/resource/Epilepsy,http://en.wikipedia.org/wiki/Epilepsy,"Epilepsy is a group of non-communicable neurological disorders characterized by recurrent epileptic seizures. Epileptic seizures can vary from brief and nearly undetectable periods to long periods of vigorous shaking due to abnormal electrical activity in the brain. These episodes can result in physical injuries, either directly such as broken bones or through causing accidents. In epilepsy, seizures tend to recur and may have no immediate underlying cause. Isolated seizures that are provoked by a specific cause such as poisoning are not deemed to represent epilepsy. People with epilepsy may be treated differently in various areas of the world and experience varying degrees of social stigma due to the alarming nature of their symptoms. The underlying mechanism of epileptic seizures is excessive and abnormal neuronal activity in the cortex of the brain which can be observed in the electroencephalogram (EEG) of an individual. The reason this occurs in most cases of epilepsy is unknown (idiopathic); some cases occur as the result of brain injury, stroke, brain tumors, infections of the brain, or birth defects through a process known as epileptogenesis. Known genetic mutations are directly linked to a small proportion of cases. The diagnosis involves ruling out other conditions that might cause similar symptoms, such as fainting, and determining if another cause of seizures is present, such as alcohol withdrawal or electrolyte problems. This may be partly done by imaging the brain and performing blood tests. Epilepsy can often be confirmed with an EEG, but a normal test does not rule out the condition. Epilepsy that occurs as a result of other issues may be preventable. Seizures are controllable with medication in about 69% of cases; inexpensive anti-seizure medications are often available. In those whose seizures do not respond to medication; surgery, neurostimulation or dietary changes may then be considered. Not all cases of epilepsy are lifelong, and many people improve to the point that treatment is no longer needed. As of 2020, about 50 million people have epilepsy. Nearly 80% of cases occur in the developing world. In 2015, it resulted in 125,000 deaths, an increase from 112,000 in 1990. Epilepsy is more common in older people. In the developed world, onset of new cases occurs most frequently in babies and the elderly. In the developing world, onset is more common in older children and young adults due to differences in the frequency of the underlying causes. About 5–10% of people will have an unprovoked seizure by the age of 80, with the chance of experiencing a second seizure rising to between 40% and 50%. In many areas of the world, those with epilepsy either have restrictions placed on their ability to drive or are not permitted to drive until they are free of seizures for a specific length of time. The word epilepsy is from Ancient Greek ἐπιλαμβάνειν, ""to seize, possess, or afflict"".",2561855096501220367,related_concept,Data reduction,2024-06-23 21:17:09.728,['Epilepsy'],"['Epilepsy', 'Classification']",{'Epilepsy'},set(),0,8.298479087452472,1.4170196446910135
226,446,Smoothing,http://dbpedia.org/resource/Smoothing,http://en.wikipedia.org/wiki/Smoothing,"In statistics and image processing, to smooth a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In smoothing, the data points of a signal are modified so individual points higher than the adjacent points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Smoothing may be used in two important ways that can aid in data analysis (1) by being able to extract more information from the data as long as the assumption of smoothing is reasonable and (2) by being able to provide analyses that are both flexible and robust. Many different algorithms are used in smoothing. Smoothing may be distinguished from the related and partially overlapping concept of curve fitting in the following ways: 
* curve fitting often involves the use of an explicit function form for the result, whereas the immediate results from smoothing are the ""smoothed"" values with no later use made of a functional form if there is one; 
* the aim of smoothing is to give a general idea of relatively slow changes of value with little attention paid to the close matching of data values, while curve fitting concentrates on achieving as close a match as possible. 
* smoothing methods often have an associated tuning parameter which is used to control the extent of smoothing. Curve fitting will adjust any number of parameters of the function to obtain the 'best' fit.",3749305007497349684,related_concept,Data reduction,2024-06-23 21:17:09.728,"['Curve fitting', 'Smoothing']",['Smoothing'],set(),set(),0,1.8625,1.3719049450095375
227,448,Sorting,http://dbpedia.org/resource/Sorting,http://en.wikipedia.org/wiki/Sorting,"Sorting refers to ordering data in an increasing or decreasing manner according to some linear relationship among the data items. 1. 
* ordering: arranging items in a sequence ordered by some criterion; 2. 
* categorizing: grouping items with similar properties. Ordering items is the combination of categorizing them based on equivalent order, and ordering the categories themselves.",6870781153371756175,related_concept,Data reduction,2024-06-23 21:17:09.728,['Sorting'],['Sorting'],set(),set(),0,3.5833333333333335,1.4425323134987555
228,449,Digitization,http://dbpedia.org/resource/Digitization,http://en.wikipedia.org/wiki/Digitization,"Digitization is the process of converting information into a digital (i.e. computer-readable) format. The result is the representation of an object, image, sound, document, or signal (usually an analog signal) obtained by generating a series of numbers that describe a discrete set of points or samples. The result is called digital representation or, more specifically, a digital image, for the object, and digital form, for the signal. In modern practice, the digitized data is in the form of binary numbers, which facilitates processing by digital computers and other operations, but digitizing simply means ""the conversion of analog source material into a numerical format""; the decimal or any other number system can be used instead. Digitization is of crucial importance to data processing, storage, and transmission, because it ""allows information of all kinds in all formats to be carried with the same efficiency and also intermingled."" Though analog data is typically more stable, digital data has the potential to be more easily shared and accessed and, in theory, can be propagated indefinitely without generation loss, provided it is migrated to new, stable formats as needed. This potential has led to institutional digitization projects designed to improve access and the rapid growth of the digital preservation field. Sometimes digitization and digital preservation are mistaken for the same thing. They are different, but digitization is often a vital first step in digital preservation. Libraries, archives, museums, and other memory institutions digitize items to preserve fragile materials and create more access points for patrons. Doing this creates challenges for information professionals and solutions can be as varied as the institutions that implement them. Some analog materials, such as audio and video tapes, are nearing the end of their life-cycle, and it is important to digitize them before equipment obsolescence and media deterioration makes the data irretrievable. There are challenges and implications surrounding digitization including time, cost, cultural history concerns, and creating an equitable platform for historically marginalized voices. Many digitizing institutions develop their own solutions to these challenges. Mass digitization projects have had mixed results over the years, but some institutions have had success even if not in the traditional Google Books model. Technological changes can happen often and quickly, so digitization standards are difficult to keep updated. Professionals in the field can attend conferences and join organizations and working groups to keep their knowledge current and add to the conversation.",4544187338549770285,related_concept,Data reduction,2024-06-23 21:17:09.728,['Digitization'],"['Digitization', 'Digital information', 'Database', 'Data']",set(),set(),0,2.8704663212435233,1.3671159145011207
229,450,Data reduction,http://dbpedia.org/resource/Data_reduction,http://en.wikipedia.org/wiki/Data_reduction,"Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold: reduce the number of data records by eliminating invalid data or produce summary data and statistics at different aggregation levels for various applications. When information is derived from instrument readings there may also be a transformation from analog to digital form. When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, encoding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed. The data reduction is often undertaken in the presence of reading or measurement errors. Some idea of the nature of these errors is needed before the most likely value may be determined. An example in astronomy is the data reduction in the Kepler satellite. This satellite records 95-megapixel images once every six seconds, generating dozens of megabytes of data per second, which is orders-of-magnitudes more than the downlink bandwidth of 550 kB/s. The on-board data reduction encompasses co-adding the raw frames for thirty minutes, reducing the bandwidth by a factor of 300. Furthermore, interesting targets are pre-selected and only the relevant pixels are processed, which is 6% of the total. This reduced data is then sent to Earth where it is processed further. Research has also been carried out on the use of data reduction in wearable (wireless) devices for health monitoring and diagnosis applications. For example, in the context of epilepsy diagnosis, data reduction has been used to increase the battery lifetime of a wearable EEG device by selecting and only transmitting, EEG data that is relevant for diagnosis and discarding background activity.",4726467684422275160,main_concept,,2024-06-23 21:17:09.728,"['Data reduction', 'Data']","['Data reduction', 'Data', 'Dimension', 'Dimensionality reduction']",set(),set(),0,1.2875,5.0534121075022644e-304
230,451,Database model,http://dbpedia.org/resource/Database_model,http://en.wikipedia.org/wiki/Database_model,"A database model is a type of data model that determines the logical structure of a database. It fundamentally determines in which manner data can be stored, organized and manipulated. The most popular example of a database model is the relational model, which uses a table-based format.",5714468377611547198,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['SQL', 'Dimension', 'Object database']",set(),set(),0,0.990506329113924,0.6529770225010996
231,452,Data Vault Modeling,http://dbpedia.org/resource/Data_Vault_Modeling,http://en.wikipedia.org/wiki/Data_Vault_Modeling,,2522305906522666816,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['Data', 'SQL']",set(),set(),0,0.17391304347826086,0.35471230166961715
232,453,Semantic data model,http://dbpedia.org/resource/Semantic_data_model,http://en.wikipedia.org/wiki/Semantic_data_model,"Semantic data model (SDM) is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it. SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system. A semantic data model in software engineering has various meanings: 1. 
* It is a conceptual data model in which semantic information is included. This means that the model describes the meaning of its instances. Such a semantic data model is an abstraction that defines how the stored symbols (the instance data) relate to the real world. 2. 
* It is a conceptual data model that includes the capability to express and exchange information which enables parties to interpret meaning (semantics) from the instances, without the need to know the meta-model. Such semantic models are fact-oriented (as opposed to object-oriented). Facts are typically expressed by binary relations between data elements, whereas higher order relations are expressed as collections of binary relations. Typically binary relations have the form of triples: Object-RelationType-Object. For example: the Eiffel Tower Paris. Typically the instance data of semantic data models explicitly include the kinds of relationships between the various data elements, such as . To interpret the meaning of the facts from the instances, it is required that the meaning of the kinds of relations (relation types) be known. Therefore, semantic data models typically standardize such relation types. This means that the second kind of semantic data models enables that the instances express facts that include their own meanings. The second kind of semantic data models are usually meant to create semantic databases. The ability to include meaning in semantic databases facilitates building distributed databases that enable applications to interpret the meaning from the content. This implies that semantic databases can be integrated when they use the same (standard) relation types. This also implies that in general they have a wider applicability than relational or object-oriented databases.",1656070387434140723,related_concept,Data modeling,2024-06-23 21:17:09.728,['Semantic data model'],"['Database', 'Data']",set(),set(),0,1.9830508474576272,4.425408156173771e-304
233,454,Entity–relationship model,http://dbpedia.org/resource/Entity–relationship_model,http://en.wikipedia.org/wiki/Entity–relationship_model,"An entity–relationship model (or ER model) describes interrelated things of interest in a specific domain of knowledge. A basic ER model is composed of entity types (which classify the things of interest) and specifies relationships that can exist between entities (instances of those entity types). In software engineering, an ER model is commonly formed to represent things a business needs to remember in order to perform business processes. Consequently, the ER model becomes an abstract data model, that defines a data or information structure which can be implemented in a database, typically a relational database. Entity–relationship modeling was developed for database and design by Peter Chen and published in a 1976 paper, with variants of the idea existing previously, but today it is commonly used for teaching students the basics of data base structure. Some ER models show super and subtype entities connected by generalization-specialization relationships, and an ER model can be used also in the specification of domain-specific ontologies.",1160171761087213974,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['Entity–relationship model', 'SQL', 'Data']",set(),set(),0,1.9619377162629759,4.4287328388223365e-304
234,455,Three schema approach,http://dbpedia.org/resource/Three_schema_approach,http://en.wikipedia.org/wiki/Three_schema_approach,,6870459210298867760,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['Database', 'Data', 'IDEF1X']",set(),set(),0,1.3870967741935485,0.29519944020643996
235,456,Conceptual modeling,http://dbpedia.org/resource/Conceptual_modeling,http://en.wikipedia.org/wiki/Conceptual_modeling,,2533733377788114699,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['Conceptual modeling', 'Data', 'Entity–relationship model', 'IDEF1X', 'Model theory', 'Mathematical model', 'Prediction', 'Model selection']",set(),set(),0,0.104,4.2324638696698134e-305
236,457,Business process,http://dbpedia.org/resource/Business_process,http://en.wikipedia.org/wiki/Business_process,"A business process, business method or business function is a collection of related, structured activities or tasks by people or equipment in which a specific sequence produces a service or product (serves a particular business goal) for a particular customer or customers. Business processes occur at all organizational levels and may or may not be visible to the customers. A business process may often be visualized (modeled) as a flowchart of a sequence of activities with interleaving decision points or as a process matrix of a sequence of activities with relevance rules based on data in the process. The benefits of using business processes include improved customer satisfaction and improved agility for reacting to rapid market change. Process-oriented organizations break down the barriers of structural departments and try to avoid functional silos.",2211658754414008956,related_concept,Data modeling,2024-06-23 21:17:09.728,['Business process'],"['Business process', 'Knowledge management', 'Data model', 'Data', 'Data modeling']",set(),set(),0,2.961206896551724,1.3209252314500681
237,460,Metadata modeling,http://dbpedia.org/resource/Metadata_modeling,http://en.wikipedia.org/wiki/Metadata_modeling,"Metadata modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable to and useful for some predefined class of problems. Meta-modeling is the analysis, construction and development of the frames, rules, constraints, models and theories applicable and useful for the modeling in a predefined class of problems. The meta-data side of the diagram consists of a concept diagram. This is basically an adjusted class diagram as described in Booch, Rumbaugh and Jacobson (1999). Important notions are concept, generalization, association, multiplicity and aggregation.",8612568947849175986,related_concept,Data modeling,2024-06-23 21:17:09.728,['Metadata modeling'],['Metadata modeling'],set(),set(),0,2.75,4.432866474349567e-304
238,461,Requirements analysis,http://dbpedia.org/resource/Requirements_analysis,http://en.wikipedia.org/wiki/Requirements_analysis,"In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements. Requirements analysis is critical to the success or failure of a systems or software project. The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.",590682010499991372,related_concept,Data modeling,2024-06-23 21:17:09.728,['Requirements analysis'],['Requirements analysis'],set(),set(),0,1.3269689737470167,1.323698473699699
239,462,Conceptual schema,http://dbpedia.org/resource/Conceptual_schema,http://en.wikipedia.org/wiki/Conceptual_schema,"A conceptual schema is a high-level description of informational needs underlying the design of a database. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatypes, relationships and constraints.",1837269322604765305,related_concept,Data modeling,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.7384615384615385,4.42768361736513e-304
240,463,IDEF1X,http://dbpedia.org/resource/IDEF1X,http://en.wikipedia.org/wiki/IDEF1X,"Integration DEFinition for information modeling (IDEF1X) is a data modeling language for the development of semantic data models. IDEF1X is used to produce a graphical information model which represents the structure and semantics of information within an environment or system. IDEF1X permits the construction of semantic data models which may serve to support the management of data as a resource, the integration of information systems, and the building of computer databases. This standard is part of the IDEF family of modeling languages in the field of software engineering.",1274512212471800079,related_concept,Data modeling,2024-06-23 21:17:09.728,['IDEF1X'],"['IDEF1X', 'SQL']",set(),set(),0,0.9622641509433962,4.528163255094757e-304
241,464,Information model,http://dbpedia.org/resource/Information_model,http://en.wikipedia.org/wiki/Information_model,"An information model in software engineering is a representation of concepts and the relationships, constraints, rules, and operations to specify data semantics for a chosen domain of discourse. Typically it specifies relations between kinds of things, but may also include relations with individual things. It can provide sharable, stable, and organized structure of information requirements or knowledge for the domain context.",6012029004477722164,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['IDEF1X', 'SQL', 'Information model', 'Database', 'Data', 'Data model']",set(),set(),0,2.0273972602739727,0.8406204083272556
242,465,Enterprise data modeling,http://dbpedia.org/resource/Enterprise_data_modeling,http://en.wikipedia.org/wiki/Enterprise_data_modeling,,410378266621068419,related_concept,Data modeling,2024-06-23 21:17:09.728,[],"['Data model', 'Data', 'Data modeling', 'AI']",set(),set(),0,0.13333333333333333,0.20596645752955262
243,466,Business process modeling,http://dbpedia.org/resource/Business_process_modeling,http://en.wikipedia.org/wiki/Business_process_modeling,"Business process modeling (BPM) in business process management and systems engineering is the activity of representing processes of an enterprise, so that the current business processes may be analyzed, improved, and automated. BPM is typically performed by business analysts, who provide expertise in the modeling discipline; by subject matter experts, who have specialized knowledge of the processes being modeled; or more commonly by a team comprising both. Alternatively, the process model can be derived directly from events' logs using process mining tools. The business objective is often to increase process speed or reduce cycle time; to increase quality; or to reduce costs, such as labor, materials, scrap, or capital costs. In practice, a management decision to invest in business process modeling is often motivated by the need to document requirements for an information technology project. Change management programs are typically involved to put any improved business processes into practice. With advances in software design, the vision of BPM models becoming fully executable (and capable of simulations and round-trip engineering) is coming closer to reality.",1321022378086721623,related_concept,Data modeling,2024-06-23 21:17:09.728,"['Business process', 'Business process modeling']","['Business process', 'Business process modeling', 'AI']",set(),set(),0,0.6201923076923077,1.2898000794381788
244,468,Physical schema,http://dbpedia.org/resource/Physical_schema,http://en.wikipedia.org/wiki/Physical_schema,"A physical data model (or database design) is a representation of a data design as implemented, or intended to be implemented, in a database management system. In the lifecycle of a project it typically derives from a logical data model, though it may be reverse-engineered from a given database implementation. A complete physical data model will include all the required to create relationships between tables or to achieve performance goals, such as indexes, constraint definitions, linking tables, partitioned tables or clusters. Analysts can usually use a physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system. As of 2012 seven main databases dominate the commercial marketplace: Informix, Oracle, Postgres, SQL Server, Sybase, IBM Db2 and MySQL. Other RDBMS systems tend either to be legacy databases or used within academia such as universities or further education colleges. Physical data models for each implementation would differ significantly, not least due to underlying operating-system requirements that may sit underneath them. For example: SQL Server runs only on Microsoft Windows operating-systems (Starting with SQL Server 2017, SQL Server runs on Linux. It's the same SQL Server database engine, with many similar features and services regardless of your operating system), while Oracle and MySQL can run on Solaris, Linux and other UNIX-based operating-systems as well as on Windows. This means that the disk requirements, security requirements and many other aspects of a physical data model will be influenced by the RDBMS that a database administrator (or an organization) chooses to use.",1721031434423978770,related_concept,Data modeling,2024-06-23 21:17:09.728,['SQL'],"['SQL', 'Physical schema']",set(),set(),0,1.0888888888888888,4.479937831854955e-304
245,469,Systems analysis,http://dbpedia.org/resource/Systems_analysis,http://en.wikipedia.org/wiki/Systems_analysis,"Systems analysis is ""the process of studying a procedure or business to identify its goal and purposes and create systems and procedures that will efficiently achieve them"". Another view sees system analysis as a problem-solving technique that breaks down a system into its component pieces, and how well those parts work and interact to accomplish their purpose. The field of system analysis relates closely to requirements analysis or to operations research. It is also ""an explicit formal inquiry carried out to help a decision maker identify a better course of action and make a better decision than they might otherwise have made."" The terms analysis and synthesis stems from Greek, meaning ""to take apart"" and ""to put together,"" respectively. These terms are used in many scientific disciplines, from mathematics and logic to economics and psychology, to denote similar investigative procedures. The analysis is defined as ""the procedure by which we break down an intellectual or substantial whole into parts,"" while synthesis means ""the procedure by which we combine separate elements or components to form a coherent whole."" System analysis researchers apply methodology to the systems involved, forming an overall picture. System analysis is used in every field where something is developed. Analysis can also be a series of components that perform organic functions together, such as system engineering. System engineering is an interdisciplinary field of engineering that focuses on how complex engineering projects should be designed and managed.",2611544871144149300,related_concept,Data modeling,2024-06-23 21:17:09.728,['Systems analysis'],['Systems analysis'],set(),set(),0,3.6914893617021276,1.3540692932064926
246,470,Logical schema,http://dbpedia.org/resource/Logical_schema,http://en.wikipedia.org/wiki/Logical_schema,"A logical data model or logical schema is a data model of a specific problem domain expressed independently of a particular database management product or storage technology (physical data model) but in terms of data structures such as relational tables and columns, object-oriented classes, or XML tags. This is as opposed to a conceptual data model, which describes the semantics of an organization without reference to technology.",1537186556490096114,related_concept,Data modeling,2024-06-23 21:17:09.728,[],['Data'],set(),set(),0,1.6170212765957446,4.4258711541215535e-304
247,473,Ultrametricity,http://dbpedia.org/resource/Ultrametricity,http://en.wikipedia.org/wiki/Ultrametricity,,3734965009707347186,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.1016949152542373,0.1925614184684338
248,474,Weissella,http://dbpedia.org/resource/Weissella,http://en.wikipedia.org/wiki/Weissella,"Weissella is a genus of gram-positive bacteria placed within the family Lactobacillaceae, formerly considered species of the Leuconostoc paramesenteroides group. The morphology of Weissella species varies from spherical or lenticular cells to irregular rods. Several strains of Weissella cibaria and Weissella confusa have shown probiotic potential. In particular, the cell-free culture supernatant of Weissella confusa shows a number of beneficial characteristics, such as antibacterial potential and anti-inflammatory efficiency. However, several strains of W. confusa are opportunistic bacteria. A number of studies have been done on the safety of the bacterial species, indicating their probiotic potential. The Senate Commission on Food Safety has validated the use of W. confusa in food.",3542522559599442481,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,['Weissella'],['Weissella'],{'Weissella'},set(),0,0.4634146341463415,1.3733096755880583
249,475,Kruskal's algorithm,http://dbpedia.org/resource/Kruskal's_algorithm,http://en.wikipedia.org/wiki/Kruskal's_algorithm,"Kruskal's algorithm finds a minimum spanning forest of an undirected edge-weighted graph. If the graph is connected, it finds a minimum spanning tree. (A minimum spanning tree of a connected graph is a subset of the edges that forms a tree that includes every vertex, where the sum of the weights of all the edges in the tree is minimized. For a disconnected graph, a minimum spanning forest is composed of a minimum spanning tree for each connected component.) It is a greedy algorithm in graph theory as in each step it adds the next lowest-weight edge that will not form a cycle to the minimum spanning forest. This algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48–50 in 1956, and was written by Joseph Kruskal. It was rediscovered by . Other algorithms for this problem include Prim's algorithm, the reverse-delete algorithm, and Borůvka's algorithm.",2228485408313236355,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,"[""Kruskal's algorithm"", ""Prim's algorithm""]","[""Kruskal's algorithm"", ""Prim's algorithm""]",set(),set(),0,3.875,1.1805947938107513
250,476,Prim's algorithm,http://dbpedia.org/resource/Prim's_algorithm,http://en.wikipedia.org/wiki/Prim's_algorithm,"In computer science, Prim's algorithm (also known as Jarník's algorithm) is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex. The algorithm was developed in 1930 by Czech mathematician Vojtěch Jarník and later rediscovered and republished by computer scientists Robert C. Prim in 1957 and Edsger W. Dijkstra in 1959. Therefore, it is also sometimes called the Jarník's algorithm, Prim–Jarník algorithm, Prim–Dijkstra algorithmor the DJP algorithm. Other well-known algorithms for this problem include Kruskal's algorithm and Borůvka's algorithm. These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest. In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms.However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.",5394848321116392158,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,"[""Kruskal's algorithm"", ""Prim's algorithm""]","[""Prim's algorithm"", ""Kruskal's algorithm""]",set(),set(),0,3.852272727272727,1.096510693689012
251,477,UPGMA,http://dbpedia.org/resource/UPGMA,http://en.wikipedia.org/wiki/UPGMA,"UPGMA (unweighted pair group method with arithmetic mean) is a simple agglomerative (bottom-up) hierarchical clustering method. The method is generally attributed to Sokal and Michener. The UPGMA method is similar to its weighted variant, the WPGMA method. Note that the unweighted term indicates that all distances contribute equally to each average that is computed and does not refer to the math by which it is achieved. Thus the simple averaging in WPGMA produces a weighted result and the proportional averaging in UPGMA produces an unweighted result.",6756066248936705926,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,"['UPGMA', 'WPGMA']","['UPGMA', 'WPGMA', 'Bacillus subtilis', '5S ribosomal RNA', 'Acholeplasma', 'Micrococcus luteus']",set(),set(),0,2.131868131868132,0.7409484523024802
252,478,Complete-linkage clustering,http://dbpedia.org/resource/Complete-linkage_clustering,http://en.wikipedia.org/wiki/Complete-linkage_clustering,"Complete-linkage clustering is one of several methods of agglomerative hierarchical clustering. At the beginning of the process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. The method is also known as farthest neighbour clustering. The result of the clustering can be visualized as a dendrogram, which shows the sequence of cluster fusion and the distance at which each fusion took place.",4352305564867599370,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,['Complete-linkage clustering'],"['Complete-linkage clustering', 'Bacillus subtilis', '5S ribosomal RNA', 'Acholeplasma', 'Micrococcus luteus']",set(),set(),0,3.2857142857142856,3.338204217515926e-304
253,480,Bacillus subtilis,http://dbpedia.org/resource/Bacillus_subtilis,http://en.wikipedia.org/wiki/Bacillus_subtilis,"Bacillus subtilis, known also as the hay bacillus or grass bacillus, is a Gram-positive, catalase-positive bacterium, found in soil and the gastrointestinal tract of ruminants, humans and marine sponges. As a member of the genus Bacillus, B. subtilis is rod-shaped, and can form a tough, protective endospore, allowing it to tolerate extreme environmental conditions. B. subtilis has historically been classified as an obligate aerobe, though evidence exists that it is a facultative anaerobe. B. subtilis is considered the best studied Gram-positive bacterium and a model organism to study bacterial chromosome replication and cell differentiation. It is one of the bacterial champions in secreted enzyme production and used on an industrial scale by biotechnology companies.",1402051101484866754,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,['Bacillus subtilis'],['Bacillus subtilis'],set(),set(),0,2.379591836734694,1.3651238767823313
254,481,5S ribosomal RNA,http://dbpedia.org/resource/5S_ribosomal_RNA,http://en.wikipedia.org/wiki/5S_ribosomal_RNA,"The 5S ribosomal RNA (5S rRNA) is an approximately 120 nucleotide-long ribosomal RNA molecule with a mass of 40 kDa. It is a structural and functional component of the large subunit of the ribosome in all domains of life (bacteria, archaea, and eukaryotes), with the exception of mitochondrial ribosomes of fungi and animals. The designation 5S refers to the molecule's sedimentation velocity in an ultracentrifuge, which is measured in Svedberg units (S).",1671422513748105979,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,['5S ribosomal RNA'],['5S ribosomal RNA'],set(),set(),0,2.21505376344086,1.3492144985121477
255,483,Neighbor-joining,http://dbpedia.org/resource/Neighbor-joining,http://en.wikipedia.org/wiki/Neighbor-joining,,5966838683842055842,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,[],"['UPGMA', 'WPGMA']",set(),set(),0,0.9204545454545454,0.3028368354509677
256,484,Acholeplasma,http://dbpedia.org/resource/Acholeplasma,http://en.wikipedia.org/wiki/Acholeplasma,"Acholeplasma are wall-less bacteria in the Mollicutes class. They include saprotrophic or pathogenic species. There are 15 recognised species. The G+C content is low, ranging from 26 - 36% (mol%). The genomes of Acholeplasma species range in size from 1.5 to 1.65 Mbp. Cholesterol is not required for growth. The species are found on animals, and some plants and insects. The optimum growth temperature is 30 to 37 degrees Celsius. Acholeplasma laidlawii is a common contaminant of cell culture media products, and has also been used in extensive studies of lipid polymorphism because this organism alters its ratio of MGlcDG (monoglucosyl diacylglycerol) to DGlcDG (diglucosyl diacylglycerol) in response to growth conditions.",7250213200333327057,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,['Acholeplasma'],['Acholeplasma'],set(),set(),0,0.11165048543689321,1.366924362312074
257,485,Minimum spanning tree,http://dbpedia.org/resource/Minimum_spanning_tree,http://en.wikipedia.org/wiki/Minimum_spanning_tree,"A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. That is, it is a spanning tree whose sum of edge weights is as small as possible. More generally, any edge-weighted undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of the minimum spanning trees for its connected components. There are many use cases for minimum spanning trees. One example is a telecommunications company trying to lay cable in a new neighborhood. If it is constrained to bury the cable only along certain paths (e.g. roads), then there would be a graph containing the points (e.g. houses) connected by those paths. Some of the paths might be more expensive, because they are longer, or require the cable to be buried deeper; these paths would be represented by edges with larger weights. Currency is an acceptable unit for edge weight – there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality. A spanning tree for that graph would be a subset of those paths that has no cycles but still connects every house; there might be several spanning trees possible. A minimum spanning tree would be one with the lowest total cost, representing the least expensive path for laying the cable.",6936863362284452226,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,[],"[""Prim's algorithm"", ""Kruskal's algorithm"", 'Minimum spanning tree']",set(),set(),0,2.8732394366197185,1.1403680214828429
258,486,Ward's method,http://dbpedia.org/resource/Ward's_method,http://en.wikipedia.org/wiki/Ward's_method,"In statistics, Ward's method is a criterion applied in hierarchical cluster analysis. Ward's minimum variance method is a special case of the objective function approach originally presented by Joe H. Ward, Jr. Ward suggested a general agglomerative hierarchical clustering procedure, where the criterion for choosing the pair of clusters to merge at each step is based on the optimal value of an objective function. This objective function could be ""any function that reflects the investigator's purpose."" Many of the standard clustering procedures are contained in this very general class. To illustrate the procedure, Ward used the example where the objective function is the error sum of squares, and this example is known as Ward's method or more precisely Ward's minimum variance method. The nearest-neighbor chain algorithm can be used to find the same clustering defined by Ward's method, in time proportional to the size of the input distance matrix and space linear in the number of points being clustered.",3850522337994905090,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,"[""Ward's method""]","[""Ward's method"", 'Euclidean distance']",set(),set(),0,2.764705882352941,1.0440418047735005
259,487,Micrococcus luteus,http://dbpedia.org/resource/Micrococcus_luteus,http://en.wikipedia.org/wiki/Micrococcus_luteus,"Micrococcus luteus is a Gram-positive, to Gram-variable, nonmotile, coccus, tetrad-arranging, pigmented, saprotrophic bacterium that belongs to the family Micrococcaceae. It is urease and catalase positive. An obligate aerobe, M. luteus is found in soil, dust, water and air, and as part of the normal microbiota of the mammalian skin. The bacterium also colonizes the human mouth, mucosae, oropharynx and upper respiratory tract. Micrococcus luteus is considered a contaminant in sick patients and is resistant by slowing of major metabolic processes and induction of unique genes. It is a high G + C ratio bacterium. Micrococcus luteus is coagulase negative, bacitracin susceptible, and forms bright yellow colonies on nutrient agar. Micrococcus luteus has been shown to survive in oligotrophic environments for extended periods of time. Recent work by Greenblatt et al. demonstrate that Micrococcus luteus has survived for at least 34,000 to 170,000 years on the basis of 16S rRNA analysis, and possibly much longer. It was sequenced in 2010 and has one of the smallest genomes of free-living Actinomycetota sequenced to date, comprising a single circular chromosome of 2,501,097 bp.",7893810685776938422,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,['Micrococcus luteus'],['Micrococcus luteus'],set(),set(),0,0.9152542372881356,1.368499505445168
260,488,Models of DNA evolution,http://dbpedia.org/resource/Models_of_DNA_evolution,http://en.wikipedia.org/wiki/Models_of_DNA_evolution,"A number of different Markov models of DNA sequence evolution have been proposed. These substitution models differ in terms of the parameters used to describe the rates at which one nucleotide replaces another during evolution. These models are frequently used in molecular phylogenetic analyses. In particular, they are used during the calculation of likelihood of a tree (in Bayesian and maximum likelihood approaches to tree estimation) and they are used to estimate the evolutionary distance between sequences from the observed differences between the sequences.",5255858617705986476,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,[],"['Evolution', 'Theorem']",{'Bioinformatics'},set(),0,1.2463768115942029,1.3341048344851205
261,489,WPGMA,http://dbpedia.org/resource/WPGMA,http://en.wikipedia.org/wiki/WPGMA,"WPGMA (Weighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method, generally attributed to Sokal and Michener. The WPGMA method is similar to its unweighted variant, the UPGMA method.",1493897378734177145,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,"['UPGMA', 'WPGMA', 'Mean']","['WPGMA', 'Mean', 'UPGMA', 'Bacillus subtilis', '5S ribosomal RNA', 'Acholeplasma', 'Micrococcus luteus']",set(),set(),0,0.5375,0.8702749484502026
262,491,Molecular clock,http://dbpedia.org/resource/Molecular_clock,http://en.wikipedia.org/wiki/Molecular_clock,"The molecular clock is a figurative term for a technique that uses the mutation rate of biomolecules to deduce the time in prehistory when two or more life forms diverged. The biomolecular data used for such calculations are usually nucleotide sequences for DNA, RNA, or amino acid sequences for proteins. The benchmarks for determining the mutation rate are often fossil or archaeological dates. The molecular clock was first tested in 1962 on the hemoglobin protein variants of various animals, and is commonly used in molecular evolution to estimate times of speciation or radiation. It is sometimes called a gene clock or an evolutionary clock.",5144926685539534418,related_concept,Single-linkage clustering,2024-06-23 21:17:09.728,[],['Molecular clock'],set(),set(),0,2.40327868852459,1.1334506152194832
263,494,Scikit-learn,http://dbpedia.org/resource/Scikit-learn,http://en.wikipedia.org/wiki/Scikit-learn,"Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a fiscally sponsored project.",5246243281003163048,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,"['Scikit-learn', 'DBSCAN']","['Scikit-learn', 'DBSCAN']",set(),set(),0,2.9322033898305087,1.205330664094587
264,497,Spanning tree,http://dbpedia.org/resource/Spanning_tree,http://en.wikipedia.org/wiki/Spanning_tree,"In the mathematical field of graph theory, a spanning tree T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G. In general, a graph may have several spanning trees, but a graph that is not connected will not contain a spanning tree (see about below). If all of the edges of G are also edges of a spanning tree T of G, then G is a tree and is identical to T (that is, a tree has a unique spanning tree and it is itself).",8520998119197773315,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,[],"[""Dijkstra's algorithm"", 'Laplacian matrix', 'Depth-first search', 'Spanning tree', 'Euclidean plane', 'Euclidean distance']",set(),set(),0,1.5175438596491229,1.2374231896940378
265,500,Priority queue,http://dbpedia.org/resource/Priority_queue,http://en.wikipedia.org/wiki/Priority_queue,"In computer science, a priority queue is an abstract data-type similar to a regular queue or stack data structure in which each element additionally has a priority associated with it. In a priority queue, an element with high priority is served before an element with low priority. In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued; in other implementations ordering of elements with the same priority remains undefined. While coders often implement priority queues with heaps, they are conceptually distinct from heaps. A priority queue is a concept like a list or a map; just as a list can be implemented with a linked list or with an array, a priority queue can be implemented with a heap or with a variety of other methods such as an unordered array.",5515861239741943841,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,[],"[""Dijkstra's algorithm"", ""Prim's algorithm""]",set(),set(),0,1.6287425149700598,1.4407367740352957
266,501,Dendrogram,http://dbpedia.org/resource/Dendrogram,http://en.wikipedia.org/wiki/Dendrogram,"A dendrogram is a diagram representing a tree. This diagrammatic representation is frequently used in different contexts: 
* in hierarchical clustering, it illustrates the arrangement of the clusters produced by the corresponding analyses. 
* in computational biology, it shows the clustering of genes or samples, sometimes in the margins of heatmaps. 
* in phylogenetics, it displays the evolutionary relationships among various biological taxa. In this case, the dendrogram is also called a phylogenetic tree. The name dendrogram derives from the two ancient greek words δένδρον (déndron), meaning ""tree"", and γράμμα (grámma), meaning ""drawing, mathematical figure"".",8165209372987516652,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,5.342105263157895,1.3779999900301214
267,506,Heap (data structure),http://dbpedia.org/resource/Heap_(data_structure),http://en.wikipedia.org/wiki/Heap_(data_structure),"In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the ""top"" of the heap (with no parents) is called the root node. The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as ""heaps"", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority, or when insertions need to be interspersed with removals of the root node. A common implementation of a heap is the binary heap, in which the tree is a binary tree (see figure). The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm. Heaps are also crucial in several efficient graph algorithms such as Dijkstra's algorithm. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes and a branches for each node always has loga N height. Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap.",3487185493054953077,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,"[""Dijkstra's algorithm""]","[""Dijkstra's algorithm""]",set(),set(),0,1.9378238341968912,1.3892676338823635
268,507,Local outlier factor,http://dbpedia.org/resource/Local_outlier_factor,http://en.wikipedia.org/wiki/Local_outlier_factor,"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours. LOF shares some concepts with DBSCAN and OPTICS such as the concepts of ""core distance"" and ""reachability distance"", which are used for local density estimation.",4333523350381145056,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,"['Hans-Peter Kriegel', 'DBSCAN']","['Hans-Peter Kriegel', 'DBSCAN', 'Outlier']",set(),set(),0,1.9361702127659575,1.2661282816115984
269,508,K-d tree,http://dbpedia.org/resource/K-d_tree,http://en.wikipedia.org/wiki/K-d_tree,"In computer science, a k-d tree (short for k-dimensional tree) is a space-partitioning data structure for organizing points in a k-dimensional space. k-d trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g. range searches and nearest neighbor searches) and creating point clouds. k-d trees are a special case of binary space partitioning trees.",5393281459135371745,related_concept,OPTICS algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.18562874251497,1.372557865945426
270,511,Single-linkage clustering,http://dbpedia.org/resource/Single-linkage_clustering,http://en.wikipedia.org/wiki/Single-linkage_clustering,"In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other. A drawback of this method is that it tends to produce long thin clusters in which nearby elements of the same cluster have small distances, but elements at opposite ends of a cluster may be much farther from each other than two elements of other clusters. This may lead to difficulties in defining classes that could usefully subdivide the data.",6577557391253045266,main_concept,OPTICS algorithm,2024-06-23 21:17:09.728,[],"['Bacillus subtilis', '5S ribosomal RNA', 'Acholeplasma', 'Micrococcus luteus', ""Kruskal's algorithm"", 'UPGMA', ""Ward's method"", 'WPGMA']",set(),set(),0,3.5161290322580645,3.3481447799794745e-304
271,515,Set estimation,http://dbpedia.org/resource/Set_estimation,http://en.wikipedia.org/wiki/Set_estimation,"In statistics, a random vector x is classically represented by a probability density function. In a set-membership approach or set estimation, x is represented by a set X to which x is assumed to belong. This means that the support of the probability distribution function of x is included inside X. On the one hand, representing random vectors by sets makes it possible to provide fewer assumptions on the random variables (such as independence) and dealing with nonlinearities is easier. On the other hand, a probability distribution function provides a more accurate information than a set enclosing its support.",3378875117385796771,related_concept,Outlier,2024-06-23 21:17:09.728,[],['Set estimation'],set(),set(),0,1.4285714285714286,4.758690724322954e-304
272,520,Chauvenet's criterion,http://dbpedia.org/resource/Chauvenet's_criterion,http://en.wikipedia.org/wiki/Chauvenet's_criterion,"In statistical theory, Chauvenet's criterion (named for William Chauvenet) is a means of assessing whether one piece of experimental data — an outlier — from a set of observations, is likely to be spurious.",5051930370953282698,related_concept,Outlier,2024-06-23 21:17:09.728,"[""Chauvenet's criterion""]","[""Chauvenet's criterion"", 'Quantile', ""Grubbs's test for outliers"", 'Outlier']",set(),set(),0,1.2105263157894737,1.3505169884470913
273,521,Studentized residual,http://dbpedia.org/resource/Studentized_residual,http://en.wikipedia.org/wiki/Studentized_residual,"In statistics, a studentized residual is the quotient resulting from the division of a residual by an estimate of its standard deviation. It is a form of a Student's t-statistic, with the estimate of error varying between points. This is an important technique in the detection of outliers. It is among several named in honor of William Sealey Gosset, who wrote under the pseudonym Student. Dividing a statistic by a sample standard deviation is called studentizing, in analogy with standardizing and normalizing.",6269559569321730462,related_concept,Outlier,2024-06-23 21:17:09.728,"[""Student's t-statistic""]","[""Student's t-statistic"", 'Studentized residual', ""Student's t-distribution""]",set(),set(),0,1.4375,1.3513349122481124
274,523,Grubbs's test for outliers,http://dbpedia.org/resource/Grubbs's_test_for_outliers,http://en.wikipedia.org/wiki/Grubbs's_test_for_outliers,,7272760447178723839,related_concept,Outlier,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.27586206896551724,4.212626686393268e-305
275,526,Normal probability plot,http://dbpedia.org/resource/Normal_probability_plot,http://en.wikipedia.org/wiki/Normal_probability_plot,"The normal probability plot is a graphical technique to identify substantive departures from normality. This includes identifying outliers, skewness, kurtosis, a need for transformations, and mixtures. Normal probability plots are made of raw data, residuals from model fits, and estimated parameters. In a normal probability plot (also called a ""normal plot""), the sorted data are plotted vs. values selected to make the resulting image look close to a straight line if the data are approximately normally distributed. Deviations from a straight line suggest departures from normality. The plotting can be manually performed by using a special graph paper, called normal probability paper. With modern computers normal plots are commonly made with software. The normal probability plot is a special case of the Q–Q probability plot for a normal distribution. The theoretical quantiles are generally chosen to approximate either the mean or the median of the corresponding order statistics.",1830112254743369795,related_concept,Outlier,2024-06-23 21:17:09.728,['Normal probability plot'],"['Normal probability plot', 'R (programming language)', 'Probability']",set(),set(),0,1.3414634146341464,1.2819927300856782
276,527,Random sample consensus,http://dbpedia.org/resource/Random_sample_consensus,http://en.wikipedia.org/wiki/Random_sample_consensus,"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations. RANSAC uses repeated random sub-sampling.A basic assumption is that the data consists of ""inliers"", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and ""outliers"" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.",8424959760327079099,related_concept,Outlier,2024-06-23 21:17:09.728,['Random sample consensus'],"['Random sample consensus', 'Data', 'M-estimator']",set(),set(),0,1.4294117647058824,1.1759173941114378
277,528,Sample maximum,http://dbpedia.org/resource/Sample_maximum,http://en.wikipedia.org/wiki/Sample_maximum,,7726910277604577054,related_concept,Outlier,2024-06-23 21:17:09.728,[],"[""Student's t-distribution""]",set(),set(),0,0.3541666666666667,0.3268888913988404
278,537,Gradient method,http://dbpedia.org/resource/Gradient_method,http://en.wikipedia.org/wiki/Gradient_method,"In optimization, a gradient method is an algorithm to solve problems of the form with the search directions defined by the gradient of the function at the current point. Examples of gradient methods are the gradient descent and the conjugate gradient.",2705657018735094391,related_concept,Backpropagation,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.22,4.6482698072682345e-304
279,539,Backpropagation through time,http://dbpedia.org/resource/Backpropagation_through_time,http://en.wikipedia.org/wiki/Backpropagation_through_time,Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.,7926004614590209044,related_concept,Backpropagation,2024-06-23 21:17:09.728,"['Backpropagation through time', 'Backpropagation']","['Backpropagation through time', 'Backpropagation']",set(),set(),0,1.2380952380952381,1.2312902835584913
280,540,Monte Carlo tree search,http://dbpedia.org/resource/Monte_Carlo_tree_search,http://en.wikipedia.org/wiki/Monte_Carlo_tree_search,"In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree. MCTS was combined with neural networks in 2016 and has been used in multiple board games like Chess, Shogi, Checkers, Backgammon, Contract Bridge, Computer Go, Scrabble, and Clobber as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI).",989742854677553928,related_concept,Backpropagation,2024-06-23 21:17:09.728,"['Monte Carlo tree search', 'AI']","['Monte Carlo tree search', 'AI', 'Heuristic']",set(),set(),0,2.416,0.7249615529841619
281,545,AdaBoost,http://dbpedia.org/resource/AdaBoost,http://en.wikipedia.org/wiki/AdaBoost,"AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm formulated by Yoav Freund and Robert Schapire in 1995, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. Usually, AdaBoost is presented for binary classification, although it can be generalized to multiple classes or bounded intervals on the real line. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner. Although AdaBoost is typically used to combine weak base learners (such as decision stumps), it has been shown that it can also effectively combine strong base learners (such as deep decision trees), producing an even more accurate model. Every learning algorithm tends to suit some problem types better than others, and typically has many different parameters and configurations to adjust before it achieves optimal performance on a dataset. AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples.",3349385646874714175,main_concept,Backpropagation,2024-06-23 21:17:09.728,['AdaBoost'],"['AdaBoost', 'LPBoost']",set(),set(),0,2.18,4.724856765314285e-304
282,551,Dynamic programming,http://dbpedia.org/resource/Dynamic_programming,http://en.wikipedia.org/wiki/Dynamic_programming,"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure. If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.",2549432000844596340,related_concept,Backpropagation,2024-06-23 21:17:09.728,['Dynamic programming'],"['Dynamic programming', 'Algorithm', 'Prolog', ""Dijkstra's algorithm"", 'Backtracking']",set(),set(),0,2.6981818181818182,4.267845842228526e-304
283,552,Automatic differentiation,http://dbpedia.org/resource/Automatic_differentiation,http://en.wikipedia.org/wiki/Automatic_differentiation,"In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation, computational differentiation, auto-differentiation, or simply autodiff, is a set of techniques to evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program. Automatic differentiation is distinct from symbolic differentiation and numerical differentiation. Symbolic differentiation faces the difficulty of converting a computer program into a single mathematical expression and can lead to inefficient code. Numerical differentiation (the method of finite differences) can introduce round-off errors in the discretization process and cancellation. Both of these classical methods have problems with calculating higher derivatives, where complexity and errors increase. Finally, both of these classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems.",3824560738441742125,related_concept,Backpropagation,2024-06-23 21:17:09.728,['Automatic differentiation'],"['Automatic differentiation', 'Backpropagation']",set(),set(),0,1.6517412935323383,1.2558238016504353
284,556,Statistical assumptions,http://dbpedia.org/resource/Statistical_assumptions,http://en.wikipedia.org/wiki/Statistical_assumptions,,1446800580072153059,related_concept,Statistical model,2024-06-23 21:17:09.728,[],"['Statistics', 'Statistical assumptions']",set(),set(),0,0.1951219512195122,0.3977132984064239
285,558,Semiparametric model,http://dbpedia.org/resource/Semiparametric_model,http://en.wikipedia.org/wiki/Semiparametric_model,"In statistics, a semiparametric model is a statistical model that has parametric and nonparametric components. A statistical model is a parameterized family of distributions: indexed by a parameter . 
* A parametric model is a model in which the indexing parameter is a vector in -dimensional Euclidean space, for some nonnegative integer . Thus, is finite-dimensional, and . 
* With a nonparametric model, the set of possible values of the parameter is a subset of some space , which is not necessarily finite-dimensional. For example, we might consider the set of all distributions with mean 0. Such spaces are vector spaces with topological structure, but may not be finite-dimensional as vector spaces. Thus, for some possibly infinite-dimensional space . 
* With a semiparametric model, the parameter has both a finite-dimensional component and an infinite-dimensional component (often a real-valued function defined on the real line). Thus, , where is an infinite-dimensional space. It may appear at first that semiparametric models include nonparametric models, since they have an infinite-dimensional as well as a finite-dimensional component. However, a semiparametric model is considered to be ""smaller"" than a completely nonparametric model because we are often interested only in the finite-dimensional component of . That is, the infinite-dimensional component is regarded as a nuisance parameter. In nonparametric models, by contrast, the primary interest is in estimating the infinite-dimensional parameter. Thus the estimation task is statistically harder in nonparametric models. These models often use smoothing or kernels.",7915059171286313612,related_concept,Statistical model,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.3181818181818183,1.333338392214548
286,560,Parametric model,http://dbpedia.org/resource/Parametric_model,http://en.wikipedia.org/wiki/Parametric_model,"In statistics, a parametric model or parametric family or finite-dimensional model is a particular class of statistical models. Specifically, a parametric model is a family of probability distributions that has a finite number of parameters.",4055347547417014772,related_concept,Statistical model,2024-06-23 21:17:09.728,[],['Parametric model'],set(),set(),0,2.282608695652174,1.328519375153682
287,564,Deterministic system,http://dbpedia.org/resource/Deterministic_system,http://en.wikipedia.org/wiki/Deterministic_system,"In mathematics, computer science and physics, a deterministic system is a system in which no randomness is involved in the development of future states of the system. A deterministic model will thus always produce the same output from a given starting condition or initial state.",3523636841576274844,related_concept,Statistical model,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.223684210526316,1.173515976915598
288,569,Statistical theory,http://dbpedia.org/resource/Statistical_theory,http://en.wikipedia.org/wiki/Statistical_theory,"The theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics. The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures. Apart from philosophical considerations about how to make statistical inferences and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.",6784849518860196598,related_concept,Statistical model,2024-06-23 21:17:09.728,[],"['Statistical theory', 'Statistical model']",set(),set(),0,1.6814404432132963,1.3623241811193751
289,570,Statistical model specification,http://dbpedia.org/resource/Statistical_model_specification,http://en.wikipedia.org/wiki/Statistical_model_specification,"In statistics, model specification is part of the process of building a statistical model: specification consists of selecting an appropriate functional form for the model and choosing which variables to include. For example, given personal income together with years of schooling and on-the-job experience , we might specify a functional relationship as follows: where is the unexplained error term that is supposed to comprise independent and identically distributed Gaussian variables. The statistician Sir David Cox has said, ""How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis"".",4755626479108141836,related_concept,Statistical model,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1948051948051948,1.3952942862488464
290,580,Mallows's Cp,http://dbpedia.org/resource/Mallows's_Cp,http://en.wikipedia.org/wiki/Mallows's_Cp,"In statistics, Mallows's Cp, named for Colin Lingwood Mallows, is used to assess the fit of a regression model that has been estimated using ordinary least squares. It is applied in the context of model selection, where a number of predictor variables are available for predicting some outcome, and the goal is to find the best model involving a subset of these predictors. A small value of Cp means that the model is relatively precise. Mallows's Cp has been shown to be equivalent to Akaike information criterion in the special case of Gaussian linear regression.",683883590192583387,related_concept,Stepwise regression,2024-06-23 21:17:09.728,"[""Mallows's Cp"", 'Akaike information criterion']","[""Mallows's Cp"", 'Akaike information criterion']",set(),set(),0,3.033333333333333,1.3602460661007316
291,583,Akaike information criterion,http://dbpedia.org/resource/Akaike_information_criterion,http://en.wikipedia.org/wiki/Akaike_information_criterion,"The Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. AIC is founded on information theory. When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model. In estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting. The Akaike information criterion is named after the Japanese statistician Hirotugu Akaike, who formulated it. It now forms the basis of a paradigm for the foundations of statistics and is also widely used for statistical inference.",6389124348038966288,related_concept,Stepwise regression,2024-06-23 21:17:09.728,"['Akaike information criterion', 'AI']","['Akaike information criterion', 'AI', 'Kullback–Leibler divergence', ""Welch's t-test"", 'Interval estimation', 'Statistical inference', 'Hypothesis', 'Bayesian inference', 'Bayesianism', 'Normal distribution']",set(),set(),0,1.69,1.3967388855117004
292,587,Residual sum of squares,http://dbpedia.org/resource/Residual_sum_of_squares,http://en.wikipedia.org/wiki/Residual_sum_of_squares,"In statistics, the residual sum of squares (RSS), also known as the sum of squared residuals (SSR) or the sum of squared estimate of errors (SSE), is the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model, such as a linear regression. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection. In general, total sum of squares = explained sum of squares + residual sum of squares. For a proof of this in the multivariate ordinary least squares (OLS) case, see partitioning in the general OLS model.",3452575220578907052,related_concept,Stepwise regression,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.75,1.1364924782149932
293,588,Freedman's paradox,http://dbpedia.org/resource/Freedman's_paradox,http://en.wikipedia.org/wiki/Freedman's_paradox,"In statistical analysis, Freedman's paradox, named after David Freedman, is a problem in model selection whereby predictor variables with no relationship to the dependent variable can pass tests of significance – both individually via a t-test, and jointly via an F-test for the significance of the regression. Freedman demonstrated (through simulation and asymptotic calculation) that this is a common occurrence when the number of variables is similar to the number of data points. Specifically, if the dependent variable and k regressors are independent normal variables, and there are n observations, then as k and n jointly go to infinity in the ratio k/n=ρ, 1. 
* the R2 goes to ρ, 2. 
* the F-statistic for the overall regression goes to 1.0, and 3. 
* the number of spuriously significant regressors goes to αk where α is the chosen critical probability (probability of Type I error for a regressor). This third result is intuitive because it says that the number of Type I errors equals the probability of a Type I error on an individual parameter times the number of parameters for which significance is tested. More recently, new information-theoretic estimators have been developed in an attempt to reduce this problem, in addition to the accompanying issue of model selection bias, whereby estimators of predictor variables that have a weak relationship with the response variable are biased.",6673769343696547964,related_concept,Stepwise regression,2024-06-23 21:17:09.728,"[""Freedman's paradox"", 'F-test']","[""Freedman's paradox"", 'F-test']",set(),set(),0,2.4166666666666665,1.2888206474652926
294,590,Least-angle regression,http://dbpedia.org/resource/Least-angle_regression,http://en.wikipedia.org/wiki/Least-angle_regression,"In statistics, least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. Suppose we expect a response variable to be determined by a linear combination of a subset of potential covariates. Then the LARS algorithm provides a means of producing an estimate of which variables to include, as well as their coefficients. Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one's correlations with the residual.",3341232562566165124,related_concept,Stepwise regression,2024-06-23 21:17:09.728,[],['Least-angle regression'],set(),set(),0,1.2771084337349397,4.9787983881433945e-304
295,592,Lasso (statistics),http://dbpedia.org/resource/Lasso_(statistics),http://en.wikipedia.org/wiki/Lasso_(statistics),"In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model. It was originally introduced in geophysics, and later by Robert Tibshirani, who coined the term. Lasso was originally formulated for linear regression models. This simple case reveals a substantial amount about the estimator. These include its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression) the coefficient estimates do not need to be unique if covariates are collinear. Though originally defined for linear regression, lasso regularization is easily extended to other statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators. Lasso's ability to perform subset selection relies on the form of the constraint and has a variety of interpretations including in terms of geometry, Bayesian statistics and convex analysis. The LASSO is closely related to basis pursuit denoising.",889071197607577810,related_concept,Stepwise regression,2024-06-23 21:17:09.728,"['M-estimator', 'Bayesian statistics']","['M-estimator', 'Bayesian statistics', 'Ridge regression', 'Algorithm', 'Akaike information criterion', 'AI']",set(),set(),0,1.1944444444444444,1.2174030861947396
296,600,HSL and HSV,http://dbpedia.org/resource/HSL_and_HSV,http://en.wikipedia.org/wiki/HSL_and_HSV,"HSL (for hue, saturation, lightness) and HSV (for hue, saturation, value; also known as HSB, for hue, saturation, brightness) are alternative representations of the RGB color model, designed in the 1970s by computer graphics researchers to more closely align with the way human vision perceives color-making attributes. In these models, colors of each hue are arranged in a radial slice, around a central axis of neutral colors which ranges from black at the bottom to white at the top. The HSL representation models the way different paints mix together to create color in the real world, with the lightness dimension resembling the varying amounts of black or white paint in the mixture (e.g. to create ""light red"", a red pigment can be mixed with white paint; this white paint corresponds to a high ""lightness"" value in the HSL representation). Fully saturated colors are placed around a circle at a lightness value of ½, with a lightness value of 0 or 1 corresponding to fully black or white, respectively. Meanwhile, the HSV representation models how colors appear under light. The difference between HSL and HSV is that a color with maximum lightness in HSL is pure white, but a color with maximum value/brightness in HSV is analogous to shining a white light on a colored object (e.g. shining a bright white light on a red object causes the object to still appear red, just brighter and more intense, while shining a dim light on a red object causes the object to appear darker and less bright). The issue with both HSV and HSL is that these approaches do not effectively separate color into their three value components according to human perception of color. This can be seen when the saturation settings are altered — it is quite easy to notice the difference in perceptual lightness despite the ""V"" or ""L"" setting being fixed.",173529586442372033,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,"['HSL and HSV', 'Mean']","['HSL and HSV', 'RGB color space', 'Computer science']",set(),set(),0,3.237885462555066,1.4133671135449994
297,601,Transcription factor,http://dbpedia.org/resource/Transcription_factor,http://en.wikipedia.org/wiki/Transcription_factor,"In molecular biology, a transcription factor (TF) (or sequence-specific DNA-binding factor) is a protein that controls the rate of transcription of genetic information from DNA to messenger RNA, by binding to a specific DNA sequence. The function of TFs is to regulate—turn on and off—genes in order to make sure that they are expressed in the desired cells at the right time and in the right amount throughout the life of the cell and the organism. Groups of TFs function in a coordinated fashion to direct cell division, cell growth, and cell death throughout life; cell migration and organization (body plan) during embryonic development; and intermittently in response to signals from outside the cell, such as a hormone. There are up to 1600 TFs in the human genome. Transcription factors are members of the proteome as well as regulome. TFs work alone or with other proteins in a complex, by promoting (as an activator), or blocking (as a repressor) the recruitment of RNA polymerase (the enzyme that performs the transcription of genetic information from DNA to RNA) to specific genes. A defining feature of TFs is that they contain at least one DNA-binding domain (DBD), which attaches to a specific sequence of DNA adjacent to the genes that they regulate. TFs are grouped into classes based on their DBDs. Other proteins such as coactivators, chromatin remodelers, histone acetyltransferases, histone deacetylases, kinases, and methylases are also essential to gene regulation, but lack DNA-binding domains, and therefore are not TFs. TFs are of interest in medicine because TF mutations can cause specific diseases, and medications can be potentially targeted toward them.",4287750129506617861,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,['Transcription factor'],['Transcription factor'],set(),set(),0,2.4259421560035057,1.2801240712502586
298,602,Fuzzy set,http://dbpedia.org/resource/Fuzzy_set,http://en.wikipedia.org/wiki/Fuzzy_set,"In mathematics, fuzzy sets (a.k.a. uncertain sets) are sets whose elements have degrees of membership. Fuzzy sets were introduced independently by Lotfi A. Zadeh in 1965 as an extension of the classical notion of set.At the same time, defined a more general kind of structure called an , which he studied in an abstract algebraic context. Fuzzy relations, which are now used throughout fuzzy mathematics and have applications in areas such as linguistics, decision-making, and clustering, are special cases of L-relations when L is the unit interval [0, 1]. In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition—an element either belongs or does not belong to the set. By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set; this is described with the aid of a membership function valued in the real unit interval [0, 1]. Fuzzy sets generalize classical sets, since the indicator functions (aka characteristic functions) of classical sets are special cases of the membership functions of fuzzy sets, if the latter only takes values 0 or 1. In fuzzy set theory, classical bivalent sets are usually called crisp sets. The fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as bioinformatics.",7762989145036683133,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,['Fuzzy set'],['Fuzzy set'],set(),set(),0,2.9375,1.425886074772728
299,603,HCL color space,http://dbpedia.org/resource/HCL_color_space,http://en.wikipedia.org/wiki/HCL_color_space,"HCL (Hue-Chroma-Luminance) or LCh refers to any of the many cylindrical color space models that are designed to accord with human perception of color with the three parameters. Lch has been adopted by information visualization practitioners to present data without the bias implicit in using varying saturation. They are, in general, designed to have characteristics of both cylindrical translations of the RGB color space, such as HSL and HSV, and the L*a*b* color space. Some conflicting definitions of the terms are: 
* A name for a cylindrical transformation of CIELuv (CIELChuv) employed by Ihaka (2003) and adopted by Zeileis et al. (2009, 2020). This name appears to be the one most commonly used in information visualization. Ihaka, Zeileis, and co-authors also provide software implementations and web pages to promote its use. 
* A name for cylindrical CIELab (CIELChab), employed by chroma.js. 
* ""HCL"" designed in 2005 by Sarifuddin and Missaou, which is a transformation of whatever type of RGB color space is in use.",8390934982334445599,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,"['HSL and HSV', 'RGB color space']","['HSL and HSV', 'RGB color space']",set(),set(),0,0.9724770642201835,1.422904576935658
300,607,FLAME Clustering,http://dbpedia.org/resource/FLAME_Clustering,http://en.wikipedia.org/wiki/FLAME_Clustering,,7747814009464954734,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,[],['Fuzzy clustering'],set(),set(),0,0.3333333333333333,0.5263403563040531
301,609,Data point,http://dbpedia.org/resource/Data_point,http://en.wikipedia.org/wiki/Data_point,,8061728448049935751,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,[],['Statistical inference'],set(),set(),0,2.259259259259259,0.2898137696279109
302,610,Image segmentation,http://dbpedia.org/resource/Image_segmentation,http://en.wikipedia.org/wiki/Image_segmentation,"In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics. The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different color respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like marching cubes.",4185356347714285093,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,['Image segmentation'],"['Image segmentation', 'Mean', 'Histogram', 'Segmentation-based object categorization', 'Simulated annealing', ""Bayes' theorem""]",set(),set(),0,1.5705128205128205,1.3258613507198806
303,611,RGB color space,http://dbpedia.org/resource/RGB_color_space,http://en.wikipedia.org/wiki/RGB_color_space,,2941693835984050241,related_concept,Fuzzy clustering,2024-06-23 21:17:09.728,[],['RGB color space'],set(),set(),0,1.1092436974789917,0.42364741210944806
304,613,Universal Darwinism,http://dbpedia.org/resource/Universal_Darwinism,http://en.wikipedia.org/wiki/Universal_Darwinism,"Universal Darwinism, also known as generalized Darwinism, universal selection theory, or Darwinian metaphysics, is a variety of approaches that extend the theory of Darwinism beyond its original domain of biological evolution on Earth. Universal Darwinism aims to formulate a generalized version of the mechanisms of variation, selection and heredity proposed by Charles Darwin, so that they can apply to explain evolution in a wide variety of other domains, including psychology, linguistics, economics, culture, medicine, computer science, and physics.",4772535059916583769,related_concept,Natural selection,2024-06-23 21:17:09.728,['Universal Darwinism'],['Universal Darwinism'],{'Evolution'},set(),0,1.7685589519650655,2.422538882911001e-304
305,614,Frequency-dependent selection,http://dbpedia.org/resource/Frequency-dependent_selection,http://en.wikipedia.org/wiki/Frequency-dependent_selection,"Frequency-dependent selection is an evolutionary process by which the fitness of a phenotype or genotype depends on the phenotype or genotype composition of a given population. 
* In positive frequency-dependent selection, the fitness of a phenotype or genotype increases as it becomes more common. 
* In negative frequency-dependent selection, the fitness of a phenotype or genotype decreases as it becomes more common. This is an example of balancing selection. 
* More generally, frequency-dependent selection includes when biological interactions make an individual's fitness depend on the frequencies of other phenotypes or genotypes in the population. Frequency-dependent selection is usually the result of interactions between species (predation, parasitism, or competition), or between genotypes within species (usually competitive or symbiotic), and has been especially frequently discussed with relation to anti-predator adaptations. Frequency-dependent selection can lead to polymorphic equilibria, which result from interactions among genotypes within species, in the same way that multi-species equilibria require interactions between species in competition (e.g. where αij parameters in Lotka-Volterra competition equations are non-zero). Frequency-dependent selection can also lead to dynamical chaos when some individuals' fitnesses become very low at intermediate allele frequencies.",5287902959200149239,related_concept,Natural selection,2024-06-23 21:17:09.728,['Frequency-dependent selection'],['Frequency-dependent selection'],set(),set(),0,2.42,0.5169279820086103
306,615,Ecological selection,http://dbpedia.org/resource/Ecological_selection,http://en.wikipedia.org/wiki/Ecological_selection,"Ecological selection (or environmental selection or survival selection or individual selection or asexual selection) refers to natural selection without sexual selection, i.e. strictly ecological processes that operate on a species' inherited traits without reference to mating or secondary sex characteristics. The variant names describe varying circumstances where sexual selection is wholly suppressed as a mating factor. Ecologists often study ecological selection when examining the abundance of individuals per population across regions, and what governs such abundances.",3214486591890865578,related_concept,Natural selection,2024-06-23 21:17:09.728,['Ecological selection'],['Ecological selection'],set(),set(),0,0.3402061855670103,0.3449809668688772
307,616,Group selection,http://dbpedia.org/resource/Group_selection,http://en.wikipedia.org/wiki/Group_selection,"Group selection is a proposed mechanism of evolution in which natural selection acts at the level of the group, instead of at the level of the individual or gene. Early authors such as V. C. Wynne-Edwards and Konrad Lorenz argued that the behavior of animals could affect their survival and reproduction as groups, speaking for instance of actions for the good of the species. In the 1930s, R.A. Fisher and J.B.S. Haldane proposed the concept of kin selection, a form of altruism from the gene-centered view of evolution, arguing that animals should sacrifice for their relatives, and thereby implying that they should not sacrifice for non-relatives. From the mid 1960s, evolutionary biologists such as John Maynard Smith, W. D. Hamilton, George C. Williams, and Richard Dawkins argued that natural selection acted primarily at the level of the gene. They argued on the basis of mathematical models that individuals would not altruistically sacrifice fitness for the sake of a group unless it would ultimately increase the likelihood of an individual passing on their genes. A consensus emerged that group selection did not occur, including in special situations such as the haplodiploid social insects like honeybees (in the Hymenoptera), where kin selection explains the behaviour of non-reproductives equally well, since the only way for them to reproduce their genes is via kin. In 1994 David Sloan Wilson and Elliott Sober argued for multi-level selection, including group selection, on the grounds that groups, like individuals, could compete. In 2010 three authors including E. O. Wilson, known for his work on social insects especially ants, again revisited the arguments for group selection. They argued that group selection can occur when competition between two or more groups, some containing altruistic individuals who act cooperatively together, is more important for survival than competition between individuals within each group, provoking a strong rebuttal from a large group of ethologists.",5553396676331852539,related_concept,Natural selection,2024-06-23 21:17:09.728,['Group selection'],"['Group selection', 'Adaptation', 'Kin selection']",set(),set(),0,1.2947368421052632,2.39917187245378e-304
308,617,Genetic diversity,http://dbpedia.org/resource/Genetic_diversity,http://en.wikipedia.org/wiki/Genetic_diversity,"Genetic diversity is the total number of genetic characteristics in the genetic makeup of a species, it ranges widely from the number of species to differences within species and can be attributed to the span of survival for a species. It is distinguished from genetic variability, which describes the tendency of genetic characteristics to vary. Genetic diversity serves as a way for populations to adapt to changing environments. With more variation, it is more likely that some individuals in a population will possess variations of alleles that are suited for the environment. Those individuals are more likely to survive to produce offspring bearing that allele. The population will continue for more generations because of the success of these individuals. The academic field of population genetics includes several hypotheses and theories regarding genetic diversity. The neutral theory of evolution proposes that diversity is the result of the accumulation of neutral substitutions. Diversifying selection is the hypothesis that two subpopulations of a species live in different environments that select for different alleles at a particular locus. This may occur, for instance, if a species has a large range relative to the mobility of individuals within it. Frequency-dependent selection is the hypothesis that as alleles become more common, they become more vulnerable. This occurs in host–pathogen interactions, where a high frequency of a defensive allele among the host means that it is more likely that a pathogen will spread if it is able to overcome that allele.",1404283494192657086,related_concept,Natural selection,2024-06-23 21:17:09.728,"['Frequency-dependent selection', 'Genetic diversity']","['Genetic diversity', 'Frequency-dependent selection', 'Natural selection']",set(),set(),0,4.917391304347826,2.420220901084144e-304
309,618,Reinforcement (speciation),http://dbpedia.org/resource/Reinforcement_(speciation),http://en.wikipedia.org/wiki/Reinforcement_(speciation),"Reinforcement is a process of speciation where natural selection increases the reproductive isolation (further divided to pre-zygotic isolation and post-zygotic isolation) between two populations of species. This occurs as a result of selection acting against the production of hybrid individuals of low fitness. The idea was originally developed by Alfred Russel Wallace and is sometimes referred to as the Wallace effect. The modern concept of reinforcement originates from Theodosius Dobzhansky. He envisioned a species separated allopatrically, where during secondary contact the two populations mate, producing hybrids with lower fitness. Natural selection results from the hybrid's inability to produce viable offspring; thus members of one species who do not mate with members of the other have greater reproductive success. This favors the evolution of greater prezygotic isolation (differences in behavior or biology that inhibit formation of hybrid zygotes). Reinforcement is one of the few cases in which selection can favor an increase in prezygotic isolation, influencing the process of speciation directly. This aspect has been particularly appealing among evolutionary biologists. The support for reinforcement has fluctuated since its inception, and terminological confusion and differences in usage over history have led to multiple meanings and complications. Various objections have been raised by evolutionary biologists as to the plausibility of its occurrence. Since the 1990s, data from theory, experiments, and nature have overcome many of the past objections, rendering reinforcement widely accepted, though its prevalence in nature remains unknown. Numerous models have been developed to understand its operation in nature, most relying on several facets: genetics, population structures, influences of selection, and mating behaviors. Empirical support for reinforcement exists, both in the laboratory and in nature. Documented examples are found in a wide range of organisms: both vertebrates and invertebrates, fungi, and plants. The secondary contact of originally separated incipient species (the initial stage of speciation) is increasing due to human activities such as the introduction of invasive species or the modification of natural habitats. This has implications for measures of biodiversity and may become more relevant in the future.",2396238341627039141,related_concept,Natural selection,2024-06-23 21:17:09.728,['Natural selection'],"['Natural selection', 'The Genetical Theory of Natural Selection']",set(),set(),0,0.9031339031339032,2.404814157089709e-304
310,619,Evolution,http://dbpedia.org/resource/Evolution,http://en.wikipedia.org/wiki/Evolution,"Evolution is change in the heritable characteristics of biological populations over successive generations. These characteristics are the expressions of genes, which are passed on from parent to offspring during reproduction. Variation tends to exist within any given population as a result of genetic mutation and recombination. Evolution occurs when evolutionary processes such as natural selection (including sexual selection) and genetic drift act on this variation, resulting in certain characteristics becoming more common or more rare within a population. The evolutionary pressures that determine whether a characteristic is common or rare within a population constantly change, resulting in a change in heritable characteristics arising over successive generations. It is this process of evolution that has given rise to biodiversity at every level of biological organisation, including the levels of species, individual organisms, and molecules. The theory of evolution by natural selection was conceived independently by Charles Darwin and Alfred Russel Wallace in the mid-19th century and was set out in detail in Darwin's book On the Origin of Species. Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive (2) traits vary among individuals with respect to their morphology, physiology, and behaviour (phenotypic variation); (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness). In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics. In the early 20th century, other competing ideas of evolution such as mutationism and orthogenesis were refuted as the modern synthesis concluded Darwinian evolution acts on Mendelian genetic variation. All life on Earth shares a last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite to microbial mat fossils to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits are more similar among species that share a more recent common ancestor, and these traits can be used to reconstruct phylogenetic trees. Evolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but numerous other scientific and industrial fields, including agriculture, medicine, and computer science.",6268285179904766131,related_concept,Natural selection,2024-06-23 21:17:09.728,['Evolution'],"['Evolution', 'Heritability', 'Natural selection', 'Adaptation', 'Evolutionary algorithms', 'Genetic algorithm']",{'Evolution'},set(),0,14.039795918367346,2.4149119360449273e-304
311,620,Evolutionary biology,http://dbpedia.org/resource/Evolutionary_biology,http://en.wikipedia.org/wiki/Evolutionary_biology,"Evolutionary biology is the subfield of biology that studies the evolutionary processes (natural selection, common descent, speciation) that produced the diversity of life on Earth. Simply, it is also defined as the study of the history of life forms on Earth. Evolution is based on the theory that all species are related and they gradually change over time. In a population, the genetic variations affect the physical characteristics i.e. phenotypes of an organism. These changes in the phenotypes will be an advantage to some organisms, which will then be passed onto their offspring. Some examples of evolution in species over many generations are the Peppered Moth and Flightless birds. In the 1930s, the discipline of evolutionary biology emerged through what Julian Huxley called the modern synthesis of understanding, from previously unrelated fields of biological research, such as genetics and ecology, systematics, and paleontology. The importance of studying Evolutionary biology is mainly to understand the principles behind the origin and extinction of species. The investigational range of current research has widened to encompass the genetic architecture of adaptation, molecular evolution, and the different forces that contribute to evolution, such as sexual selection, genetic drift, and biogeography. Moreover, the newer field of evolutionary developmental biology (""evo-devo"") investigates how embryogenesis, the development of the embryo, is controlled, thus yielding a wider synthesis that integrates developmental biology with the fields of study covered by the earlier evolutionary synthesis.",6165060975489439267,related_concept,Natural selection,2024-06-23 21:17:09.728,"['Evolution', 'Evolutionary biology']","['Evolution', 'Evolutionary biology', 'Natural selection']",set(),set(),0,5.78434065934066,2.435755472006208e-304
312,621,Sexual selection,http://dbpedia.org/resource/Sexual_selection,http://en.wikipedia.org/wiki/Sexual_selection,"Sexual selection is a mode of natural selection in which members of one biological sex choose mates of the other sex to mate with (intersexual selection), and compete with members of the same sex for access to members of the opposite sex (intrasexual selection). These two forms of selection mean that some individuals have greater reproductive success than others within a population, for example because they are more attractive or prefer more attractive partners to produce offspring. Successful males benefit from frequent mating and monopolizing access to one or more fertile females. Females can maximise the return on the energy they invest in reproduction by selecting and mating with the best males. The concept was first articulated by Charles Darwin who wrote of a ""second agency"" other than natural selection, in which competition between mate candidates could lead to speciation. The theory was given a mathematical basis by Ronald Fisher in the early 20th century. Sexual selection can lead males to extreme efforts to demonstrate their fitness to be chosen by females, producing sexual dimorphism in secondary sexual characteristics, such as the ornate plumage of birds-of-paradise and peafowl, or the antlers of deer. This is caused by a positive feedback mechanism known as a Fisherian runaway, where the passing-on of the desire for a trait in one sex is as important as having the trait in the other sex in producing the runaway effect. Although the sexy son hypothesis indicates that females would prefer male offspring, Fisher's principle explains why the sex ratio is most often 1:1. Sexual selection is widely distributed in the animal kingdom, and is also found in plants and fungi.",277345122776593907,related_concept,Natural selection,2024-06-23 21:17:09.728,['Sexual selection'],"['Sexual selection', 'The Genetical Theory of Natural Selection']",set(),set(),0,2.9773584905660377,2.396800481834736e-304
313,623,Microevolution,http://dbpedia.org/resource/Microevolution,http://en.wikipedia.org/wiki/Microevolution,"Microevolution is the change in allele frequencies that occurs over time within a population. This change is due to four different processes: mutation, selection (natural and artificial), gene flow and genetic drift. This change happens over a relatively short (in evolutionary terms) amount of time compared to the changes termed macroevolution. Population genetics is the branch of biology that provides the mathematical structure for the study of the process of microevolution. Ecological genetics concerns itself with observing microevolution in the wild. Typically, observable instances of evolution are examples of microevolution; for example, bacterial strains that have antibiotic resistance. Microevolution may lead to speciation, which provides the raw material for macroevolution.",1206490906256040296,related_concept,Natural selection,2024-06-23 21:17:09.728,['Microevolution'],"['Microevolution', 'Natural selection', 'Evolution']",set(),set(),0,1.7392550143266476,0.5647717003750579
314,624,Evolutionary psychology,http://dbpedia.org/resource/Evolutionary_psychology,http://en.wikipedia.org/wiki/Evolutionary_psychology,"Evolutionary psychology is a theoretical approach in psychology that examines cognition and behavior from a modern evolutionary perspective. It seeks to identify which human psychological traits are evolved adaptations – that is, the functional products of natural selection or sexual selection in human evolution. Adaptationist thinking about physiological mechanisms, such as the heart, lungs, and immune system, is common in evolutionary biology. Some evolutionary psychologists apply the same thinking to psychology, arguing that the modularity of mind is similar to that of the body and with different modular adaptations serving different functions. These evolutionary psychologists argue that much of human behavior is the output of psychological adaptations that evolved to solve recurrent problems in human ancestral environments. Some evolutionary psychologists argue that evolutionary theory can provide a foundational, metatheoretical framework that integrates the entire field of psychology in the same way evolutionary biology has for biology. Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations, including the abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, and cooperate with others. Findings have been made regarding human social behaviour related to infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price, and parental investment. The theories and findings of evolutionary psychology have applications in many fields, including economics, environment, health, law, management, psychiatry, politics, and literature. Criticism of evolutionary psychology involves questions of testability, cognitive and evolutionary assumptions (such as modular functioning of the brain, and large uncertainty about the ancestral environment), importance of non-genetic and non-adaptive explanations, as well as political and ethical issues due to interpretations of research results.",5249102466348237458,related_concept,Natural selection,2024-06-23 21:17:09.728,"['Evolution', 'Evolutionary psychology', 'Adaptation']","['Evolution', 'Evolutionary psychology', 'Adaptation', 'Evolutionary biology', 'Sexual selection', 'Consistency', 'PDF', 'Natural selection']",set(),set(),0,2.2380546075085324,2.409628257600388e-304
315,625,Disruptive selection,http://dbpedia.org/resource/Disruptive_selection,http://en.wikipedia.org/wiki/Disruptive_selection,"Disruptive selection, also called diversifying selection, describes changes in population genetics in which extreme values for a trait are favored over intermediate values. In this case, the variance of the trait increases and the population is divided into two distinct groups. In this more individuals acquire peripheral character value at both ends of the distribution curve.",7743631372488509580,related_concept,Natural selection,2024-06-23 21:17:09.728,['Disruptive selection'],"['Disruptive selection', 'Natural selection']",set(),set(),0,2.238095238095238,0.542922000214386
316,627,Stabilizing selection,http://dbpedia.org/resource/Stabilizing_selection,http://en.wikipedia.org/wiki/Stabilizing_selection,"Stabilizing selection (not to be confused with negative or purifying selection) is a type of natural selection in which the population mean stabilizes on a particular non-extreme trait value. This is thought to be the most common mechanism of action for natural selection because most traits do not appear to change drastically over time. Stabilizing selection commonly uses negative selection (a.k.a. purifying selection) to select against extreme values of the character. Stabilizing selection is the opposite of disruptive selection. Instead of favoring individuals with extreme phenotypes, it favors the intermediate variants. Stabilizing selection tends to remove the more severe phenotypes, resulting in the reproductive success of the norm or average phenotypes. This means that most common phenotype in the population is selected for and continues to dominate in future generations.",7592039446442186160,related_concept,Natural selection,2024-06-23 21:17:09.728,['Stabilizing selection'],"['Stabilizing selection', 'Evolution']",set(),set(),0,3.5652173913043477,0.7384453461294096
317,628,Kin selection,http://dbpedia.org/resource/Kin_selection,http://en.wikipedia.org/wiki/Kin_selection,"Kin selection is the evolutionary strategy that favours the reproductive success of an organism's relatives, even when at a cost to the organism's own survival and reproduction. Kin altruism can look like altruistic behaviour whose evolution is driven by kin selection. Kin selection is an instance of inclusive fitness, which combines the number of offspring produced with the number an individual can ensure the production of by supporting others, such as siblings. Charles Darwin discussed the concept of kin selection in his 1859 book, On the Origin of Species, where he reflected on the puzzle of sterile social insects, such as honey bees, which leave reproduction to their mothers, arguing that a selection benefit to related organisms (the same ""stock"") would allow the evolution of a trait that confers the benefit but destroys an individual at the same time. R.A. Fisher in 1930 and J.B.S. Haldane in 1932 set out the mathematics of kin selection, with Haldane famously joking that he would willingly die for two brothers or eight cousins. In 1964, W.D. Hamilton popularised the concept and the major advance in the mathematical treatment of the phenomenon by George R. Price which has become known as Hamilton's rule. In the same year, John Maynard Smith used the term ""kin selection"" for the first time. According to Hamilton's rule, kin selection causes genes to increase in frequency when the genetic relatedness of a recipient to an actor multiplied by the benefit to the recipient is greater than the reproductive cost to the actor. Hamilton proposed two mechanisms for kin selection. First, kin recognition allows individuals to be able to identify their relatives. Second, in viscous populations, populations in which the movement of organisms from their place of birth is relatively slow, local interactions tend to be among relatives by default. The viscous population mechanism makes kin selection and social cooperation possible in the absence of kin recognition. In this case, nurture kinship, the treatment of individuals as kin as a result of living together, is sufficient for kin selection, given reasonable assumptions about population dispersal rates. Note that kin selection is not the same thing as group selection, where natural selection is believed to act on the group as a whole. In humans, altruism is both more likely and on a larger scale with kin than with unrelated individuals; for example, humans give presents according to how closely related they are to the recipient. In other species, vervet monkeys use allomothering, where related females such as older sisters or grandmothers often care for young, according to their relatedness. The social shrimp Synalpheus regalis protects juveniles within highly related colonies.",232258097665177349,related_concept,Natural selection,2024-06-23 21:17:09.728,['Kin selection'],"['Kin selection', 'Evolution']",set(),set(),0,1.5194585448392555,0.25124177984897367
318,629,The Genetical Theory of Natural Selection,http://dbpedia.org/resource/The_Genetical_Theory_of_Natural_Selection,http://en.wikipedia.org/wiki/The_Genetical_Theory_of_Natural_Selection,"The Genetical Theory of Natural Selection is a book by Ronald Fisher which combines Mendelian genetics with Charles Darwin's theory of natural selection, with Fisher being the first to argue that ""Mendelism therefore validates Darwinism"" and stating with regard to mutations that ""The vast majority of large mutations are deleterious; small mutations are both far more frequent and more likely to be useful"", thus refuting orthogenesis. First published in 1930 by The Clarendon Press, it is one of the most important books of the modern synthesis, and helped define population genetics. It is commonly cited in biology books, outlining many concepts that are still considered important such as Fisherian runaway, Fisher's principle, reproductive value, Fisher's fundamental theorem of natural selection, Fisher's geometric model, the sexy son hypothesis, mimicry and the evolution of dominance. It was dictated to his wife in the evenings as he worked at Rothamsted Research in the day.",2574024107368155953,related_concept,Natural selection,2024-06-23 21:17:09.728,['The Genetical Theory of Natural Selection'],['The Genetical Theory of Natural Selection'],set(),set(),0,0.7717391304347826,0.4173839575367288
319,630,Adaptation,http://dbpedia.org/resource/Adaptation,http://en.wikipedia.org/wiki/Adaptation,"In biology, adaptation has three related meanings. Firstly, it is the dynamic evolutionary process of natural selection that fits organisms to their environment, enhancing their evolutionary fitness. Secondly, it is a state reached by the population during that process. Thirdly, it is a phenotypic trait or adaptive trait, with a functional role in each individual organism, that is maintained and has evolved through natural selection. Historically, adaptation has been described from the time of the ancient Greek philosophers such as Empedocles and Aristotle. In 18th and 19th century natural theology, adaptation was taken as evidence for the existence of a deity. Charles Darwin proposed instead that it was explained by natural selection. Adaptation is related to biological fitness, which governs the rate of evolution as measured by change in allele frequencies. Often, two or more species co-adapt and co-evolve as they develop adaptations that interlock with those of the other species, such as with flowering plants and pollinating insects. In mimicry, species evolve to resemble other species; in Müllerian mimicry this is a mutually beneficial co-evolution as each of a group of strongly defended species (such as wasps able to sting) come to advertise their defenses in the same way. Features evolved for one purpose may be co-opted for a different one, as when the insulating feathers of dinosaurs were co-opted for bird flight. Adaptation is a major topic in the philosophy of biology, as it concerns function and purpose (teleology). Some biologists try to avoid terms which imply purpose in adaptation, not least because it suggests a deity's intentions, but others note that adaptation is necessarily purposeful.",2546680025263106835,related_concept,Natural selection,2024-06-23 21:17:09.728,['Adaptation'],"['Evolution', 'Machine learning', 'Evolutionary robotics', 'Genetic programming', 'Genetic algorithm', 'Partial differential equation', 'Ordinary differential equation', 'Adaptation', 'Natural selection']",set(),set(),0,2.3502890173410407,2.4168267795479684e-304
320,631,Heritability,http://dbpedia.org/resource/Heritability,http://en.wikipedia.org/wiki/Heritability,"Heritability is a statistic used in the fields of breeding and genetics that estimates the degree of variation in a phenotypic trait in a population that is due to genetic variation between individuals in that population. The concept of heritability can be expressed in the form of the following question: ""What is the proportion of the variation in a given trait within a population that is not explained by the environment or random chance?"" Other causes of measured variation in a trait are characterized as environmental factors, including observational error. In human studies of heritability these are often apportioned into factors from ""shared environment"" and ""non-shared environment"" based on whether they tend to result in persons brought up in the same household being more or less similar to persons who were not. Heritability is estimated by comparing individual phenotypic variation among related individuals in a population, by examining the association between individual phenotype and genotype data, or even by modeling summary-level data from genome-wide association studies (GWAS). Heritability is an important concept in quantitative genetics, particularly in selective breeding and behavior genetics (for instance, twin studies). It is the source of much confusion due to the fact that its technical definition is different from its commonly-understood folk definition. Therefore, its use conveys the incorrect impression that behavioral traits are ""inherited"" or specifically passed down through the genes. Behavioral geneticists also conduct heritability analyses based on the assumption that genes and environments contribute in a separate, additive manner to behavioral traits.",5230547156721342174,related_concept,Natural selection,2024-06-23 21:17:09.728,['Heritability'],"['Heritability', 'ANOVA']",set(),set(),0,2.523076923076923,0.9917617318532397
321,632,Natural selection,http://dbpedia.org/resource/Natural_selection,http://en.wikipedia.org/wiki/Natural_selection,"Natural selection is the differential survival and reproduction of individuals due to differences in phenotype. It is a key mechanism of evolution, the change in the heritable traits characteristic of a population over generations. Charles Darwin popularised the term ""natural selection"", contrasting it with artificial selection, which in his view is intentional, whereas natural selection is not. Variation exists within all populations of organisms. This occurs partly because random mutations arise in the genome of an individual organism, and their offspring can inherit such mutations. Throughout the lives of the individuals, their genomes interact with their environments to cause variations in traits. The environment of a genome includes the molecular biology in the cell, other cells, other individuals, populations, species, as well as the abiotic environment. Because individuals with certain variants of the trait tend to survive and reproduce more than individuals with other less successful variants, the population evolves. Other factors affecting reproductive success include sexual selection (now often included in natural selection) and fecundity selection. Natural selection acts on the phenotype, the characteristics of the organism which actually interact with the environment, but the genetic(heritable) basis of any phenotype that gives that phenotype a reproductive advantage may become more common in a population. Over time, this process can result in populations that specialise for particular ecological niches (microevolution) and may eventually result in speciation (the emergence of new species, macroevolution). In other words, natural selection is a key process in the evolution of a population. Natural selection is a cornerstone of modern biology. The concept, published by Darwin and Alfred Russel Wallace in a joint presentation of papers in 1858, was elaborated in Darwin's influential 1859 book On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. He described natural selection as analogous to artificial selection, a process by which animals and plants with traits considered desirable by human breeders are systematically favoured for reproduction. The concept of natural selection originally developed in the absence of a valid theory of heredity; at the time of Darwin's writing, science had yet to develop modern theories of genetics. The union of traditional Darwinian evolution with subsequent discoveries in classical genetics formed the modern synthesis of the mid-20th century. The addition of molecular genetics has led to evolutionary developmental biology, which explains evolution at the molecular level. While genotypes can slowly change by random genetic drift, natural selection remains the primary explanation for adaptive evolution.",8875078959742297030,main_concept,,2024-06-23 21:17:09.728,"['Natural selection', 'Mean']","['Natural selection', 'Mean', 'The Genetical Theory of Natural Selection', 'Disruptive selection', 'Stabilizing selection', 'Group selection', 'Ecological selection', 'Sexual selection']",{'Evolution'},set(),0,4.815568862275449,0.7343358839411526
322,633,Contextual Query Language,http://dbpedia.org/resource/Contextual_Query_Language,http://en.wikipedia.org/wiki/Contextual_Query_Language,"Contextual Query Language (CQL), previously known as Common Query Language, is a formal language for representing queries to information retrieval systems such as search engines, bibliographic catalogs and museum collection information. Based on the semantics of Z39.50, its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex query languages. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the Library of Congress.",3304192028261367047,related_concept,Query language,2024-06-23 21:17:09.728,['Contextual Query Language'],['Contextual Query Language'],set(),set(),0,1.0,3.7201484763238486e-304
323,634,SPARQL,http://dbpedia.org/resource/SPARQL,http://en.wikipedia.org/wiki/SPARQL,"SPARQL (pronounced ""sparkle"" /ˈspɑːkəl/, a recursive acronym for SPARQL Protocol and RDF Query Language) is an RDF query language—that is, a semantic query language for databases—able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. It was made a standard by the RDF Data Access Working Group (DAWG) of the World Wide Web Consortium, and is recognized as one of the key technologies of the semantic web. On 15 January 2008, SPARQL 1.0 was acknowledged by W3C as an official recommendation, and SPARQL 1.1 in March, 2013. SPARQL allows for a query to consist of triple patterns, conjunctions, disjunctions, and optional patterns. Implementations for multiple programming languages exist. There exist tools that allow one to connect and semi-automatically construct a SPARQL query for a SPARQL endpoint, for example ViziQuer.In addition, tools exist to translate SPARQL queries to other query languages, for example to SQL and to XQuery.",7914667345307889915,related_concept,Query language,2024-06-23 21:17:09.728,"['SPARQL', 'SQL', 'XQuery', 'Data']","['SPARQL', 'Data', 'SQL', 'XQuery']",{'SPARQL'},set(),0,4.4678362573099415,3.73818125929481e-304
324,635,RDQL,http://dbpedia.org/resource/RDQL,http://en.wikipedia.org/wiki/RDQL,,2367140252246590812,related_concept,Query language,2024-06-23 21:17:09.728,[],"['SPARQL', 'RDQL', 'SQL', 'Data', 'XQuery', 'Query language', 'Cypher Query Language']",set(),set(),0,0.20689655172413793,0.49582225033740607
325,636,Data manipulation language,http://dbpedia.org/resource/Data_manipulation_language,http://en.wikipedia.org/wiki/Data_manipulation_language,"A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. A DML is often a sublanguage of a broader database language such as SQL, with the DML comprising some of the operators in the language. Read-only selecting of data is sometimes distinguished as being part of a separate data query language (DQL), but it is closely related and sometimes also considered a component of a DML; some operators may perform both selecting (reading) and writing. A popular data manipulation language is that of Structured Query Language (SQL), which is used to retrieve and manipulate data in a relational database. Other forms of DML are those used by IMS/DLI, CODASYL databases, such as IDMS and others.",8089985537649733233,related_concept,Query language,2024-06-23 21:17:09.728,['SQL'],"['SQL', 'Data manipulation language', 'Data']",{'SQL'},set(),0,1.3673469387755102,0.8488809278695689
326,638,Datalog,http://dbpedia.org/resource/Datalog,http://en.wikipedia.org/wiki/Datalog,"Datalog is a declarative logic programming language. While it is syntactically a subset of Prolog, Datalog generally uses a bottom-up rather than top-down evaluation model. This difference yields significantly different behavior and properties from Prolog. It is often used as a query language for deductive databases. In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, cloud computing and machine learning. Its origins date back to the beginning of logic programming, but it became prominent as a separate area around 1977 when and Jack Minker organized a workshop on logic and databases. David Maier is credited with coining the term Datalog.",348801468000406613,related_concept,Query language,2024-06-23 21:17:09.728,"['Datalog', 'Data', 'Prolog']","['Datalog', 'Data', 'Prolog', 'SQL']",set(),set(),0,0.6380090497737556,1.3468849974060817
327,639,Jaql,http://dbpedia.org/resource/Jaql,http://en.wikipedia.org/wiki/Jaql,"Jaql (pronounced ""jackal"") is a functional data processing and query language most commonly used for JSON query processing on big data. It started as an open source project at Google but the latest release was on 2010-07-12. IBM took it over as primary data processing language for their Hadoop software package BigInsights. Although having been developed for JSON it supports a variety of other data sources like CSV, TSV, XML. A comparison to other BigData query languages like PIG Latin and Hive QL illustrates performance and usability aspects of these technologies. Jaql supports lazy evaluation, so expressions are only materialized when needed.",5156062523076493460,related_concept,Query language,2024-06-23 21:17:09.728,"['Jaql', 'Data']","['Jaql', 'Data', 'SQL']",set(),set(),0,0.22641509433962265,3.752279207963886e-304
328,640,XML database,http://dbpedia.org/resource/XML_database,http://en.wikipedia.org/wiki/XML_database,"An XML database is a data persistence software system that allows data to be specified, and sometimes stored, in XML format. This data can be queried, transformed, exported and returned to a calling system. XML databases are a flavor of document-oriented databases which are in turn a category of NoSQL database.",9051966732500964200,related_concept,Query language,2024-06-23 21:17:09.728,"['XML database', 'SQL']","['XML database', 'SQL', 'XQuery']",set(),set(),0,1.1818181818181819,1.1005070105948571
329,641,.QL,http://dbpedia.org/resource/.QL,http://en.wikipedia.org/wiki/.QL,".QL (pronounced ""dot-cue-el"") is an object-oriented query language used to retrieve data from relational database management systems. It is reminiscent of the standard query language SQL and the object-oriented programming language Java. .QL is an object-oriented variant of a logical query language called Datalog. Hierarchical data can therefore be naturally queried in .QL in a recursive manner. Queries written in .QL are optimised, compiled into SQL and can then be executed on any major relational database management system. .QL query language is being used in SemmleCode to query a relational representation of Java programs. .QL is developed at Semmle Limited and is based on the company's proprietary technology.",5727233554205418965,related_concept,Query language,2024-06-23 21:17:09.728,"['Datalog', '.QL', 'SQL', 'Data']","['Datalog', '.QL', 'SQL', 'Data']",set(),set(),0,0.711864406779661,3.739757292844102e-304
330,643,SQL,http://dbpedia.org/resource/SQL,http://en.wikipedia.org/wiki/SQL,"Structured Query Language, abbreviated as SQL,(/ˈsiːkwəl/ ""sequel"", /ˌɛsˌkjuːˈɛl/ S-Q-L; ) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e. data incorporating relations among entities and variables. SQL offers two main advantages over older read–write APIs such as ISAM or VSAM. Firstly, it introduced the concept of accessing many records with one single command. Secondly, it eliminates the need to specify how to reach a record, e.g. with or without an index. Originally based upon relational algebra and tuple relational calculus, SQL consists of many types of statements, which may be informally classed as sublanguages, commonly: a data query language (DQL), a data definition language (DDL), a data control language (DCL), and a data manipulation language (DML). The scope of SQL includes data query, data manipulation (insert, update, and delete), data definition (schema creation and modification), and data access control. Although SQL is essentially a declarative language (4GL), it also includes procedural elements. SQL was one of the first commercial languages to use Edgar F. Codd’s relational model. The model was described in his influential 1970 paper, ""A Relational Model of Data for Large Shared Data Banks"". Despite not entirely adhering to the relational model as described by Codd, it became the most widely used database language. SQL became a standard of the American National Standards Institute (ANSI) in 1986 and of the International Organization for Standardization (ISO) in 1987. Since then, the standard has been revised to include a larger set of features. Despite the existence of standards, most SQL code requires at least some changes before being ported to different database systems.",963312136758777900,related_concept,Query language,2024-06-23 21:17:09.728,"['SQL', 'Data']","['SQL', 'Data', 'Database', 'Information technology']",{'SQL'},set(),0,4.945474372955289,3.787721366812024e-304
331,644,Computer language,http://dbpedia.org/resource/Computer_language,http://en.wikipedia.org/wiki/Computer_language,"A computer language is a formal language used to communicate with a computer. Types of computer languages include: 
* Construction language – all forms of communication by which a human can specify an executable problem solution to a computer 
* Command language – a language used to control the tasks of the computer itself, such as starting programs 
* Configuration language – a language used to write configuration files 
* Programming language – a formal language designed to communicate instructions to a machine, particularly a computer 
* Query language – a language used to make queries in databases and information systems 
* Transformation language – designed to transform some input text in a certain formal language into a modified output text that meets some specific goal 
* Data exchange language – a language that is domain-independent and can be used for data from any kind of discipline; examples: JSON, XML 
* Markup language – a grammar for annotating a document in a way that is syntactically distinguishable from the text, such as HTML 
* Modeling language – an artificial language used to express information or knowledge, often for use in computer system design 
* Architecture description language – used as a language (or a conceptual model) to describe and represent system architectures 
* Hardware description language – used to model integrated circuits 
* Page description language – describes the appearance of a printed page in a higher level than an actual output bitmap 
* Simulation language – a language used to describe simulations 
* Specification language – a language used to describe what a system should do 
* Style sheet language – a computer language that expresses the presentation of structured documents, such as CSS [hard drive]Computer program 
* Data serialization 
* Domain-specific language – a language specialized to a particular application domain 
* General-purpose language – a language that is broadly applicable across application domains, and lacks specialized features for a particular domain 
* Lists of programming languages 
* Natural language processing – the use of computers to process text or speech in human language",5666228722199090178,related_concept,Query language,2024-06-23 21:17:09.728,"['Query language', 'Data', 'Natural language processing']",[],set(),set(),0,6.953488372093023,1.358617048614882
332,645,Object Query Language,http://dbpedia.org/resource/Object_Query_Language,http://en.wikipedia.org/wiki/Object_Query_Language,"Object Query Language (OQL) is a query language standard for object-oriented databases modeled after SQL and developed by the Object Data Management Group (ODMG). Because of its overall complexity the complete OQL standard has not yet been fully implemented in any software. The OQL standard influenced the design of later query languages such as and EJB QL, though none are considered to be any version of OQL.",381195585470371359,related_concept,Query language,2024-06-23 21:17:09.728,"['SQL', 'Object Query Language', 'Data']","['SQL', 'Object Query Language', 'Data']",set(),set(),0,1.28,3.752344948225787e-304
333,646,Controlled natural language,http://dbpedia.org/resource/Controlled_natural_language,http://en.wikipedia.org/wiki/Controlled_natural_language,"Controlled natural languages (CNLs) are subsets of natural languages that are obtained by restricting the grammar and vocabulary in order to reduce or eliminate ambiguity and complexity. Traditionally, controlled languages fall into two major types: those that improve readability for human readers (e.g. non-native speakers),and those that enable reliable automatic semantic analysis of the language. The first type of languages (often called ""simplified"" or ""technical"" languages), for example ASD Simplified Technical English, Caterpillar Technical English, IBM's Easy English, are used in the industry to increase the quality of technical documentation, and possibly simplify the semi-automatic translation of the documentation. These languages restrict the writer by general rules such as ""Keep sentences short"", ""Avoid the use of pronouns"", ""Only use dictionary-approved words"", and ""Use only the active voice"". The second type of languages have a formal syntax and semantics, and can be mapped to an existing formal language, such as first-order logic. Thus, those languages can be used as knowledge representation languages, and writing of those languages is supported by fully automatic consistency and redundancy checks, query answering, etc.",6001025762655837722,related_concept,Query language,2024-06-23 21:17:09.728,['Controlled natural language'],['Controlled natural language'],set(),set(),0,1.72,1.3911753048207844
334,647,JSONiq,http://dbpedia.org/resource/JSONiq,http://en.wikipedia.org/wiki/JSONiq,"JSONiq is a query and functional programming language that is designed to declaratively query and transform collections of hierarchical and heterogeneous data in format of JSON, XML, as well as unstructured, textual data. JSONiq is an open specification published under the Creative Commons Attribution-ShareAlike 3.0 license. It is based on the XQuery language, with which it shares the same core expressions and operations on atomic types. JSONiq comes in two syntactical flavors, which both support JSON and XML natively. 1. 
* The JSONiq syntax (a superset of JSON) extended with XML support through a compatible subset of XQuery. 2. 
* The XQuery syntax (native XML support) extended with JSON support through a compatible subset (the JSONiq extension to XQuery) of the above JSONiq syntax.",1879557373362973980,related_concept,Query language,2024-06-23 21:17:09.728,"['JSONiq', 'XQuery', 'Query language']","['JSONiq', 'XQuery', 'Query language', 'SQL', 'Data']",set(),set(),0,0.4666666666666667,3.730637563901425e-304
335,648,QUEL query languages,http://dbpedia.org/resource/QUEL_query_languages,http://en.wikipedia.org/wiki/QUEL_query_languages,"QUEL is a relational database query language, based on tuple relational calculus, with some similarities to SQL. It was created as a part of the Ingres DBMS effort at University of California, Berkeley, based on Codd's earlier suggested but not implemented Data Sub-Language ALPHA. QUEL was used for a short time in most products based on the freely available Ingres source code, most notably in an implementation called POSTQUEL supported by POSTGRES. As Oracle and DB2 gained market share in the early 1980s, most companies then supporting QUEL moved to SQL instead. QUEL continues to be available as a part of the Ingres DBMS, although no QUEL-specific language enhancements have been added for many years.",6291727721473950069,related_concept,Query language,2024-06-23 21:17:09.728,"['SQL', 'Data']","['SQL', 'Data']",set(),set(),0,1.0508474576271187,3.733719638477643e-304
336,649,Cypher Query Language,http://dbpedia.org/resource/Cypher_Query_Language,http://en.wikipedia.org/wiki/Cypher_Query_Language,,7996261435693080515,related_concept,Query language,2024-06-23 21:17:09.728,[],"['SQL', 'Data']",set(),set(),0,0.6153846153846154,0.4695223052268659
337,650,Deductive database,http://dbpedia.org/resource/Deductive_database,http://en.wikipedia.org/wiki/Deductive_database,"A deductive database is a database system that can make deductions (i.e. conclude additional facts) based on rules and facts stored in the (deductive) database. Datalog is the language typically used to specify facts, rules and queries in deductive databases. Deductive databases have grown out of the desire to combine logic programming with relational databases to construct systems that support a powerful formalism and are still fast and able to deal with very large datasets. Deductive databases are more expressive than relational databases but less expressive than logic programming systems. In recent years, deductive databases such as Datalog have found new application in data integration, information extraction, networking, program analysis, security, and cloud computing. Deductive databases reuse many concepts from logic programming; rules and facts specified in the deductive database language Datalog look very similar to those in Prolog. However important differences between deductive databases and logic programming: 
* Order sensitivity and procedurality: In Prolog, program execution depends on the order of rules in the program and on the order of parts of rules; these properties are used by programmers to build efficient programs. In database languages (like SQL or Datalog), however, program execution is independent of the order of rules and facts. 
* Special predicates: In Prolog, programmers can directly influence the procedural evaluation of the program with special predicates such as the cut, this has no correspondence in deductive databases. 
* Function symbols: Logic Programming languages allow function symbols to build up complex symbols. This is not allowed in deductive databases. 
* Tuple-oriented processing: Deductive databases use set-oriented processing while logic programming languages concentrate on one tuple at a time.",2536972339629893997,related_concept,Query language,2024-06-23 21:17:09.728,"['Datalog', 'SQL', 'Deductive database', 'Data', 'Prolog']","['Datalog', 'Deductive database', 'Data', 'Prolog']",set(),set(),0,1.5689655172413792,1.3322239797416013
338,651,XQuery,http://dbpedia.org/resource/XQuery,http://en.wikipedia.org/wiki/XQuery,"XQuery (XML Query) is a query and functional programming language that queries and transforms collections of structured and unstructured data, usually in the form of XML, text and with vendor-specific extensions for other data formats (JSON, binary, etc.). The language is developed by the XML Query working group of the W3C. The work is closely coordinated with the development of XSLT by the XSL Working Group; the two groups share responsibility for XPath, which is a subset of XQuery. XQuery 1.0 became a W3C Recommendation on January 23, 2007. XQuery 3.0 became a W3C Recommendation on April 8, 2014. XQuery 3.1 became a W3C Recommendation on March 21, 2017. ""The mission of the XML Query project is to provide flexible query facilities to extract data from real and virtual documents on the World Wide Web, therefore finally providing the needed interaction between the Web world and the database world. Ultimately, collections of XML files will be accessed like databases.""",6739735864906575282,related_concept,Query language,2024-06-23 21:17:09.728,['XQuery'],"['XQuery', 'Prolog', 'SQL', 'Data', 'JSONiq']",set(),set(),0,1.6527777777777777,0.7305641795701524
339,653,Query language,http://dbpedia.org/resource/Query_language,http://en.wikipedia.org/wiki/Query_language,"Query languages, data query languages or database query languages (DQL) are computer languages used to make queries in databases and information systems.A well known example is the Structured Query Language (SQL).",2755522501287647527,main_concept,,2024-06-23 21:17:09.728,"['SQL', 'Query language']",['SQL'],set(),set(),0,1.9725490196078432,3.735870026688718e-304
340,654,Low-noise amplifier,http://dbpedia.org/resource/Low-noise_amplifier,http://en.wikipedia.org/wiki/Low-noise_amplifier,"A low-noise amplifier (LNA) is an electronic amplifier that amplifies a very low-power signal without significantly degrading its signal-to-noise ratio. An amplifier will increase the power of both the signal and the noise present at its input, but the amplifier will also introduce some additional noise. LNAs are designed to minimize that additional noise. Designers can minimize additional noise by choosing low-noise components, operating points, and circuit topologies. Minimizing additional noise must balance with other design goals such as power gain and impedance matching. LNAs are found in radio communications systems, medical instruments and electronic test equipment. A typical LNA may supply a power gain of 100 (20 decibels (dB)) while decreasing the signal-to-noise ratio by less than a factor of two (a 3 dB noise figure (NF)). Although LNAs are primarily concerned with weak signals that are just above the noise floor, they must also consider the presence of larger signals that cause intermodulation distortion.",7229318384359699923,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],['Radio'],set(),set(),0,1.5522388059701493,3.863019879850324e-304
341,655,Noise power,http://dbpedia.org/resource/Noise_power,http://en.wikipedia.org/wiki/Noise_power,"In telecommunication, the term noise power has the following meanings: 1. 
* The measured total noise in a given bandwidth at the input or output of a device when the signal is not present; the integral of noise spectral density over the bandwidth 2. 
* The power generated by a random electromagnetic process. 3. 
* Interfering and unwanted power in an electrical device or system. 4. 
* In the acceptance testing of radio transmitters, the supplied to the antenna transmission line by a radio transmitter when loaded with noise having a Gaussian amplitude-vs.-frequency distribution.",338982616307642679,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],['Noise power'],set(),set(),0,5.095238095238095,0.48122343660447414
342,656,Shannon–Hartley theorem,http://dbpedia.org/resource/Shannon–Hartley_theorem,http://en.wikipedia.org/wiki/Shannon–Hartley_theorem,"In information theory, the Shannon–Hartley theorem tells the maximum rate at which information can be transmitted over a communications channel of a specified bandwidth in the presence of noise. It is an application of the noisy-channel coding theorem to the archetypal case of a continuous-time analog communications channel subject to Gaussian noise. The theorem establishes Shannon's channel capacity for such a communication link, a bound on the maximum amount of error-free information per time unit that can be transmitted with a specified bandwidth in the presence of the noise interference, assuming that the signal power is bounded, and that the Gaussian noise process is characterized by a known power or power spectral density. The law is named after Claude Shannon and Ralph Hartley.",7230384797020230849,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],['Shannon–Hartley theorem'],set(),set(),0,2.388059701492537,5.071096425979226e-304
343,657,Signal (electrical engineering),http://dbpedia.org/resource/Signal_(electrical_engineering),http://en.wikipedia.org/wiki/Signal_(electrical_engineering),,7587771735325972418,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],"['Digital signal', 'Dynamical system']",set(),set(),0,1.64,0.5218218094020205
344,658,Contrast-to-noise ratio,http://dbpedia.org/resource/Contrast-to-noise_ratio,http://en.wikipedia.org/wiki/Contrast-to-noise_ratio,"Contrast-to-noise ratio (CNR) is a measure used to determine image quality. CNR is similar to the metric signal-to-noise ratio (SNR), but subtracts a term before taking the ratio. This is important when there is a significant bias in an image, such as from haze. As can be seen in the picture at right, the intensity is rather high even though the features of the image are washed out by the haze. Thus this image may have a high SNR metric, but will have a low CNR metric. One way to define contrast-to-noise ratio is: where SA and SB are signal intensities for signal producing structures A and B in the region of interest and σo is the standard deviation of the pure image noise.",9026001180121669358,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['Contrast-to-noise ratio'],['Contrast-to-noise ratio'],set(),set(),0,1.0101010101010102,3.760036024346448e-304
345,660,Peak signal-to-noise ratio,http://dbpedia.org/resource/Peak_signal-to-noise_ratio,http://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio,"Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale. PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.",3250840777043814683,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['Peak signal-to-noise ratio'],['Peak signal-to-noise ratio'],set(),set(),0,1.149171270718232,3.749740745765835e-304
346,661,Signal-to-noise statistic,http://dbpedia.org/resource/Signal-to-noise_statistic,http://en.wikipedia.org/wiki/Signal-to-noise_statistic,"In mathematics the signal-to-noise statistic distance between two vectors a and b with mean values and and standard deviation and respectively is: In the case of Gaussian-distributed data and unbiased class distributions, this statistic can be related to classification accuracy given an ideal linear discrimination, and a decision boundary can be derived. This distance is frequently used to identify vectors that have significant difference. One usage is in bioinformatics to locate genes that are differential expressed on microarray experiments.",6462346071230503349,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5714285714285714,1.4280510945444551
347,662,Video quality,http://dbpedia.org/resource/Video_quality,http://en.wikipedia.org/wiki/Video_quality,"Video quality is a characteristic of a video passed through a video transmission or processing system that describes perceived video degradation (typically, compared to the original video). Video processing systems may introduce some amount of distortion or artifacts in the video signal that negatively impacts the user's perception of a system. For many stakeholders in video production and distribution, assurance of video quality is an important task. Video quality evaluation is performed to describe the quality of a set of video sequences under study. Video quality can be evaluated objectively (by mathematical models) or subjectively (by asking users for their rating). Also, the quality of a system can be determined offline (i.e., in a laboratory setting for developing new codecs or services), or in-service (to monitor and ensure a certain level of quality).",7001915789853331636,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['Video quality'],"['Video quality', 'Database', 'Data']",set(),set(),0,3.4029850746268657,0.9435478483811197
348,663,Signal averaging,http://dbpedia.org/resource/Signal_averaging,http://en.wikipedia.org/wiki/Signal_averaging,"Signal averaging is a signal processing technique applied in the time domain, intended to increase the strength of a signal relative to noise that is obscuring it. By averaging a set of replicate measurements, the signal-to-noise ratio (SNR) will be increased, ideally in proportion to the square root of the number of measurements.",6708825168698080607,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['Signal averaging'],"['Signal averaging', 'MATLAB']",set(),set(),0,0.1836734693877551,3.7845775764836996e-304
349,664,Decibel,http://dbpedia.org/resource/Decibel,http://en.wikipedia.org/wiki/Decibel,"The decibel (symbol: dB) is a relative unit of measurement equal to one tenth of a bel (B). It expresses the ratio of two values of a power or root-power quantity on a logarithmic scale. Two signals whose levels differ by one decibel have a power ratio of 101/10 (approximately 1.26) or root-power ratio of 101⁄20 (approximately 1.12). The unit expresses a relative change or an absolute value. In the latter case, the numeric value expresses the ratio of a value to a fixed reference value; when used in this way, the unit symbol is often suffixed with letter codes that indicate the reference value. For example, for the reference value of 1 volt, a common suffix is """" (e.g., ""20 dBV""). Two principal types of scaling of the decibel are in common use. When expressing a power ratio, it is defined as ten times the logarithm in base 10. That is, a change in power by a factor of 10 corresponds to a 10 dB change in level. When expressing root-power quantities, a change in amplitude by a factor of 10 corresponds to a 20 dB change in level. The decibel scales differ by a factor of two, so that the related power and root-power levels change by the same value in linear systems, where power is proportional to the square of amplitude. The definition of the decibel originated in the measurement of transmission loss and power in telephony of the early 20th century in the Bell System in the United States. The bel was named in honor of Alexander Graham Bell, but the bel is seldom used. Instead, the decibel is used for a wide variety of measurements in science and engineering, most prominently in acoustics, electronics, and control theory. In electronics, the gains of amplifiers, attenuation of signals, and signal-to-noise ratios are often expressed in decibels.",870110024651176785,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],[],set(),set(),0,5.212355212355212,1.1071350833970333
350,665,Distortion,http://dbpedia.org/resource/Distortion,http://en.wikipedia.org/wiki/Distortion,"In signal processing, distortion is the alteration of the original shape (or other characteristic) of a signal. In communications and electronics it means the alteration of the waveform of an information-bearing signal, such as an audio signal representing sound or a video signal representing images, in an electronic device or communication channel. Distortion is usually unwanted, and so engineers strive to eliminate or minimize it. In some situations, however, distortion may be desirable. For example, in noise reduction systems like the Dolby system, an audio signal is deliberately distorted in ways that emphasize aspects of the signal that are subject to electrical noise, then it is symmetrically ""undistorted"" after passing through a noisy communication channel, reducing the noise in the received signal. Distortion is also used as a musical effect, particularly with electric guitars. The addition of noise or other outside signals (hum, interference) is not considered distortion, though the effects of quantization distortion are sometimes included in noise. Quality measures that reflect both noise and distortion include the signal-to-noise and distortion (SINAD) ratio and total harmonic distortion plus noise (THD+N).",4994382933888522937,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,"['Distortion', 'SINAD']","['Distortion', 'SINAD']",set(),set(),0,2.2984126984126982,3.893636402992122e-304
351,668,Noise (electronic),http://dbpedia.org/resource/Noise_(electronic),http://en.wikipedia.org/wiki/Noise_(electronic),,6269571306653723241,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],['SINAD'],set(),set(),0,0.0707070707070707,0.555748689036082
352,669,Matched filter,http://dbpedia.org/resource/Matched_filter,http://en.wikipedia.org/wiki/Matched_filter,"In signal processing, a matched filter is obtained by correlating a known delayed signal, or template, with an unknown signal to detect the presence of the template in the unknown signal. This is equivalent to convolving the unknown signal with a conjugated time-reversed version of the template. The matched filter is the optimal linear filter for maximizing the signal-to-noise ratio (SNR) in the presence of additive stochastic noise. Matched filters are commonly used in radar, in which a known signal is sent out, and the reflected signal is examined for common elements of the out-going signal. Pulse compression is an example of matched filtering. It is so called because the impulse response is matched to input pulse signals. Two-dimensional matched filters are commonly used in image processing, e.g., to improve the SNR of X-ray observations.Matched filtering is a demodulation technique with LTI (linear time invariant) filters to maximize SNR.It was originally also known as a North filter.",6493138231757224194,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['Matched filter'],"['Matched filter', 'Signal-to-noise ratio', 'Neyman–Pearson lemma', 'Inference']",set(),set(),0,1.2533333333333334,1.26700460050291
353,670,Signal-to-interference-plus-noise ratio,http://dbpedia.org/resource/Signal-to-interference-plus-noise_ratio,http://en.wikipedia.org/wiki/Signal-to-interference-plus-noise_ratio,"In information theory and telecommunication engineering, the signal-to-interference-plus-noise ratio (SINR) (also known as the signal-to-noise-plus-interference ratio (SNIR)) is a quantity used to give theoretical upper bounds on channel capacity (or the rate of information transfer) in wireless communication systems such as networks. Analogous to the signal-to-noise ratio (SNR) used often in wired communications systems, the SINR is defined as the power of a certain signal of interest divided by the sum of the interference power (from all the other interfering signals) and the power of some background noise. If the power of noise term is zero, then the SINR reduces to the signal-to-interference ratio (SIR). Conversely, zero interference reduces the SINR to the SNR, which is used less often when developing mathematical models of wireless networks such as cellular networks. The complexity and randomness of certain types of wireless networks and signal propagation has motivated the use of stochastic geometry models in order to model the SINR, particularly for cellular or mobile phone networks.",5196584511950400126,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.9537037037037037,3.788294165918022e-304
354,671,Signal-to-noise ratio (imaging),http://dbpedia.org/resource/Signal-to-noise_ratio_(imaging),http://en.wikipedia.org/wiki/Signal-to-noise_ratio_(imaging),"Signal-to-noise ratio (SNR) is used in imaging to characterize image quality. The sensitivity of a (digital or film) imaging system is typically described in the terms of the signal level that yields a threshold level of SNR. Industry standards define sensitivity in terms of the ISO film speed equivalent, using SNR thresholds (at average scene luminance) of 40:1 for ""excellent"" image quality and 10:1 for ""acceptable"" image quality. SNR is sometimes quantified in decibels (dB) of signal power relative to noise power, though in the imaging field the concept of ""power"" is sometimes taken to be the power of a voltage signal proportional to optical power; so a 20 dB SNR may mean either 10:1 or 100:1 optical power, depending on which definition is in use.",4892530668924186571,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['Signal-to-noise ratio'],['Signal-to-noise ratio'],set(),set(),0,0.32299741602067183,3.758322441874911e-304
355,672,SINAD,http://dbpedia.org/resource/SINAD,http://en.wikipedia.org/wiki/SINAD,"Signal-to-noise and distortion ratio (SINAD) is a measure of the quality of a signal from a communications device, often defined as where is the average power of the signal, noise and distortion components. SINAD is usually expressed in dB and is quoted alongside the receiver RF sensitivity, to give a quantitative evaluation of the receiver sensitivity. Note that with this definition, unlike SNR, a SINAD reading can never be less than 1 (i.e. it is always positive when quoted in dB). When calculating the distortion, it is common to exclude the DC components. Due to widespread use, SINAD has collected several different definitions. SINAD is commonly defined as: 1. 
* The ratio of (a) total received power, i.e., the signal to (b) the noise-plus-distortion power. This is modeled by the equation above. 2. 
* The ratio of (a) the power of a test signal, i.e. a sine wave, to (b) the residual received power, i.e. noise-plus-distortion power. With this definition, it is possible to have a SINAD level less than one. This definition is used in the calculation of ENOB for DACs and ADCs. Information on the relations between SINAD, ENOB, SNR, THD and SFDR can be found in.",7304644153001862224,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,['SINAD'],"['SINAD', 'Radio', 'Radio receiver']",set(),set(),0,1.027027027027027,0.8787541391322887
356,673,Noise (signal processing),http://dbpedia.org/resource/Noise_(signal_processing),http://en.wikipedia.org/wiki/Noise_(signal_processing),"In signal processing, noise is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion. Sometimes the word is also used to mean signals that are random (unpredictable) and carry no useful information; even if they are not interfering with other signals or may have been introduced intentionally, as in comfort noise. Noise reduction, the recovery of the original signal from the noise-corrupted one, is a very common goal in the design of signal processing systems, especially filters. The mathematical limits for noise removal are set by information theory.",4039814273073892181,related_concept,Signal-to-noise ratio,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.0091743119266054,3.964564026202474e-304
357,674,Signal-to-noise ratio,http://dbpedia.org/resource/Signal-to-noise_ratio,http://en.wikipedia.org/wiki/Signal-to-noise_ratio,"Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise. SNR, bandwidth, and channel capacity of a communication channel are connected by the Shannon–Hartley theorem.",4324584452595848612,main_concept,,2024-06-23 21:17:09.728,['Signal-to-noise ratio'],"['Signal-to-noise ratio', 'Shannon–Hartley theorem', 'Signal-to-noise ratio (imaging)']",set(),set(),0,6.149484536082475,3.757549169113175e-304
358,675,Multivariate analysis of covariance,http://dbpedia.org/resource/Multivariate_analysis_of_covariance,http://en.wikipedia.org/wiki/Multivariate_analysis_of_covariance,"Multivariate analysis of covariance (MANCOVA) is an extension of analysis of covariance (ANCOVA) methods to cover cases where there is more than one dependent variable and where the control of concomitant continuous independent variables – covariates – is required. The most prominent benefit of the MANCOVA design over the simple MANOVA is the 'factoring out' of noise or error that has been introduced by the covariant. A commonly used multivariate version of the ANOVA F-statistic is Wilks' Lambda (Λ), which represents the ratio between the error variance (or covariance) and the effect variance (or covariance).",1469281731096098785,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,"['Multivariate analysis of covariance', 'ANOVA', 'ANCOVA']","['Multivariate analysis of covariance', 'ANOVA', 'ANCOVA']",set(),set(),0,0.48,5.921218168701517e-304
359,677,Wishart distribution,http://dbpedia.org/resource/Wishart_distribution,http://en.wikipedia.org/wiki/Wishart_distribution,"In statistics, the Wishart distribution is a generalization to multiple dimensions of the gamma distribution. It is named in honor of John Wishart, who first formulated the distribution in 1928. It is a family of probability distributions defined over symmetric, nonnegative-definite random matrices (i.e. matrix-valued random variables). In random matrix theory, the space of Wishart matrices is called the Wishart ensemble. These distributions are of great importance in the estimation of covariance matrices in multivariate statistics. In Bayesian statistics, the Wishart distribution is the conjugate prior of the inverse covariance-matrix of a multivariate-normal random-vector.",1330487470836235726,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,"['Wishart distribution', 'Bayesian statistics']","['Wishart distribution', 'Bayesian statistics', 'Fisher information', 'Kullback–Leibler divergence']",set(),set(),0,1.2234042553191489,5.846737336770719e-304
360,679,Bivariate analysis,http://dbpedia.org/resource/Bivariate_analysis,http://en.wikipedia.org/wiki/Bivariate_analysis,"Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association. Bivariate analysis can help determine to what extent it becomes easier to know and predict a value for one variable (possibly a dependent variable) if we know the value of the other variable (possibly the independent variable) (see also correlation and simple linear regression). Bivariate analysis can be contrasted with univariate analysis in which only one variable is analysed. Like univariate analysis, bivariate analysis can be descriptive or inferential. It is the analysis of the relationship between the two variables. Bivariate analysis is a simple (two variable) special case of multivariate analysis (where multiple relations between multiple variables are examined simultaneously).",139138976581609252,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,['Bivariate analysis'],"['Bivariate analysis', 'Simple linear regression', 'Pearson correlation coefficient', 'Covariance']",set(),set(),0,0.12130177514792899,1.0223521991936042
361,681,Bayesian multivariate linear regression,http://dbpedia.org/resource/Bayesian_multivariate_linear_regression,http://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression,"In statistics, Bayesian multivariate linear regression is aBayesian approach to multivariate linear regression, i.e. linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator.",925979043533734404,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,['Bayesian multivariate linear regression'],"['Bayesian multivariate linear regression', 'Calculus', 'Bayesian linear regression', 'Wishart distribution']",set(),set(),0,1.488888888888889,1.2541705535736523
362,682,Univariate analysis,http://dbpedia.org/resource/Univariate_analysis,http://en.wikipedia.org/wiki/Univariate_analysis,"Univariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate.",2229079803462602749,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,['Univariate analysis'],"['Univariate analysis', 'Data', 'Central tendency', 'Descriptive statistics', 'Histogram']",set(),set(),0,0.4027777777777778,5.983888437287854e-304
363,684,Estimation of covariance matrices,http://dbpedia.org/resource/Estimation_of_covariance_matrices,http://en.wikipedia.org/wiki/Estimation_of_covariance_matrices,"In statistics, sometimes the covariance matrix of a multivariate random variable is not known but has to be estimated. Estimation of covariance matrices then deals with the question of how to approximate the actual covariance matrix on the basis of a sample from the multivariate distribution. Simple cases, where observations are complete, can be dealt with by using the sample covariance matrix. The sample covariance matrix (SCM) is an unbiased and efficient estimator of the covariance matrix if the space of covariance matrices is viewed as an extrinsic convex cone in Rp×p; however, measured using the intrinsic geometry of positive-definite matrices, the SCM is a biased and inefficient estimator. In addition, if the random variable has a normal distribution, the sample covariance matrix has a Wishart distribution and a slightly differently scaled version of it is the maximum likelihood estimate. Cases involving missing data, heteroscedasticity, or autocorrelated residuals require deeper considerations. Another issue is the robustness to outliers, to which sample covariance matrices are highly sensitive. Statistical analyses of multivariate data often involve exploratory studies of the way in which the variables change in relation to one another and this may be followed up by explicit statistical models involving the covariance matrix of the variables. Thus the estimation of covariance matrices directly from observational data plays two roles: 
* to provide initial estimates that can be used to study the inter-relationships; 
* to provide sample estimates that can be used for model checking. Estimates of covariance matrices are required at the initial stages of principal component analysis and factor analysis, and are also involved in versions of regression analysis that treat the dependent variables in a data-set, jointly with the independent variable as the outcome of a random sample.",4007593110926844855,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,"['Wishart distribution', 'Estimation of covariance matrices']","['Wishart distribution', 'Estimation of covariance matrices']",set(),set(),0,0.1588089330024814,5.813602378031191e-304
364,685,Factor analysis,http://dbpedia.org/resource/Factor_analysis,http://en.wikipedia.org/wiki/Factor_analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus ""error"" terms, hence factor analysis can be thought of as a special case of errors-in-variables models. Simply put, the factor loading of a variable quantifies the extent to which the variable is related to a given factor. A common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. It may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.",2364095706455697971,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,['Factor analysis'],"['Factor analysis', 'Principal component analysis', 'SPSS', 'Variance', ""Bayes' theorem"", 'DNA microarray']",set(),set(),0,2.8480176211453743,1.3625072682718227
365,692,Multivariate analysis of variance,http://dbpedia.org/resource/Multivariate_analysis_of_variance,http://en.wikipedia.org/wiki/Multivariate_analysis_of_variance,"In statistics, multivariate analysis of variance (MANOVA) is a procedure for comparing multivariate sample means. As a multivariate procedure, it is used when there are two or more dependent variables, and is often followed by significance tests involving individual dependent variables separately. Without relation to the image, the dependent variables may be k life satisfactions scores measured at sequential time points and p job satisfaction scores measured at sequential time points. In this case there are k+p dependent variables whose linear combination follows a multivariate normal distribution, multivariate variance-covariance matrix homogeneity, and linear relationship, no multicollinearity, and each without outliers.",2463028871372789941,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,['ANOVA'],"['ANOVA', 'Wishart distribution']",set(),set(),0,1.518421052631579,5.843243108703638e-304
366,694,Multivariate Student distribution,http://dbpedia.org/resource/Multivariate_Student_distribution,http://en.wikipedia.org/wiki/Multivariate_Student_distribution,,2914671352983449958,related_concept,Multivariate statistics,2024-06-23 21:17:09.728,[],"[""Student's t-distribution"", 'Cauchy distribution', 'PDF']",set(),set(),0,0.06557377049180328,0.6283136240548549
367,697,Whistled language,http://dbpedia.org/resource/Whistled_language,http://en.wikipedia.org/wiki/Whistled_language,"Whistled languages use whistling to emulate speech and facilitate communication. A whistled language is a system of whistled communication which allows fluent whistlers to transmit and comprehend a potentially unlimited number of messages over long distances. Whistled languages are different in this respect from the restricted codes sometimes used by herders or animal trainers to transmit simple messages or instructions. Generally, whistled languages emulate the tones or vowel formants of a natural spoken language, as well as aspects of its intonation and prosody, so that trained listeners who speak that language can understand the encoded message. Whistled language is rare compared to spoken language, but it is found in cultures around the world. It is especially common in tone languages where the whistled tones transmit the tones of the syllables (tone melodies of the words). This might be because in tone languages the tone melody carries more of the functional load of communication while non-tonal phonology carries proportionally less. The genesis of a whistled language has never been recorded in either case and has not yet received much productive study.",8033609627173698588,related_concept,Confusion matrix,2024-06-23 21:17:09.728,['Whistled language'],['Whistled language'],set(),set(),0,5.269565217391304,1.1591579483103838
368,712,Local search (Internet),http://dbpedia.org/resource/Local_search_(Internet),http://en.wikipedia.org/wiki/Local_search_(Internet),"Local search is the use of specialized Internet search engines that allow users to submit geographically constrained searches against a structured database of local business listings. Typical local search queries include not only information about ""what"" the site visitor is searching for (such as keywords, a business category, or the name of a consumer product) but also ""where"" information, such as a street address, city name, postal code, or geographic coordinates like latitude and longitude. Examples of local searches include ""Hong Kong hotels"", ""Manhattan restaurants"", and ""Dublin car rental"". Local searches exhibit explicit or implicit local intent. A search that includes a location modifier, such as ""Bellevue, WA"" or ""14th arrondissement"", is an explicit local search. A search that references a product or service that is typically consumed locally, such as ""restaurant"" or ""nail salon"", is an implicit local search. Local searches on Google Search typically return organic results prefaced with a 'local 3-pack', a list of three local results. More local results can be obtained by clicking on “more places” under the 3-pack. The list of results one obtains is also called the Local Finder. Search engines and directories are primarily supported by advertising from businesses that wish to be prominently featured when users search for specific products and services in specific locations. Google for instance, has developed local inventory ads and features ads in the local pack. Local search advertising can be highly effective because it allows ads to be targeted very precisely to the search terms and location provided by the user.",222210513420156392,related_concept,Search engine,2024-06-23 21:17:09.728,"['Google Search', 'Search engine']","['Google Search', 'Search engine']",set(),set(),0,1.7590361445783131,0.6682267870098035
369,713,YaCy,http://dbpedia.org/resource/YaCy,http://en.wikipedia.org/wiki/YaCy,"YaCy (pronounced “ya see”) is a free distributed search engine, built on the principles of peer-to-peer (P2P) networks created by Michael Christen in 2003. The engine is written in Java and distributed on several hundred computers, as of September 2006, so-called YaCy-peers. Each YaCy-peer independently crawls through the Internet, analyzes and indexes found web pages, and stores indexing results in a common database (so-called index) which is shared with other YaCy-peers using principles of peer-to-peer. It is a search engine that everyone can use to build a search portal for their intranet and to help search the public internet clearly. Compared to semi-distributed search engines, the YaCy-network has a distributed architecture. All YaCy-peers are equal and no central server exists. It can be run either in a crawling mode or as a local proxy server, indexing web pages visited by the person running YaCy on their computer. Several mechanisms are provided to protect the user's privacy. Access to the search functions is made by a locally run web server which provides a search box to enter search terms, and returns search results in a similar format to other popular search engines.",1002002620346474103,related_concept,Search engine,2024-06-23 21:17:09.728,['YaCy'],['YaCy'],set(),set(),0,0.384,3.221006191206731e-304
370,715,Search engine indexing,http://dbpedia.org/resource/Search_engine_indexing,http://en.wikipedia.org/wiki/Search_engine_indexing,"Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science. An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing. Popular engines focus on the full-text indexing of online, natural language documents. Media types such as pictures, video, audio, and graphics are also searchable. Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.",4250816129764313534,related_concept,Search engine,2024-06-23 21:17:09.728,"['Search engine indexing', 'Search engine']","['Search engine indexing', 'Search engine', 'Natural language processing', 'PDF']",set(),set(),0,1.0333333333333334,3.279087736110582e-304
371,717,Web query,http://dbpedia.org/resource/Web_query,http://en.wikipedia.org/wiki/Web_query,"A web query or web search query is a query that a user enters into a web search engine to satisfy their information needs. Web search queries are distinctive in that they are often plain text and boolean search directives are rarely used. They vary greatly from standard query languages, which are governed by strict syntax rules as command languages with keyword or positional parameters.",122760089098094976,related_concept,Search engine,2024-06-23 21:17:09.728,[],['Search engine'],set(),set(),0,0.8860759493670886,0.6341124492377962
372,718,Search engine results page,http://dbpedia.org/resource/Search_engine_results_page,http://en.wikipedia.org/wiki/Search_engine_results_page,"Search Engine Results Pages (SERP) are the pages displayed by search engines in response to a query by a user. The main component of the SERP is the listing of results that are returned by the search engine in response to a keyword query. The page that a search engine returns after a user submits a search query. In addition to organic search results, search engine results pages (SERPs) usually include paid search and pay-per-click (PPC) ads. The results are of two general types : 
* organic search: retrieved by the search engine's algorithm 
* sponsored search: advertisements. The results are normally ranked by relevance to the query. Each result displayed on the SERP normally includes a title, a link that points to the actual page on the Web, and a short description showing where the keywords have matched content within the page for organic results. For sponsored results, the advertiser chooses what to display. Due to the huge number of items that are available or related to the query, there are usually several pages in response to a single search query as the search engine or the user's preferences restrict viewing to a subset of results per page. Each succeeding page will tend to have lower ranking or lower relevancy results. Just like the world of traditional print media and its advertising, this enables competitive pricing for page real estate, but is complicated by the dynamics of consumer expectations and intent— unlike static print media where the content and the advertising on every page is the same all of the time for all viewers, despite such hard copy being localized to some degree, usually geographic, like state, metro-area, city, or neighbourhood, search engine results can vary based on individual factors such as browsing habits.",8353394656189767605,related_concept,Search engine,2024-06-23 21:17:09.728,[],['Search engine'],set(),set(),0,5.666666666666667,3.1621623807987276e-304
373,719,Yebol,http://dbpedia.org/resource/Yebol,http://en.wikipedia.org/wiki/Yebol,"Yebol was a vertical ""decision"" search engine that had developed a knowledge-based, semantic search platform. Based in San Jose, California, Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and contents that it presents in a visually indexed format that is more aligned with initial human intent. Yebol used association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol presented as one of its goals the creation of a unique ""homepage look"" for every possible search term.",2836393176208187840,related_concept,Search engine,2024-06-23 21:17:09.728,['Yebol'],['Yebol'],set(),set(),0,1.1020408163265305,3.2129400269823646e-304
374,720,Comparison of web search engines,http://dbpedia.org/resource/Comparison_of_web_search_engines,http://en.wikipedia.org/wiki/Comparison_of_web_search_engines,"Web search engines are listed in tables below for comparison purposes. The first table lists the company behind the engine, volume and ad support and identifies the nature of the software being used as free software or proprietary software. The second and third table lists internet privacy aspects along with other technical parameters, such as whether the engine provides personalization (alternatively viewed as a filter bubble). Defunct or acquired search engines are not listed here.",7147782408101154378,related_concept,Search engine,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.3388429752066116,3.189614901517692e-304
375,721,Inverted index,http://dbpedia.org/resource/Inverted_index,http://en.wikipedia.org/wiki/Inverted_index,"In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines. Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204. There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created.",6934124613416937822,related_concept,Search engine,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.1818181818181817,1.1237528459268358
376,722,Keyword (Internet search),http://dbpedia.org/resource/Keyword_(Internet_search),http://en.wikipedia.org/wiki/Keyword_(Internet_search),,5463418581790185155,related_concept,Search engine,2024-06-23 21:17:09.728,[],"['Search engine', 'PageRank', 'Google Search', 'IP address']",set(),set(),0,0.13513513513513514,0.5250739390612016
377,723,Google Search,http://dbpedia.org/resource/Google_Search,http://en.wikipedia.org/wiki/Google_Search,"Google Search (also known simply as Google) is a search engine provided by Google. Handling more than 3.5 billion searches per day, it has a 92% share of the global search engine market. It is also the most-visited website in the world. The order of search results returned by Google is based, in part, on a priority rank system called ""PageRank"". Google Search also provides many different options for customized searches, using symbols to include, exclude, specify or require certain search behavior, and offers specialized interactive experiences, such as flight status and package tracking, weather forecasts, currency, unit, and time conversions, word definitions, and more. The main purpose of Google Search is to search for text in publicly accessible documents offered by web servers, as opposed to other data, such as images or data contained in databases. It was originally developed in 1996 by Larry Page, Sergey Brin, and Scott Hassan. In 2011, Google introduced ""Google Voice Search"" to search for spoken, rather than typed, words. In 2012, Google introduced a Knowledge Graph semantic search feature. Analysis of the frequency of search terms may indicate economic, social and health trends. Data about the frequency of use of search terms on Google can be openly inquired via Google Trends and have been shown to correlate with flu outbreaks and unemployment levels, and provide the information faster than traditional reporting methods and surveys. As of mid-2016, Google's search engine has begun to rely on deep neural networks.",4271904174556549878,related_concept,Search engine,2024-06-23 21:17:09.728,"['Google Search', 'PageRank', 'Data']","['Google Search', 'PageRank', 'Data', 'PDF', 'Search algorithm', 'Search engine', 'AI']",set(),set(),0,6.5424940428911835,0.5929873186774268
378,724,Link analysis,http://dbpedia.org/resource/Link_analysis,http://en.wikipedia.org/wiki/Link_analysis,"In network theory, link analysis is a data-analysis technique used to evaluate relationships (connections) between nodes. Relationships may be identified among various types of nodes (objects), including organizations, people and transactions. Link analysis has been used for investigation of criminal activity (fraud detection, counterterrorism, and intelligence), computer security analysis, search engine optimization, market research, medical research, and art.",546562795152632168,related_concept,Search engine,2024-06-23 21:17:09.728,['Link analysis'],"['Link analysis', ""Dijkstra's algorithm"", 'Data', 'Data analysis', 'AI', 'Unsupervised learning', 'Supervised learning', 'Heuristic']",set(),set(),0,2.283582089552239,1.2978966712855828
379,726,Content-based image retrieval,http://dbpedia.org/resource/Content-based_image_retrieval,http://en.wikipedia.org/wiki/Content-based_image_retrieval,"Content-based image retrieval, also known as query by image content and content-based visual information retrieval (CBVIR), is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey for a scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing). ""Content-based"" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term ""content"" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself. CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness. Having humans manually annotate images by entering keywords or metadata in a large database can be time-consuming and may not capture the keywords desired to describe the image. The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined. In the same regard, CBIR systems have similar challenges in defining success. ""Keywords also limit the scope of queries to the set of predetermined criteria."" and, ""having been set up"" are less reliable than using the content itself.",8206391962777079344,related_concept,Search engine,2024-06-23 21:17:09.728,['Content-based image retrieval'],"['Content-based image retrieval', 'PageRank', 'Machine learning']",set(),set(),0,1.1357142857142857,0.8004855150763115
380,727,Yandex Search,http://dbpedia.org/resource/Yandex_Search,http://en.wikipedia.org/wiki/Yandex_Search,"Yandex Search (Russian: Яндекс) is a search engine. It is owned by Yandex, based in Russia. In January 2015, Yandex Search generated 51.2% of all of the search traffic in Russia according to .",5361203253234177723,related_concept,Search engine,2024-06-23 21:17:09.728,['Yandex Search'],"['Yandex Search', 'PageRank']",set(),set(),0,0.8473684210526315,0.5187955975223889
381,728,Computer science,http://dbpedia.org/resource/Computer_science,http://en.wikipedia.org/wiki/Computer_science,"Computer science is the study of computation, automation, and information. Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software). Computer science is generally considered an area of academic research and distinct from computer programming. Algorithms and data structures are central to computer science.The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data. The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.",8051819885706764196,related_concept,Search engine,2024-06-23 21:17:09.728,"['Computer science', 'Algorithm']","['Computer science', 'Algorithm', 'Datalog', 'Data', 'Data structure', 'AI', 'Data mining']",set(),set(),0,34.751736111111114,1.4116186374046786
382,729,PageRank,http://dbpedia.org/resource/PageRank,http://en.wikipedia.org/wiki/PageRank,"PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results. It is named after both the term ""web page"" and co-founder Larry Page. PageRank is a way of measuring the importance of website pages. According to Google: PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. Currently, PageRank is not the only algorithm used by Google to order search results, but it is the first algorithm that was used by the company, and it is the best known. As of September 24, 2019, PageRank and all associated patents are expired.",2555768622559106938,related_concept,Search engine,2024-06-23 21:17:09.728,"['Google Search', 'PageRank']","['Google Search', 'PageRank', 'Data', 'Search engine']",set(),set(),0,10.957446808510639,0.9121718052099964
383,732,Search engine,http://dbpedia.org/resource/Search_engine,http://en.wikipedia.org/wiki/Search_engine,"A search engine is a software system designed to carry out web searches. They search the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). When a user enters a query into a search engine, the engine scans its index of web pages to find those that are relevant to the user's query. The results are then ranked by relevancy and displayed to the user. The information may be a mix of links to web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories and social bookmarking sites, which are maintained by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Any internet-based content that can't be indexed and searched by a web search engine falls under the category of deep web.",6076006922789351538,main_concept,,2024-06-23 21:17:09.728,[],"['Google Search', 'Link analysis', 'PageRank', 'Search engine']",set(),set(),0,5.898305084745763,3.1811647766469442e-304
384,733,Matthews correlation coefficient,http://dbpedia.org/resource/Matthews_correlation_coefficient,http://en.wikipedia.org/wiki/Matthews_correlation_coefficient,,5187402008111893086,related_concept,F-score,2024-06-23 21:17:09.728,[],"['Matthews correlation coefficient', 'Pearson correlation coefficient', 'Informedness', 'Markedness', 'Data']",set(),set(),0,0.32142857142857145,0.4933241442364289
385,737,Harmonic mean,http://dbpedia.org/resource/Harmonic_mean,http://en.wikipedia.org/wiki/Harmonic_mean,"In mathematics, the harmonic mean is one of several kinds of average, and in particular, one of the Pythagorean means. It is sometimes appropriate for situations when the average rate is desired. The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is",8806355246708092297,related_concept,F-score,2024-06-23 21:17:09.728,[],['F-score'],set(),set(),0,1.7608695652173914,0.7178402649805705
386,738,METEOR,http://dbpedia.org/resource/METEOR,http://en.wikipedia.org/wiki/METEOR,"METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric for the evaluation of machine translation output. The metric is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as stemming and synonymy matching, along with the standard exact word matching. The metric was designed to fix some of the problems found in the more popular BLEU metric, and also produce good correlation with human judgement at the sentence or segment level. This differs from the BLEU metric in that BLEU seeks correlation at the corpus level. Results have been presented which give correlation of up to 0.964 with human judgement at the corpus level, compared to BLEU's achievement of 0.817 on the same data set. At the sentence level, the maximum correlation with human judgement achieved was 0.403.",6629205994366333324,related_concept,F-score,2024-06-23 21:17:09.728,['METEOR'],"['METEOR', 'Precision and recall']",set(),set(),0,0.8181818181818182,1.4032008579819195
387,739,P4-metric,http://dbpedia.org/resource/P4-metric,http://en.wikipedia.org/wiki/P4-metric,"P4 metric enables performance evaluation of the binary classifier.It is calculated from precision, recall, specificity and NPV (negative predictive value).P4 is designed in similar way to F1 metric, however addressing the criticisms leveled against F1. It may be perceived as its extension. Like the other known metrics, P4 is a function of: TP (true positives), TN (true negatives), FP (false positives), FN (false negatives).",2394842295312110672,related_concept,F-score,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.4489795918367347,1.0658218275576112
388,742,Word error rate,http://dbpedia.org/resource/Word_error_rate,http://en.wikipedia.org/wiki/Word_error_rate,"Word error rate (WER) is a common metric of the performance of a speech recognition or machine translation system. The general difficulty of measuring performance lies in the fact that the recognized word sequence can have a different length from the reference word sequence (supposedly the correct one). The WER is derived from the Levenshtein distance, working at the word level instead of the phoneme level. The WER is a valuable tool for comparing different systems as well as for evaluating improvements within one system. This kind of measurement, however, provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort. This problem is solved by first aligning the recognized word sequence with the reference (spoken) word sequence using dynamic string alignment. Examination of this issue is seen through a theory called the power law that states the correlation between perplexity and word error rate. Word error rate can then be computed as: where 
* S is the number of substitutions, 
* D is the number of deletions, 
* I is the number of insertions, 
* C is the number of correct words, 
* N is the number of words in the reference (N=S+D+C) The intuition behind 'deletion' and 'insertion' is how to get from the reference to the hypothesis. So if we have the reference ""This is wikipedia"" and hypothesis ""This _ wikipedia"", we call it a deletion. When reporting the performance of a speech recognition system, sometimes word accuracy (WAcc) is used instead: Note that since N is the number of words in the reference, the word error rate can be larger than 1.0, and thus, the word accuracy can be smaller than 0.0.",650295899737173648,related_concept,F-score,2024-06-23 21:17:09.728,['Word error rate'],['Word error rate'],set(),set(),0,1.5,1.4115310551905023
389,745,Fowlkes–Mallows index,http://dbpedia.org/resource/Fowlkes–Mallows_index,http://en.wikipedia.org/wiki/Fowlkes–Mallows_index,"The Fowlkes–Mallows index is an external evaluation method that is used to determine the similarity between two clusterings (clusters obtained after a clustering algorithm), and also a metric to measure confusion matrices. This measure of similarity could be either between two hierarchical clusterings or a clustering and a benchmark classification. A higher value for the Fowlkes–Mallows index indicates a greater similarity between the clusters and the benchmark classifications. It was invented by Bell Labs statisticians Edward Fowlkes and Collin Mallows in 1983.",5614064139495152904,related_concept,F-score,2024-06-23 21:17:09.728,[],['Fowlkes–Mallows index'],set(),set(),0,2.347826086956522,0.891139764080428
390,748,LEPOR,http://dbpedia.org/resource/LEPOR,http://en.wikipedia.org/wiki/LEPOR,"LEPOR (Length Penalty, Precision, n-gram Position difference Penalty and Recall) is an automatic language independent machine translation evaluation metric with tunable parameters and reinforced factors.",5243299224009808708,related_concept,F-score,2024-06-23 21:17:09.728,['LEPOR'],"['LEPOR', 'METEOR', 'Pearson correlation coefficient']",set(),set(),0,0.4166666666666667,1.4032970666457243
391,755,ANOVA,http://dbpedia.org/resource/ANOVA,http://en.wikipedia.org/wiki/ANOVA,,6271955953626275362,related_concept,Design matrix,2024-06-23 21:17:09.728,[],"['ANOVA', 'Statistical Methods for Research Workers', 'Randomization', 'F-test']",set(),set(),0,0.16897081413210446,0.515556394242101
392,757,Scatter matrix,http://dbpedia.org/resource/Scatter_matrix,http://en.wikipedia.org/wiki/Scatter_matrix,"In multivariate statistics and probability theory, the scatter matrix is a statistic that is used to make estimates of the covariance matrix, for instance of the multivariate normal distribution.",5713818231871047469,related_concept,Design matrix,2024-06-23 21:17:09.728,[],['Wishart distribution'],set(),set(),0,1.173913043478261,5.793269319605271e-304
393,758,Projection matrix,http://dbpedia.org/resource/Projection_matrix,http://en.wikipedia.org/wiki/Projection_matrix,"In statistics, the projection matrix , sometimes also called the influence matrix or hat matrix , maps the vector of response values (dependent variable values) to the vector of fitted values (or predicted values). It describes the influence each response value has on each fitted value. The diagonal elements of the projection matrix are the leverages, which describe the influence each response value has on the fitted value for that same observation.",4130984275189571260,related_concept,Design matrix,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.8805970149253731,1.3010643409399183
394,763,General linear model,http://dbpedia.org/resource/General_linear_model,http://en.wikipedia.org/wiki/General_linear_model,"The general linear model or general multivariate regression model is a compact way of simultaneously writing several multiple linear regression models. In that sense it is not a separate statistical linear model. The various multiple linear regression models may be compactly written as where Y is a matrix with series of multivariate measurements (each column being a set of measurements on one of the dependent variables), X is a matrix of observations on independent variables that might be a design matrix (each column being a set of observations on one of the independent variables), B is a matrix containing parameters that are usually to be estimated and U is a matrix containing errors (noise).The errors are usually assumed to be uncorrelated across measurements, and follow a multivariate normal distribution. If the errors do not follow a multivariate normal distribution, generalized linear models may be used to relax assumptions about Y and U. The general linear model incorporates a number of different statistical models: ANOVA, ANCOVA, MANOVA, MANCOVA, ordinary linear regression, t-test and F-test. The general linear model is a generalization of multiple linear regression to the case of more than one dependent variable. If Y, B, and U were column vectors, the matrix equation above would represent multiple linear regression. Hypothesis tests with the general linear model can be made in two ways: multivariate or as several independent univariate tests. In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are tested independently, i.e., as multiple univariate tests with the same design matrix.",2938333978045464106,related_concept,Design matrix,2024-06-23 21:17:09.728,"['ANOVA', 'ANCOVA', 'F-test', 'Hypothesis']","['ANOVA', 'ANCOVA', 'F-test', 'Hypothesis', 'Poisson regression']",set(),set(),0,1.7902813299232736,1.2716106503413598
395,764,Column vector,http://dbpedia.org/resource/Column_vector,http://en.wikipedia.org/wiki/Column_vector,,7189479184721398597,related_concept,Design matrix,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.130952380952381,0.5061251671906115
396,765,ANCOVA,http://dbpedia.org/resource/ANCOVA,http://en.wikipedia.org/wiki/ANCOVA,,4375443907146152724,related_concept,Design matrix,2024-06-23 21:17:09.728,[],"['ANOVA', 'ANCOVA', 'F-test']",set(),set(),0,0.056790123456790124,0.4538963658710938
397,766,Jacobian matrix and determinant,http://dbpedia.org/resource/Jacobian_matrix_and_determinant,http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant,"In vector calculus, the Jacobian matrix (/dʒəˈkoʊbiən/, /dʒɪ-, jɪ-/) of a vector-valued function of several variables is the matrix of all its first-order partial derivatives. When this matrix is square, that is, when the function takes the same number of variables as input as the number of vector components of its output, its determinant is referred to as the Jacobian determinant. Both the matrix and (if applicable) the determinant are often referred to simply as the Jacobian in literature. Suppose f : Rn → Rm is a function such that each of its first-order partial derivatives exist on Rn. This function takes a point x ∈ Rn as input and produces the vector f(x) ∈ Rm as output. Then the Jacobian matrix of f is defined to be an m×n matrix, denoted by J, whose (i,j)th entry is , or explicitly where is the transpose (row vector) of the gradient of the component. The Jacobian matrix, whose entries are functions of x, is denoted in various ways; common notations include Df, Jf, , and . Some authors define the Jacobian as the transpose of the form given above. The Jacobian matrix represents the differential of f at every point where f is differentiable. In detail, if h is a displacement vector represented by a column matrix, the matrix product J(x) ⋅ h is another displacement vector, that is the best linear approximation of the change of f in a neighborhood of x, if f(x) is differentiable at x. This means that the function that maps y to f(x) + J(x) ⋅ (y – x) is the best linear approximation of f(y) for all points y close to x. This linear function is known as the derivative or the differential of f at x. When m = n, the Jacobian matrix is square, so its determinant is a well-defined function of x, known as the Jacobian determinant of f. It carries important information about the local behavior of f. In particular, the function f has a differentiable inverse function in a neighborhood of a point x if and only if the Jacobian determinant is nonzero at x (see Jacobian conjecture for a related problem of global invertibility). The Jacobian determinant also appears when changing the variables in multiple integrals (see substitution rule for multiple variables). When m = 1, that is when f : Rn → R is a scalar-valued function, the Jacobian matrix reduces to the row vector ; this row vector of all first-order partial derivatives of f is the transpose of the gradient of f, i.e.. Specializing further, when m = n = 1, that is when f : R → R is a scalar-valued function of a single variable, the Jacobian matrix has a single entry; this entry is the derivative of the function f. These concepts are named after the mathematician Carl Gustav Jacob Jacobi (1804–1851).",1038528733015184812,related_concept,Design matrix,2024-06-23 21:17:09.728,[],"[""Newton's method""]",set(),set(),0,1.9475218658892128,1.3708460781656415
398,770,Indicator variable,http://dbpedia.org/resource/Indicator_variable,http://en.wikipedia.org/wiki/Indicator_variable,,1966465918723791599,related_concept,Design matrix,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.375,0.464408290819849
399,771,Moment matrix,http://dbpedia.org/resource/Moment_matrix,http://en.wikipedia.org/wiki/Moment_matrix,"In mathematics, a moment matrix is a special symmetric square matrix whose rows and columns are indexed by monomials. The entries of the matrix depend on the product of the indexing monomials only (cf. Hankel matrices.) Moment matrices play an important role in polynomial fitting, polynomial optimization (since positive semidefinite moment matrices correspond to polynomials which are sums of squares) and econometrics.",2860705011480602287,related_concept,Design matrix,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.875,0.9726714612396241
400,774,Design matrix,http://dbpedia.org/resource/Design_matrix,http://en.wikipedia.org/wiki/Design_matrix,"In statistics and in particular in regression analysis, a design matrix, also known as model matrix or regressor matrix and often denoted by X, is a matrix of values of explanatory variables of a set of objects. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. The design matrix is used in certain statistical models, e.g., the general linear model. It can contain indicator variables (ones and zeros) that indicate group membership in an ANOVA, or it can contain values of continuous variables. The design matrix contains data on the independent variables (also called explanatory variables) in statistical models which attempt to explain observed data on a response variable (often called a dependent variable) in terms of the explanatory variables. The theory relating to such models makes substantial use of matrix manipulations involving the design matrix: see for example linear regression. A notable feature of the concept of a design matrix is that it is able to represent a number of different experimental designs and statistical models, e.g., ANOVA, ANCOVA, and linear regression.",83755648523181270,main_concept,,2024-06-23 21:17:09.728,"['ANOVA', 'ANCOVA']","['ANOVA', 'ANCOVA']",{'Data'},set(),0,1.3225806451612903,1.3499809430136103
401,779,Squared deviations from the mean,http://dbpedia.org/resource/Squared_deviations_from_the_mean,http://en.wikipedia.org/wiki/Squared_deviations_from_the_mean,"Squared deviations from the mean (SDM) result from squaring deviations. In probability theory and statistics, the definition of variance is either the expected value of the SDM (when considering a theoretical distribution) or its average value (for actual experimental data). Computations for analysis of variance involve the partitioning of a sum of SDM.",104047792019513392,related_concept,Variance,2024-06-23 21:17:09.728,"['Computation', 'Squared deviations from the mean']","['Computation', 'Squared deviations from the mean', 'Hypothesis']",set(),set(),0,0.8666666666666667,1.3207892724145378
402,786,Variance-stabilizing transformation,http://dbpedia.org/resource/Variance-stabilizing_transformation,http://en.wikipedia.org/wiki/Variance-stabilizing_transformation,"In applied statistics, a variance-stabilizing transformation is a data transformation that is specifically chosen either to simplify considerations in graphical exploratory data analysis or to allow the application of simple regression-based or analysis of variance techniques.",5597013116456396950,related_concept,Variance,2024-06-23 21:17:09.728,[],"['Poisson distribution', 'Fisher transformation']",set(),set(),0,2.1333333333333333,1.3489399640430466
403,789,Pooled variance,http://dbpedia.org/resource/Pooled_variance,http://en.wikipedia.org/wiki/Pooled_variance,"In statistics, pooled variance (also known as combined variance, composite variance, or overall variance, and written ) is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same. The numerical estimate resulting from the use of this method is also called the pooled variance. Under the assumption of equal population variances, the pooled sample variance provides a higher precision estimate of variance than the individual sample variances. This higher precision can lead to increased statistical power when used in statistical tests that compare the populations, such as the t-test. The square root of a pooled variance estimator is known as a pooled standard deviation (also known as combined standard deviation, composite standard deviation, or overall standard deviation).",8959248093242786823,related_concept,Variance,2024-06-23 21:17:09.728,[],"['Pooled variance', 'Standard deviation']",set(),set(),0,1.1111111111111112,1.342865884755967
404,791,Unbiased estimation of standard deviation,http://dbpedia.org/resource/Unbiased_estimation_of_standard_deviation,http://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation,"In statistics and in particular statistical theory, unbiased estimation of a standard deviation is the calculation from a statistical sample of an estimated value of the standard deviation (a measure of statistical dispersion) of a population of values, in such a way that the expected value of the calculation equals the true value. Except in some important situations, outlined later, the task has little relevance to applications of statistics since its need is avoided by standard procedures, such as the use of significance tests and confidence intervals, or by using Bayesian analysis. However, for statistical theory, it provides an exemplar problem in the context of estimation theory which is both simple to state and for which results cannot be obtained in closed form. It also provides an example where imposing the requirement for unbiased estimation might be seen as just adding inconvenience, with no real benefit.",4392227159290961535,related_concept,Variance,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.1282798833819242,1.3347206869167265
405,793,U-statistic,http://dbpedia.org/resource/U-statistic,http://en.wikipedia.org/wiki/U-statistic,"In statistical theory, a U-statistic is a class of statistics that is especially important in estimation theory; the letter ""U"" stands for unbiased. In elementary statistics, U-statistics arise naturally in producing minimum-variance unbiased estimators. The theory of U-statistics allows a minimum-variance unbiased estimator to be derived from each unbiased estimator of an estimable parameter (alternatively, statistical functional) for large classes of probability distributions. An estimable parameter is a measurable function of the population's cumulative probability distribution: For example, for every probability distribution, the population median is an estimable parameter. The theory of U-statistics applies to general classes of probability distributions.",655774520080614335,related_concept,Variance,2024-06-23 21:17:09.728,['U-statistic'],['U-statistic'],set(),set(),0,1.5203488372093024,1.3422033721753142
406,796,IP address,http://dbpedia.org/resource/IP_address,http://en.wikipedia.org/wiki/IP_address,"An Internet Protocol address (IP address) is a numerical label such as 192.0.2.1 that is connected to a computer network that uses the Internet Protocol for communication. An IP address serves two main functions: network interface identification and location addressing. Internet Protocol version 4 (IPv4) defines an IP address as a 32-bit number. However, because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP (IPv6), using 128 bits for the IP address, was standardized in 1998. IPv6 deployment has been ongoing since the mid-2000s. IP addresses are written and displayed in human-readable notations, such as 192.0.2.1 in IPv4, and 2001:db8:0:1234:0:567:8:1 in IPv6. The size of the routing prefix of the address is designated in CIDR notation by suffixing the address with the number of significant bits, e.g., 192.0.2.1/24, which is equivalent to the historically used subnet mask 255.255.255.0. The IP address space is managed globally by the Internet Assigned Numbers Authority (IANA), and by five regional Internet registries (RIRs) responsible in their designated territories for assignment to local Internet registries, such as Internet service providers (ISPs), and other end users. IPv4 addresses were distributed by IANA to the RIRs in blocks of approximately 16.8 million addresses each, but have been exhausted at the IANA level since 2011. Only one of the RIRs still has a supply for local assignments in Africa. Some IPv4 addresses are reserved for private networks and are not globally unique. Network administrators assign an IP address to each device connected to a network. Such assignments may be on a static (fixed or permanent) or dynamic basis, depending on network practices and software features.",4937134710787083943,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,['IP address'],"['IP address', 'Boot', 'Data']",set(),set(),0,3629.438848920863,1.3247003424976813
407,797,Database,http://dbpedia.org/resource/Database,http://en.wikipedia.org/wiki/Database,"In computing, a database is an organized collection of data stored and accessed electronically. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. A (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term ""database"" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.",7233262169336864104,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,"['SQL', 'Relational databases']","['SQL', 'Relational databases', 'Database', 'Data', 'AI', 'XQuery', 'Object database', 'Data model', 'XML database']",set(),set(),0,6.831920903954802,1.0345727622160388
408,798,MIT license,http://dbpedia.org/resource/MIT_license,http://en.wikipedia.org/wiki/MIT_license,,7380243230299845148,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],['MIT license'],set(),set(),0,1.5677419354838709,0.5323645401389023
409,799,Multiset,http://dbpedia.org/resource/Multiset,http://en.wikipedia.org/wiki/Multiset,"In mathematics, a multiset (or bag, or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The number of instances given for each element is called the multiplicity of that element in the multiset. As a consequence, an infinite number of multisets exist which contain only elements a and b, but vary in the multiplicities of their elements: 
* The set {a, b} contains only elements a and b, each having multiplicity 1 when {a, b} is seen as a multiset. 
* In the multiset {a, a, b}, the element a has multiplicity 2, and b has multiplicity 1. 
* In the multiset {a, a, a, b, b, b}, a and b both have multiplicity 3. These objects are all different when viewed as multisets, although they are the same set, since they all consist of the same elements. As with sets, and in contrast to tuples, order does not matter in discriminating multisets, so {a, a, b} and {a, b, a} denote the same multiset. To distinguish between sets and multisets, a notation that incorporates square brackets is sometimes used: the multiset {a, a, b} can be denoted by [a, a, b]. The cardinality of a multiset is the sum of the multiplicities of all its elements. For example, in the multiset {a, a, b, b, b, c} the multiplicities of the members a, b, and c are respectively 2, 3, and 1, and therefore the cardinality of this multiset is 6. Nicolaas Govert de Bruijn coined the word multiset in the 1970s, according to Donald Knuth. However, the concept of multisets predates the coinage of the word multiset by many centuries. Knuth himself attributes the first study of multisets to the Indian mathematician Bhāskarāchārya, who described permutations of multisets around 1150. Other names have been proposed or used for this concept, including list, bunch, bag, heap, sample, weighted set, collection, and suite.",8428438602657368190,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],"['AI', 'Multiset', 'SQL']",set(),set(),0,2.90990990990991,1.396348308150773
410,801,Relational databases,http://dbpedia.org/resource/Relational_databases,http://en.wikipedia.org/wiki/Relational_databases,,3831972837153355661,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],"['SQL', 'Data', 'Dimension', 'Relational databases', 'XML database', 'Database']",set(),set(),0,0.297029702970297,0.42060781405369785
411,802,Frequent pattern mining,http://dbpedia.org/resource/Frequent_pattern_mining,http://en.wikipedia.org/wiki/Frequent_pattern_mining,,1485527137924817068,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],['Apriori algorithm'],set(),set(),0,0.14285714285714285,0.47156098774228067
412,803,Stock-keeping unit,http://dbpedia.org/resource/Stock-keeping_unit,http://en.wikipedia.org/wiki/Stock-keeping_unit,,5969551663237784634,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.6744186046511629,0.5483458311277811
413,804,Winepi,http://dbpedia.org/resource/Winepi,http://en.wikipedia.org/wiki/Winepi,,364449416776531907,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.2,0.45269700789222284
414,806,Hash tree (persistent data structure),http://dbpedia.org/resource/Hash_tree_(persistent_data_structure),http://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure),"In computer science, a hash tree (or hash trie) is a persistent data structure that can be used to implement sets and maps, intended to replace hash tables in purely functional programming. In its basic form, a hash tree stores the hashes of its keys, regarded as strings of bits, in a trie, with the actual keys and (optional) values stored at the trie's ""final"" nodes. Hash array mapped tries and Ctries are refined versions of this data structure, using particular type of trie implementations.",4588882119451746679,related_concept,Apriori algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.208955223880597,1.372973058431802
415,811,Decision boundary,http://dbpedia.org/resource/Decision_boundary,http://en.wikipedia.org/wiki/Decision_boundary,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.",5113148463578098070,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],['Decision boundary'],set(),set(),0,2.8461538461538463,1.3847915344304205
416,812,Affine geometry,http://dbpedia.org/resource/Affine_geometry,http://en.wikipedia.org/wiki/Affine_geometry,"In mathematics, affine geometry is what remains of Euclidean geometry when ignoring (mathematicians often say ""forgetting"") the metric notions of distance and angle. As the notion of parallel lines is one of the main properties that is independent of any metric, affine geometry is often considered as the study of parallel lines. Therefore, Playfair's axiom (Given a line L and a point P not on L, there is exactly one line parallel to L that passes through P.) is fundamental in affine geometry. Comparisons of figures in affine geometry are made with affine transformations, which are mappings that preserve alignment of points and parallelism of lines. Affine geometry can be developed in two ways that are essentially equivalent. In synthetic geometry, an affine space is a set of points to which is associated a set of lines, which satisfy some axioms (such as Playfair's axiom). Affine geometry can also be developed on the basis of linear algebra. In this context an affine space is a set of points equipped with a set of transformations (that is bijective mappings), the translations, which forms a vector space (over a given field, commonly the real numbers), and such that for any given ordered pair of points there is a unique translation sending the first point to the second; the composition of two translations is their sum in the vector space of the translations. In more concrete terms, this amounts to having an operation that associates to any ordered pair of points a vector and another operation that allows translation of a point by a vector to give another point; these operations are required to satisfy a number of axioms (notably that two successive translations have the effect of translation by the sum vector). By choosing any point as ""origin"", the points are in one-to-one correspondence with the vectors, but there is no preferred choice for the origin; thus an affine space may be viewed as obtained from its associated vector space by ""forgetting"" the origin (zero vector). The idea of forgetting the metric can be applied in the theory of manifolds. That is developed in the article on the affine connection.",6537220308670490956,related_concept,Hyperplane,2024-06-23 21:17:09.728,"['Euclidean geometry', 'Affine geometry']","['Euclidean geometry', 'Affine geometry', 'Geometry']",set(),set(),0,0.7824427480916031,3.2970530339034694e-304
417,813,Projective geometry,http://dbpedia.org/resource/Projective_geometry,http://en.wikipedia.org/wiki/Projective_geometry,"In mathematics, projective geometry is the study of geometric properties that are invariant with respect to projective transformations. This means that, compared to elementary Euclidean geometry, projective geometry has a different setting, projective space, and a selective set of basic geometric concepts. The basic intuitions are that projective space has more points than Euclidean space, for a given dimension, and that geometric transformations are permitted that transform the extra points (called ""points at infinity"") to Euclidean points, and vice-versa. Properties meaningful for projective geometry are respected by this new idea of transformation, which is more radical in its effects than can be expressed by a transformation matrix and translations (the affine transformations). The first issue for geometers is what kind of geometry is adequate for a novel situation. It is not possible to refer to angles in projective geometry as it is in Euclidean geometry, because angle is an example of a concept not invariant with respect to projective transformations, as is seen in perspective drawing. One source for projective geometry was indeed the theory of perspective. Another difference from elementary geometry is the way in which parallel lines can be said to meet in a point at infinity, once the concept is translated into projective geometry's terms. Again this notion has an intuitive basis, such as railway tracks meeting at the horizon in a perspective drawing. See projective plane for the basics of projective geometry in two dimensions. While the ideas were available earlier, projective geometry was mainly a development of the 19th century. This included the theory of complex projective space, the coordinates used (homogeneous coordinates) being complex numbers. Several major types of more abstract mathematics (including invariant theory, the Italian school of algebraic geometry, and Felix Klein's Erlangen programme resulting in the study of the classical groups) were motivated by projective geometry. It was also a subject with many practitioners for its own sake, as synthetic geometry. Another topic that developed from axiomatic studies of projective geometry is finite geometry. The topic of projective geometry is itself now divided into many research subtopics, two examples of which are projective algebraic geometry (the study of projective varieties) and projective differential geometry (the study of differential invariants of the projective transformations).",861728721675161803,related_concept,Hyperplane,2024-06-23 21:17:09.728,['Euclidean geometry'],"['Euclidean geometry', 'Projective geometry', 'Mean', 'Theorem', 'Euclidean plane', 'Axiom', 'Geometry', 'Collinear']",{'Geometry'},set(),0,1.7545454545454546,3.265698919098679e-304
418,817,Normal (geometry),http://dbpedia.org/resource/Normal_(geometry),http://en.wikipedia.org/wiki/Normal_(geometry),"In geometry, a normal is an object such as a line, ray, or vector that is perpendicular to a given object. For example, the normal line to a plane curve at a given point is the (infinite) line perpendicular to the tangent line to the curve at the point.A normal vector may have length one (a unit vector) or its length may represent the curvature of the object (a curvature vector); its algebraic sign may indicate sides (interior or exterior). In three dimensions, a surface normal, or simply normal, to a surface at point is a vector perpendicular to the tangent plane of the surface at P. The word ""normal"" is also used as an adjective: a line normal to a plane, the normal component of a force, the normal vector, etc. The concept of normality generalizes to orthogonality (right angles). The concept has been generalized to differentiable manifolds of arbitrary dimension embedded in a Euclidean space. The normal vector space or normal space of a manifold at point is the set of vectors which are orthogonal to the tangent space at Normal vectors are of special interest in the case of smooth curves and smooth surfaces. The normal is often used in 3D computer graphics (notice the singular, as only one normal will be defined) to determine a surface's orientation toward a light source for flat shading, or the orientation of each of the surface's corners (vertices) to mimic a curved surface with Phong shading. The foot of a normal at a point of interest Q (analogous to the foot of a perpendicular) can be defined at the point P on the surface where the normal vector contains Q.The normal distance of a point Q to a curve or to a surface is the Euclidean distance between Q and its foot P.",7326757841935072272,related_concept,Hyperplane,2024-06-23 21:17:09.728,['Euclidean distance'],['Euclidean distance'],set(),set(),0,3.0526315789473686,0.7710711295297876
419,818,Arrangement of hyperplanes,http://dbpedia.org/resource/Arrangement_of_hyperplanes,http://en.wikipedia.org/wiki/Arrangement_of_hyperplanes,"In geometry and combinatorics, an arrangement of hyperplanes is an arrangement of a finite set A of hyperplanes in a linear, affine, or projective space S. Questions about a hyperplane arrangement A generally concern geometrical, topological, or other properties of the complement, M(A), which is the set that remains when the hyperplanes are removed from the whole space. One may ask how these properties are related to the arrangement and its intersection semilattice.The intersection semilattice of A, written L(A), is the set of all subspaces that are obtained by intersecting some of the hyperplanes; among these subspaces are S itself, all the individual hyperplanes, all intersections of pairs of hyperplanes, etc. (excluding, in the affine case, the empty set). These intersection subspaces of A are also called the flats of A. The intersection semilattice L(A) is partially ordered by reverse inclusion. If the whole space S is 2-dimensional, the hyperplanes are lines; such an arrangement is often called an arrangement of lines. Historically, real arrangements of lines were the first arrangements investigated. If S is 3-dimensional one has an arrangement of planes.",3452266554929814175,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.8666666666666667,1.0172473509660622
420,819,Hypersurface,http://dbpedia.org/resource/Hypersurface,http://en.wikipedia.org/wiki/Hypersurface,"In geometry, a hypersurface is a generalization of the concepts of hyperplane, plane curve, and surface. A hypersurface is a manifold or an algebraic variety of dimension n − 1, which is embedded in an ambient space of dimension n, generally a Euclidean space, an affine space or a projective space.Hypersurfaces share, with surfaces in a three-dimensional space, the property of being defined by a single implicit equation, at least locally (near every point), and sometimes globally. A hypersurface in a (Euclidean, affine, or projective) space of dimension two is a plane curve. In a space of dimension three, it is a surface. For example, the equation defines an algebraic hypersurface of dimension n − 1 in the Euclidean space of dimension n. This hypersurface is also a smooth manifold, and is called a hypersphere or an (n – 1)-sphere.",8890949202231817189,related_concept,Hyperplane,2024-06-23 21:17:09.728,['Hypersurface'],['Hypersurface'],set(),set(),0,2.717391304347826,0.9835820642674282
421,820,Linear subspace,http://dbpedia.org/resource/Linear_subspace,http://en.wikipedia.org/wiki/Linear_subspace,"In mathematics, and more specifically in linear algebra, a linear subspace, also known as a vector subspace is a vector space that is a subset of some larger vector space. A linear subspace is usually simply called a subspace when the context serves to distinguish it from other types of subspaces.",3520581773195099529,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],['Algorithm'],set(),set(),0,2.611464968152866,1.3784783253980963
422,822,Reflection (mathematics),http://dbpedia.org/resource/Reflection_(mathematics),http://en.wikipedia.org/wiki/Reflection_(mathematics),"In mathematics, a reflection (also spelled reflexion) is a mapping from a Euclidean space to itself that is an isometry with a hyperplane as a set of fixed points; this set is called the axis (in dimension 2) or plane (in dimension 3) of reflection. The image of a figure by a reflection is its mirror image in the axis or plane of reflection. For example the mirror image of the small Latin letter p for a reflection with respect to a vertical axis would look like q. Its image by reflection in a horizontal axis would look like b. A reflection is an involution: when applied twice in succession, every point returns to its original location, and every geometrical object is restored to its original state. The term reflection is sometimes used for a larger class of mappings from a Euclidean space to itself, namely the non-identity isometries that are involutions. Such isometries have a set of fixed points (the ""mirror"") that is an affine subspace, but is possibly smaller than a hyperplane. For instance a reflection through a point is an involutive isometry with just one fixed point; the image of the letter p under itwould look like a d. This operation is also known as a central inversion , and exhibits Euclidean space as a symmetric space. In a Euclidean vector space, the reflection in the point situated at the origin is the same as vector negation. Other examples include reflections in a line in three-dimensional space. Typically, however, unqualified use of the term ""reflection"" means reflection in a hyperplane. Some mathematicians use ""flip"" as a synonym for ""reflection"".",9055895037858118581,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.4411764705882355,1.3631349860759678
423,823,Ambient space,http://dbpedia.org/resource/Ambient_space,http://en.wikipedia.org/wiki/Ambient_space,"An ambient space or ambient configuration space is the space surrounding an object. While the ambient space and hodological space are both considered ways of perceiving penetrable space, the former perceives space as navigable, while the latter perceives it as navigated.",7371372549930142006,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],[],{'Geometry'},set(),0,1.6578947368421053,1.2760533930937528
424,825,Euclidean subspace,http://dbpedia.org/resource/Euclidean_subspace,http://en.wikipedia.org/wiki/Euclidean_subspace,,5778232244729520726,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],['Euclidean subspace'],set(),set(),0,0.7916666666666666,0.279003207575163
425,826,Vector space,http://dbpedia.org/resource/Vector_space,http://en.wikipedia.org/wiki/Vector_space,"In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied (""scaled"") by numbers called scalars. Scalars are often real numbers, but can be complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space. Vector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations. Vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension. Many vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces.",571373814845832188,related_concept,Hyperplane,2024-06-23 21:17:09.728,['Vector space'],"['Vector space', 'Linear subspace', 'Cauchy sequence', 'Hahn–Banach theorem']",set(),set(),0,5.572727272727272,1.348426827487239
426,827,Projective space,http://dbpedia.org/resource/Projective_space,http://en.wikipedia.org/wiki/Projective_space,"In mathematics, the concept of a projective space originated from the visual effect of perspective, where parallel lines seem to meet at infinity. A projective space may thus be viewed as the extension of a Euclidean space, or, more generally, an affine space with points at infinity, in such a way that there is one point at infinity of each direction of parallel lines. This definition of a projective space has the disadvantage of not being isotropic, having two different sorts of points, which must be considered separately in proofs. Therefore, other definitions are generally preferred. There are two classes of definitions. In synthetic geometry, point and line are primitive entities that are related by the incidence relation ""a point is on a line"" or ""a line passes through a point"", which is subject to the axioms of projective geometry. For some such set of axioms, the projective spaces that are defined have been shown to be equivalent to those resulting from the following definition, which is more often encountered in modern textbooks. Using linear algebra, a projective space of dimension n is defined as the set of the vector lines (that is, vector subspaces of dimension one) in a vector space V of dimension n + 1. Equivalently, it is the quotient set of V \ {0} by the equivalence relation ""being on the same vector line"". As a vector line intersects the unit sphere of V in two antipodal points, projective spaces can be equivalently defined as spheres in which antipodal points are identified. A projective space of dimension 1 is a projective line, and a projective space of dimension 2 is a projective plane. Projective spaces are widely used in geometry, as allowing simpler statements and simpler proofs. For example, in affine geometry, two distinct lines in a plane intersect in at most one point, while, in projective geometry, they intersect in exactly one point. Also, there is only one class of conic sections, which can be distinguished only by their intersections with the line at infinity: two intersection points for hyperbolas; one for the parabola, which is tangent to the line at infinity; and no real intersection point of ellipses. In topology, and more specifically in manifold theory, projective spaces play a fundamental role, being typical examples of non-orientable manifolds.",8872874224262969999,related_concept,Hyperplane,2024-06-23 21:17:09.728,['Projective space'],"['Projective space', 'Vector space', 'Euclidean geometry', 'AI']",set(),set(),0,2.1782608695652175,3.2882728184159064e-304
427,828,Half-space (geometry),http://dbpedia.org/resource/Half-space_(geometry),http://en.wikipedia.org/wiki/Half-space_(geometry),"In geometry, a half-space is either of the two parts into which a plane divides the three-dimensional Euclidean space. If the space is two-dimensional, then a half-space is called a half-plane (open or closed). A half-space in a one-dimensional space is called a half-line or ray. More generally, a half-space is either of the two parts into which a hyperplane divides an affine space. That is, the points that are not incident to the hyperplane are partitioned into two convex sets (i.e., half-spaces), such that any subspace connecting a point in one set to a point in the other must intersect the hyperplane. A half-space can be either open or closed. An open half-space is either of the two open sets produced by the subtraction of a hyperplane from the affine space. A closed half-space is the union of an open half-space and the hyperplane that defines it. A half-space may be specified by a linear inequality, derived from the linear equation that specifies the defining hyperplane.A strict linear inequality specifies an open half-space: A non-strict one specifies a closed half-space: Here, one assumes that not all of the real numbers a1, a2, ..., an are zero.",7112114035167610537,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.260869565217392,1.029614094228012
428,829,Dimension,http://dbpedia.org/resource/Dimension,http://en.wikipedia.org/wiki/Dimension,"In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus, a line has a dimension of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are needed to locate a point within these spaces. In classical mechanics, space and time are different categories and refer to absolute space and time. That conception of the world is a four-dimensional space but not the one that was found necessary to describe electromagnetism. The four dimensions (4D) of spacetime consist of events that are not absolutely defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. 10 dimensions are used to describe superstring theory (6D hyperspace + 4D), 11 dimensions can describe supergravity and M-theory (7D hyperspace + 4D), and the state-space of quantum mechanics is an infinite-dimensional function space. The concept of dimension is not restricted to physical objects. High-dimensional spaces frequently occur in mathematics and the sciences. They may be Euclidean spaces or more general parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space in which we live.",5986878909856241727,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],[],{'Dimension'},set(),0,4.953744493392071,3.2978299576199497e-304
429,830,Codimension,http://dbpedia.org/resource/Codimension,http://en.wikipedia.org/wiki/Codimension,"In mathematics, codimension is a basic geometric idea that applies to subspaces in vector spaces, to submanifolds in manifolds, and suitable subsets of algebraic varieties. For affine and projective algebraic varieties, the codimension equals the height of the defining ideal. For this reason, the height of an ideal is often called its codimension. The dual concept is relative dimension.",2395616535063656027,related_concept,Hyperplane,2024-06-23 21:17:09.728,[],['Codimension'],{'Dimension'},set(),0,3.2452830188679247,3.2894076688173236e-304
430,834,Analytics,http://dbpedia.org/resource/Analytics,http://en.wikipedia.org/wiki/Analytics,"Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance. Organizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.",2019096065058492849,related_concept,Data analysis,2024-06-23 21:17:09.728,['Analytics'],"['Analytics', 'Data', 'Data analysis', 'IP address', 'PDF']",{'Analytics'},set(),0,5.6695652173913045,1.19977344884632
431,835,Data model,http://dbpedia.org/resource/Data_model,http://en.wikipedia.org/wiki/Data_model,"A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner. The term data model can refer to two distinct but closely related concepts. Sometimes it refers to an abstract formalization of the objects and relationships found in a particular application domain: for example the customers, products, and orders found in a manufacturing organization. At other times it refers to the set of concepts used in defining such formalizations: for example concepts such as entities, attributes, relations, or tables. So the ""data model"" of a banking application may be defined using the entity-relationship ""data model"". This article uses the term in both senses. A data model explicitly determines the structure of data. Data models are typically specified by a data specialist, data librarian, or a digital humanities scholar in a data modeling notation. These notations are often represented in graphical form. A data model can sometimes be referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models.",1422113893164221042,related_concept,Data analysis,2024-06-23 21:17:09.728,"['Data model', 'Data']","['Data model', 'Data', 'Entity–relationship model', 'Data structure', 'Data modeling', 'Information model']",set(),set(),0,2.6554621848739495,4.53208099533098e-304
432,837,Data science,http://dbpedia.org/resource/Data_science,http://en.wikipedia.org/wiki/Data_science,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains. Data science is related to data mining, machine learning, big data, computational statistics and analytics. Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyse actual phenomena"" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge. A data scientist is someone who creates programming code and combines it with statistical knowledge to create insights from data.",6020781326090456943,related_concept,Data analysis,2024-06-23 21:17:09.728,"['Data science', 'Data']","['Data science', 'Data', 'Classification', 'Big data', 'Data analysis']",set(),set(),0,6.251572327044025,1.3273525708666813
433,838,Big data,http://dbpedia.org/resource/Big_data,http://en.wikipedia.org/wiki/Big_data,"Big data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data. Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on"". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research. The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization. Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require ""massively parallel software running on tens, hundreds, or even thousands of servers"". What qualifies as ""big data"" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""",8771540144379153458,related_concept,Data analysis,2024-06-23 21:17:09.728,"['Big data', 'Data']","['Big data', 'Data', 'Data analysis', 'AI', 'Algorithm', 'Computation']",set(),set(),0,4.846753246753247,5.162340986000203e-304
434,842,Predictive analytics,http://dbpedia.org/resource/Predictive_analytics,http://en.wikipedia.org/wiki/Predictive_analytics,"Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modeling, and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions. The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.",4564114119260637630,related_concept,Data analysis,2024-06-23 21:17:09.728,['Predictive analytics'],"['Predictive analytics', 'AI', 'Machine learning', 'Regression analysis', 'Mean']",set(),set(),0,8.102941176470589,1.39107704983065
435,843,Sensitivity analysis,http://dbpedia.org/resource/Sensitivity_analysis,http://en.wikipedia.org/wiki/Sensitivity_analysis,"Sensitivity analysis is the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be divided and allocated to different sources of uncertainty in its inputs. A related practice is uncertainty analysis, which has a greater focus on uncertainty quantification and propagation of uncertainty; ideally, uncertainty and sensitivity analysis should be run in tandem. The process of recalculating outcomes under alternative assumptions to determine the impact of a variable under sensitivity analysis can be useful for a range of purposes, including: 
* Testing the robustness of the results of a model or system in the presence of uncertainty. 
* Increased understanding of the relationships between input and output variables in a system or model. 
* Uncertainty reduction, through the identification of model input that cause significant uncertainty in the output and should therefore be the focus of attention in order to increase robustness (perhaps by further research). 
* Searching for errors in the model (by encountering unexpected relationships between inputs and outputs). 
* Model simplification – fixing model input that has no effect on the output, or identifying and removing redundant parts of the model structure. 
* Enhancing communication from modelers to decision makers (e.g. by making recommendations more credible, understandable, compelling or persuasive). 
* Finding regions in the space of input factors for which the model output is either maximum or minimum or meets some optimum criterion (see optimization and Monte Carlo filtering). 
* In case of calibrating models with large number of parameters, a primary sensitivity test can ease the calibration stage by focusing on the sensitive parameters. Not knowing the sensitivity of parameters can result in time being uselessly spent on non-sensitive ones. 
* To seek to identify important connections between observations, model inputs, and predictions or forecasts, leading to the development of better models.",3445844759852719074,related_concept,Data analysis,2024-06-23 21:17:09.728,['Sensitivity analysis'],"['Sensitivity analysis', 'Regression analysis', 'Variance']",set(),set(),0,2.0442477876106193,1.2010106385572334
436,845,Structured data analysis (statistics),http://dbpedia.org/resource/Structured_data_analysis_(statistics),http://en.wikipedia.org/wiki/Structured_data_analysis_(statistics),"Structured data analysis is the statistical data analysis of structured data. This can arise either in the form of an a priori structure such as multiple-choice questionnaires or in situations with the need to search for structure that fits the given data, either exactly or approximately. This structure can then be used for making comparisons, predictions, manipulations etc.",6773972355185962465,related_concept,Data analysis,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.090909090909091,1.0923459106943525
437,846,Test method,http://dbpedia.org/resource/Test_method,http://en.wikipedia.org/wiki/Test_method,"A test method is a method for a test in science or engineering, such as a physical test, chemical test, or statistical test. It is a definitive procedure that produces a test result. In order to ensure accurate and relevant test results, a test method should be ""explicit, unambiguous, and experimentally feasible."", as well as effective and reproducible. A test can be considered an observation or experiment that determines one or more characteristics of a given sample, product, process, or service. The purpose of testing involves a prior determination of expected observation and a comparison of that expectation to what one actually observes. The results of testing can be qualitative (yes/no), quantitative (a measured value), or categorical and can be derived from personal observation or the output of a precision measuring instrument. Usually the test result is the dependent variable, the measured response based on the particular conditions of the test or the level of the independent variable. Some tests, however, may involve changing the independent variable to determine the level at which a certain response occurs: in this case, the test result is the independent variable.",5431223434060795719,related_concept,Data analysis,2024-06-23 21:17:09.728,[],['Test method'],set(),set(),0,3.0833333333333335,1.0450309327014016
438,847,Data,http://dbpedia.org/resource/Data,http://en.wikipedia.org/wiki/Data,"In the pursuit of knowledge, data (US: /ˈdætə/; UK: /ˈdeɪtə/) are a collection of discrete values that convey information, describing quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted. A datum is an individual value in a collection of data. Data are usually organized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures. Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements.Data are commonly used in scientific research, economics, and in virtually every other form of human organizational activity. Examples of data sets include price indices (such as consumer price index), unemployment rates, literacy rates, and census data. In this context, data represents the raw facts and figures which can be used in such a manner in order to capture the useful information out of it. Data are collected using techniques such as measurement, observation, query, or analysis, and typically represented as numbers or characters which may be further processed. Field data are data that are collected in an uncontrolled in-situ environment. Experimental data are data that are generated in the course of a controlled scientific experiment. Data is analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis. Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed and obvious instrument or data entry errors are corrected. Data are the atoms of decision making. As such, data can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulates over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as ""the new oil of the digital economy"". Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing. Advances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence (AI)) methods that allow for efficient applications of analytic methods to big data.",5887741910022942295,related_concept,Data analysis,2024-06-23 21:17:09.728,"['Experimental data', 'Data', 'AI', 'Outlier']","['Data', 'Experimental data', 'Outlier', 'AI', 'Data analysis']",{'Data'},set(),0,3.677530017152659,5.229002365094412e-304
439,848,Randomization,http://dbpedia.org/resource/Randomization,http://en.wikipedia.org/wiki/Randomization,"Randomization is the process of making something random. Randomization is not haphazard; instead, a random process is a sequence of random variables describing a process whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. For example, a random sample of individuals from a population refers to a sample where every individual has a known probability of being sampled. This would be contrasted with nonprobability sampling where arbitrary individuals are selected. In various contexts, randomization may involve: 
* generating a random permutation of a sequence (such as when shuffling cards); 
* selecting a random sample of a population (important in statistical sampling); 
* allocating experimental units via random assignment to a treatment or control condition; 
* generating random numbers (random number generation); or 
* transforming a data stream (such as when using a scrambler in telecommunications).",8950816673760723632,related_concept,Data analysis,2024-06-23 21:17:09.728,['Randomization'],"['Randomization', 'Inference', 'Survey sampling']",set(),set(),0,0.4927536231884058,1.0784629121739588
440,849,Exploratory data analysis,http://dbpedia.org/resource/Exploratory_data_analysis,http://en.wikipedia.org/wiki/Exploratory_data_analysis,"In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.",2883300456367511323,related_concept,Data analysis,2024-06-23 21:17:09.728,['Exploratory data analysis'],"['Exploratory data analysis', 'Data', 'Dimension', 'Dimensionality reduction', 'Statistics']",set(),set(),0,1.1434599156118144,5.077111374029456e-304
441,851,Data modeling,http://dbpedia.org/resource/Data_modeling,http://en.wikipedia.org/wiki/Data_modeling,Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques.,3501814460856926700,main_concept,Data analysis,2024-06-23 21:17:09.728,"['Data model', 'Data', 'Data modeling']","['Data model', 'Data', 'Data modeling', 'Database', 'Entity–relationship model']",set(),set(),0,2.5726872246696035,4.452236898719585e-304
442,852,Data analysis,http://dbpedia.org/resource/Data_analysis,http://en.wikipedia.org/wiki/Data_analysis,"Data analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively. Data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis. Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.",1268795899299371918,main_concept,,2024-06-23 21:17:09.728,"['Predictive analytics', 'Data', 'Data analysis', 'Data mining']","['Data', 'Data analysis', 'Predictive analytics', 'Data mining', 'Data collection', 'Descriptive statistics', 'Type I and type II errors', 'Hypothesis', 'Regression analysis', 'Analytics', 'Nonlinear system', 'Exploratory data analysis']",set(),set(),0,3.6267123287671232,1.234557428857649
443,854,Correlation and dependence,http://dbpedia.org/resource/Correlation_and_dependence,http://en.wikipedia.org/wiki/Correlation_and_dependence,,6328579468024986701,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,[],"['Pearson correlation coefficient', 'Mutual information', 'Distance correlation', ""Newton's method""]",set(),set(),0,2.255813953488372,0.38446961347978886
444,855,Concordance correlation coefficient,http://dbpedia.org/resource/Concordance_correlation_coefficient,http://en.wikipedia.org/wiki/Concordance_correlation_coefficient,"In statistics, the concordance correlation coefficient measures the agreement between two variables, e.g., to evaluate reproducibility or for inter-rater reliability.",5294590139017114947,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.25,1.364960899100176
445,856,Partial correlation,http://dbpedia.org/resource/Partial_correlation,http://en.wikipedia.org/wiki/Partial_correlation,"In probability theory and statistics, partial correlation measures the degree of association between two random variables, with the effect of a set of controlling random variables removed. When determining the numerical relationship between two variables of interest, using their correlation coefficient will give misleading results if there is another confounding variable that is numerically related to both variables of interest. This misleading information can be avoided by controlling for the confounding variable, which is done by computing the partial correlation coefficient. This is precisely the motivation for including other right-side variables in a multiple regression; but while multiple regression gives unbiased results for the effect size, it does not give a numerical value of a measure of the strength of the relationship between the two variables of interest. For example, given economic data on the consumption, income, and wealth of various individuals, consider the relationship between consumption and income. Failing to control for wealth when computing a correlation coefficient between consumption and income would give a misleading result, since income might be numerically related to wealth which in turn might be numerically related to consumption; a measured correlation between consumption and income might actually be contaminated by these other correlations. The use of a partial correlation avoids this problem. Like the correlation coefficient, the partial correlation coefficient takes on a value in the range from –1 to 1. The value –1 conveys a perfect negative correlation controlling for some variables (that is, an exact linear relationship in which higher values of one variable are associated with lower values of the other); the value 1 conveys a perfect positive linear relationship, and the value 0 conveys that there is no linear relationship. The partial correlation coincides with the conditional correlation if the random variables are jointly distributed as the multivariate normal, other elliptical, multivariate hypergeometric, multivariate negative hypergeometric, multinomial, or Dirichlet distribution, but not in general otherwise.",1271120361406795885,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,[],"['Pearson correlation coefficient', 'Computation', 'Statistics']",set(),set(),0,1.467914438502674,1.361715865582371
446,858,Fisher transformation,http://dbpedia.org/resource/Fisher_transformation,http://en.wikipedia.org/wiki/Fisher_transformation,"In statistics, the Fisher transformation (or Fisher z-transformation) of a Pearson correlation coefficient is its inverse hyperbolic tangent (artanh). When the sample correlation coefficient r is near 1 or -1, its distribution is highly skewed, which makes it difficult to estimate confidence intervals and apply tests of significance for the population correlation coefficient ρ. The Fisher transformation solves this problem by yielding a variable whose distribution is approximately normally distributed, with a variance that is stable over different values of r.",6766391321963478065,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,"['Fisher transformation', 'Pearson correlation coefficient']","['Fisher transformation', 'Pearson correlation coefficient', 'Variance-stabilizing transformation', 'Variance']",set(),set(),0,0.1111111111111111,1.349983650499819
447,863,Multiple correlation,http://dbpedia.org/resource/Multiple_correlation,http://en.wikipedia.org/wiki/Multiple_correlation,,6577214159411115355,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,[],['Pearson correlation coefficient'],set(),set(),0,2.56,0.3888401887558384
448,867,Student's t-distribution,http://dbpedia.org/resource/Student's_t-distribution,http://en.wikipedia.org/wiki/Student's_t-distribution,"In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a normally distributed population in situations where the sample size is small and the population's standard deviation is unknown. It was developed by English statistician William Sealy Gosset under the pseudonym ""Student"". The t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. Student's t-distribution also arises in the Bayesian analysis of data from a normal family. If we take a sample of observations from a normal distribution, then the t-distribution with degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term . In this way, the t-distribution can be used to construct a confidence interval for the true mean. The t-distribution is symmetric and bell-shaped, like the normal distribution. However, the t-distribution has heavier tails, meaning that it is more prone to producing values that fall far from its mean. This makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero. The Student's t-distribution is a special case of the generalized hyperbolic distribution.",3671949782660091337,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,"[""Student's t-test"", ""Student's t-distribution""]","['Cauchy distribution', ""Student's t-distribution"", 'PDF', ""Student's t-test"", 'Bayesian statistics', 'Bayesian inference', 'Confidence interval', 'Bayesian optimization']",set(),set(),0,0.9043887147335423,1.3297269607531135
449,868,Correlation ratio,http://dbpedia.org/resource/Correlation_ratio,http://en.wikipedia.org/wiki/Correlation_ratio,"In statistics, the correlation ratio is a measure of the curvilinear relationship between the statistical dispersion within individual categories and the dispersion across the whole population or sample. The measure is defined as the ratio of two standard deviations representing these types of variation. The context here is the same as that of the intraclass correlation coefficient, whose value is the square of the correlation ratio.",6307561463135260031,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,[],"['Statistics', 'Geometry']",set(),set(),0,1.1304347826086956,1.3690297383677457
450,870,RV coefficient,http://dbpedia.org/resource/RV_coefficient,http://en.wikipedia.org/wiki/RV_coefficient,"In statistics, the RV coefficientis a multivariate generalization of the squared Pearson correlation coefficient (because the RV coefficient takes values between 0 and 1). It measures the closeness of two set of points that may each be represented in a matrix. The major approaches within statistical multivariate data analysis can all be brought into a common framework in which the RV coefficient is maximised subject to relevant constraints. Specifically, these statistical methodologies include: 
* principal component analysis 
* canonical correlation analysis 
* multivariate regression 
* statistical classification (linear discrimination). One application of the RV coefficient is in functional neuroimaging where it can measure the similarity between two subjects' series of brain scansor between different scans of a same subject.",4501715892769334224,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,"['RV coefficient', 'Pearson correlation coefficient']","['RV coefficient', 'Pearson correlation coefficient']",set(),set(),0,1.0,1.312859176755097
451,872,Distance correlation,http://dbpedia.org/resource/Distance_correlation,http://en.wikipedia.org/wiki/Distance_correlation,"In statistics and in probability theory, distance correlation or distance covariance is a measure of dependence between two paired random vectors of arbitrary, not necessarily equal, dimension. The population distance correlation coefficient is zero if and only if the random vectors are independent. Thus, distance correlation measures both linear and nonlinear association between two random variables or random vectors. This is in contrast to Pearson's correlation, which can only detect linear association between two random variables. Distance correlation can be used to perform a statistical test of dependence with a permutation test. One first computes the distance correlation (involving the re-centering of Euclidean distance matrices) between two random vectors, and then compares this value to the distance correlations of many shuffles of the data.",2930091670254405264,related_concept,Pearson correlation coefficient,2024-06-23 21:17:09.728,"['Distance correlation', 'Euclidean distance']","['Distance correlation', 'Euclidean distance', 'Pearson correlation coefficient']",set(),set(),0,0.8727272727272727,1.3734016862980247
452,873,Pearson correlation coefficient,http://dbpedia.org/resource/Pearson_correlation_coefficient,http://en.wikipedia.org/wiki/Pearson_correlation_coefficient,"In statistics, the Pearson correlation coefficient (PCC, pronounced /ˈpɪərsən/) ― also known as Pearson's r, the Pearson product-moment correlation coefficient (PPMCC), the bivariate correlation, or colloquially simply as the correlation coefficient ― is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance, such that the result always has a value between −1 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationships or correlations. As a simple example, one would expect the age and height of a sample of teenagers from a high school to have a Pearson correlation coefficient significantly greater than 0, but less than 1 (as 1 would represent an unrealistically perfect correlation).",5209473431699040594,main_concept,,2024-06-23 21:17:09.728,['Pearson correlation coefficient'],"['Pearson correlation coefficient', 'Statistical inference', ""Student's t-distribution"", 'Confidence interval', 'Variance-stabilizing transformation', 'Fisher transformation', 'Variance', 'Cauchy distribution', 'Robust statistics', 'Exact test']",set(),set(),0,1.4192229038854807,1.3649300463409333
453,883,Decision rule,http://dbpedia.org/resource/Decision_rule,http://en.wikipedia.org/wiki/Decision_rule,"In decision theory, a decision rule is a function which maps an observation to an appropriate action. Decision rules play an important role in the theory of statistics and economics, and are closely related to the concept of a strategy in game theory. In order to evaluate the usefulness of a decision rule, it is necessary to have a loss function detailing the outcome of each action under different states.",4117088400968877673,related_concept,Sample size determination,2024-06-23 21:17:09.728,['Decision rule'],['Decision rule'],set(),set(),0,1.6666666666666667,1.4357214066464956
454,884,Systematic error,http://dbpedia.org/resource/Systematic_error,http://en.wikipedia.org/wiki/Systematic_error,,941585129043277551,related_concept,Sample size determination,2024-06-23 21:17:09.728,[],"['Systematic error', 'Stochastic']",set(),set(),0,1.4926108374384237,0.26731004659061386
455,887,Survey sampling,http://dbpedia.org/resource/Survey_sampling,http://en.wikipedia.org/wiki/Survey_sampling,"In statistics, survey sampling describes the process of selecting a sample of elements from a target population to conduct a survey. The term ""survey"" may refer to many different types or techniques of observation. In survey sampling it most often involves a questionnaire used to measure the characteristics and/or attitudes of people. Different ways of contacting members of a sample once they have been selected is the subject of survey data collection. The purpose of sampling is to reduce the cost and/or the amount of work that it would take to survey the entire target population. A survey that measures the entire target population is called a census. A sample refers to a group or section of a population from which information is to be obtained Survey samples can be broadly divided into two types: probability samples and super samples. Probability-based samples implement a sampling plan with specified probabilities (perhaps adapted probabilities specified by an adaptive procedure). Probability-based sampling allows design-based inference about the target population. The inferences are based on a known objective probability distribution that was specified in the study protocol. Inferences from probability-based surveys may still suffer from many types of bias. Surveys that are not based on probability sampling have greater difficulty measuring their bias or sampling error. Surveys based on non-probability samples often fail to represent the people in the target population. In academic and government survey research, probability sampling is a standard procedure. In the United States, the Office of Management and Budget's ""List of Standards for Statistical Surveys"" states that federally funded surveys must be performed: selecting samples using generally accepted statistical methods (e.g., probabilistic methods that can provide estimates of sampling error). Any use of nonprobability sampling methods (e.g., cut-off or model-based samples) must be justified statistically and be able to measure estimation error. Random sampling and design-based inference are supplemented by other statistical methods, such as model-assisted sampling and model-based sampling. For example, many surveys have substantial amounts of nonresponse. Even though the units are initially chosen with known probabilities, the nonresponse mechanisms are unknown. For surveys with substantial nonresponse, statisticians have proposed statistical models with which the data sets are analyzed. Issues related to survey sampling are discussed in several sources, including Salant and Dillman (1994).",451658424737381899,related_concept,Sample size determination,2024-06-23 21:17:09.728,"['Probability', 'Inference']","['Probability', 'Inference']",set(),set(),0,1.3697478991596639,5.951372622294211e-304
456,896,Neyman–Pearson lemma,http://dbpedia.org/resource/Neyman–Pearson_lemma,http://en.wikipedia.org/wiki/Neyman–Pearson_lemma,"In statistics, the Neyman–Pearson lemma was introduced by Jerzy Neyman and Egon Pearson in a paper in 1933. The Neyman-Pearson lemma is part of the Neyman-Pearson theory of statistical testing, which introduced concepts like errors of the second kind, power function, and inductive behavior. The previous Fisherian theory of significance testing postulated only one hypothesis. By introducing a competing hypothesis, the Neyman-Pearsonian flavor of statistical testing allows investigating the two types of errors. The trivial cases where one always rejects or accepts the null hypothesis are of little interest but it does prove that one must not relinquish control over one type of error while calibrating the other. Neyman and Pearson accordingly proceeded to restrict their attention to the class of all level tests while subsequently minimizing type II error, traditionally denoted by . Their seminal paper of 1933, including the Neyman-Pearson lemma, comes at the end of this endeavor, not only showing the existence of tests with the most power that retain a prespecified level of type I error, but also providing a way to construct such tests. The Karlin-Rubin theorem extends the Neyman-Pearson lemma to settings involving composite hypotheses with monotone likelihood ratios.",4907238941981109928,related_concept,Type I and type II errors,2024-06-23 21:17:09.728,[],['Neyman–Pearson lemma'],set(),set(),0,1.368421052631579,1.0613978656695855
457,899,Family-wise error rate,http://dbpedia.org/resource/Family-wise_error_rate,http://en.wikipedia.org/wiki/Family-wise_error_rate,"In statistics, family-wise error rate (FWER) is the probability of making one or more false discoveries, or type I errors when performing multiple hypotheses tests.",7667541832573833422,related_concept,Type I and type II errors,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.8653846153846154,5.812683179864483e-304
458,900,Type III error,http://dbpedia.org/resource/Type_III_error,http://en.wikipedia.org/wiki/Type_III_error,"In statistical hypothesis testing, there are various notions of so-called type III errors (or errors of the third kind), and sometimes type IV errors or higher, by analogy with the type I and type II errors of Jerzy Neyman and Egon Pearson. Fundamentally, type III errors occur when researchers provide the right answer to the wrong question, i.e. when the correct hypothesis is rejected but for the wrong reason. Since the paired notions of type I errors (or ""false positives"") and type II errors (or ""false negatives"") that were introduced by Neyman and Pearson are now widely used, their choice of terminology (""errors of the first kind"" and ""errors of the second kind""), has led others to suppose that certain sorts of mistakes that they have identified might be an ""error of the third kind"", ""fourth kind"", etc. None of these proposed categories have been widely accepted. The following is a brief account of some of these proposals.",8037093731937592908,related_concept,Type I and type II errors,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.7272727272727273,0.9576376527805951
459,905,Errors and residuals,http://dbpedia.org/resource/Errors_and_residuals,http://en.wikipedia.org/wiki/Errors_and_residuals,"In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its ""true value"" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.In econometrics, ""errors"" are also called disturbances.",3028905871654072248,related_concept,Type I and type II errors,2024-06-23 21:17:09.728,[],"[""Student's t-distribution"", 'ANOVA']",set(),set(),0,1.507936507936508,1.249194617175357
460,916,Fuzzy logic,http://dbpedia.org/resource/Fuzzy_logic,http://en.wikipedia.org/wiki/Fuzzy_logic,"Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1. The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by Iranian Azerbaijani mathematician Lotfi Zadeh. Fuzzy logic had, however, been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski. Fuzzy logic is based on the observation that people make decisions based on imprecise and non-numerical information. Fuzzy models or sets are mathematical means of representing vagueness and imprecise information (hence the term fuzzy). These models have the capability of recognising, representing, manipulating, interpreting, and using data and information that are vague and lack certainty. Fuzzy logic has been applied to many fields, from control theory to artificial intelligence.",4388528209379404971,related_concept,Expert system,2024-06-23 21:17:09.728,['Fuzzy logic'],"['Fuzzy logic', 'Fuzzy set', 'Neural network', 'SQL', 'Probability', ""Bayes' theorem"", 'Computation']",set(),set(),0,3.182926829268293,1.4229221535636285
461,917,Reasoning system,http://dbpedia.org/resource/Reasoning_system,http://en.wikipedia.org/wiki/Reasoning_system,"In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems. By the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. In typical use in the Information Technology field however, the phrase is usually reserved for systems that perform more complex kinds of reasoning. For example, not for systems that do fairly straightforward types of reasoning such as calculating a sales tax or customer discount but making logical inferences about a medical diagnosis or mathematical theorem. Reasoning systems come in two modes: interactive and batch processing. Interactive systems interface with the user to ask clarifying questions or otherwise allow the user to guide the reasoning process. Batch systems take in all the available information at once and generate the best answer possible without user feedback or guidance. Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing.",8839679354111486874,related_concept,Expert system,2024-06-23 21:17:09.728,['Reasoning system'],"['Reasoning system', 'Expert system', 'Heuristic', 'Bayesian inference', 'Theorem', 'Prolog', 'Deductive classifier', 'Machine learning']",set(),set(),0,0.8260869565217391,1.3905922381989626
462,918,Speech recognition,http://dbpedia.org/resource/Speech_recognition,http://en.wikipedia.org/wiki/Speech_recognition,"Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis. Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called ""speaker-independent"" systems. Systems that use training are called ""speaker dependent"". Speech recognition applications include voice user interfaces such as voice dialing (e.g. ""call home""), call routing (e.g. ""I would like to make a collect call""), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input). The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process. From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.",8935225546629473306,related_concept,Expert system,2024-06-23 21:17:09.728,['Speech recognition'],"['Speech recognition', 'Classification', 'Viterbi algorithm', 'Neural network', 'Mean', 'Accuracy', 'Word error rate']",set(),set(),0,2.748663101604278,1.4168712680023676
463,919,MYCIN,http://dbpedia.org/resource/MYCIN,http://en.wikipedia.org/wiki/MYCIN,,7607387899672747480,related_concept,Expert system,2024-06-23 21:17:09.728,[],"['MYCIN', 'Bayesian statistics', 'Bayesian network']",set(),set(),0,0.6170212765957447,4.220826237923601e-305
464,921,Prolog,http://dbpedia.org/resource/Prolog,http://en.wikipedia.org/wiki/Prolog,"Prolog is a logic programming language associated with artificial intelligence and computational linguistics. Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations. The language was developed and implemented in Marseille, France, in 1972 by Alain Colmerauer with Philippe Roussel, based on Robert Kowalski's procedural interpretation of Horn clauses at University of Edinburgh. Prolog was one of the first logic programming languages and remains the most popular such language today, with several free and commercial implementations available. The language has been used for theorem proving, expert systems, term rewriting, type systems, and automated planning, as well as its original intended field of use, natural language processing. Modern Prolog environments support the creation of graphical user interfaces, as well as administrative and networked applications. Prolog is well-suited for specific tasks that benefit from rule-based logical queries such as searching databases, voice control systems, and filling templates.",5182865640603442997,related_concept,Expert system,2024-06-23 21:17:09.728,['Prolog'],"['Prolog', 'Iterative', 'Logically', 'AI', 'Datalog', 'Data']",set(),set(),0,2.047345767575323,1.3623352731448113
465,922,Knowledge acquisition,http://dbpedia.org/resource/Knowledge_acquisition,http://en.wikipedia.org/wiki/Knowledge_acquisition,"Knowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. Expert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. As expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works on the topic used Batesonian theories of learning to guide the process. One approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems. A more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL). In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics.",6220401455717265004,related_concept,Expert system,2024-06-23 21:17:09.728,"['Knowledge acquisition', 'AI', 'Expert system']","['Knowledge acquisition', 'AI', 'Expert system']",set(),set(),0,2.689655172413793,1.3899162681879185
466,923,Inference engine,http://dbpedia.org/resource/Inference_engine,http://en.wikipedia.org/wiki/Inference_engine,"In the field of artificial intelligence, an inference engine is a component of the system that applies logical rules to the knowledge base to deduce new information. The first inference engines were components of expert systems. The typical expert system consisted of a knowledge base and an inference engine. The knowledge base stored facts about the world. The inference engine applies logical rules to the knowledge base and deduced new knowledge. This process would iterate as each new fact in the knowledge base could trigger additional rules in the inference engine. Inference engines work primarily in one of two modes either special rule or facts: forward chaining and backward chaining. Forward chaining starts with the known facts and asserts new facts. Backward chaining starts with goals, and works backward to determine what facts must be asserted so that the goals can be achieved.",1248533443565276317,related_concept,Expert system,2024-06-23 21:17:09.728,"['Inference engine', 'Inference']","['Inference engine', 'Inference', 'MYCIN', 'AI', 'Prolog']",{'Inference'},set(),0,1.6951219512195121,1.4020156264609778
467,924,Automated reasoning,http://dbpedia.org/resource/Automated_reasoning,http://en.wikipedia.org/wiki/Automated_reasoning,"In computer science, in particular in knowledge representation and reasoning and metalogic, the area of automated reasoning is dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science and philosophy. The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). Extensive work has also been done in reasoning by analogy using induction and abduction. Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system is an example of an automated argumentation system that is more specific than being just an automated theorem prover. Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and many less formal ad hoc techniques.",778443997730052074,related_concept,Expert system,2024-06-23 21:17:09.728,['Bayesian inference'],"['Bayesian inference', 'Automated reasoning', 'AI', 'Mathematics']",set(),set(),0,2.258992805755396,1.3965944525695502
468,926,Knowledge representation,http://dbpedia.org/resource/Knowledge_representation,http://en.wikipedia.org/wiki/Knowledge_representation,,8552892439172708192,related_concept,Expert system,2024-06-23 21:17:09.728,[],"['Knowledge representation', 'AI', 'Prolog', 'Expert system', 'Mean']",set(),set(),0,0.9159159159159159,0.43549584535805
469,927,AI,http://dbpedia.org/resource/AI,http://en.wikipedia.org/wiki/AI,,4235587077191874011,related_concept,Expert system,2024-06-23 21:17:09.728,[],"['AI', 'Google Search', 'Knowledge representation', 'Knowledge base', 'Machine learning', 'Unsupervised learning', 'Supervised learning', 'Deep learning', 'Computation', 'Computational learning theory', 'Natural language processing', 'Heuristic', 'Gradient', 'Gradient descent', 'Prolog', 'Inference', 'Fuzzy logic', 'Bayesian network', 'Bayesian inference', 'Kalman filter', 'Kernel methods', 'Bayes classifier', 'K-nearest neighbor', 'Neural network', 'Recurrent neural network', 'Perceptron', 'Convolutional neural network', 'Data', 'Evolution']",set(),set(),0,0.5737507525586996,0.2841409805107933
470,928,Knowledge engineering,http://dbpedia.org/resource/Knowledge_engineering,http://en.wikipedia.org/wiki/Knowledge_engineering,"Knowledge engineering (KE) refers to all technical, scientific and social aspects involved in building, maintaining and using knowledge-based systems.",905310447542642869,related_concept,Expert system,2024-06-23 21:17:09.728,['Knowledge engineering'],"['Knowledge engineering', 'MYCIN', 'Expert system', 'Knowledge acquisition']",set(),set(),0,2.2432432432432434,1.3685383108432088
471,929,Deductive classifier,http://dbpedia.org/resource/Deductive_classifier,http://en.wikipedia.org/wiki/Deductive_classifier,"A deductive classifier is a type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to theorem provers in that they take as input and produce output via First Order Logic. Classifiers originated with KL-ONE Frame languages. They are increasingly significant now that they form a part in the enabling technology of the Semantic Web. Modern classifiers leverage the Web Ontology Language. The models they analyze and generate are called ontologies.",2417221200671257162,related_concept,Expert system,2024-06-23 21:17:09.728,[],['Rule-based system'],set(),set(),0,0.8666666666666667,1.4147814509674488
472,931,CADUCEUS (expert system),http://dbpedia.org/resource/CADUCEUS_(expert_system),http://en.wikipedia.org/wiki/CADUCEUS_(expert_system),"CADUCEUS was a medical expert system finished in the mid-1980s (first begun in the 1970s- it took a long time to build the knowledge base) by (of the University of Pittsburgh), building on Pople's years of interviews with Dr. Jack Meyers, one of the top internal medicine diagnosticians and a professor at the University of Pittsburgh. Their motivation was to improve on MYCIN (which focused on blood-borne infectious bacteria) to focus on more comprehensive issues than a narrow field like blood poisoning (though it would do it in a similar manner); instead embracing all internal medicine. CADUCEUS eventually could diagnose up to 1000 different diseases. While CADUCEUS worked using an inference engine similar to MYCIN's, it made a number of changes (like incorporating abductive reasoning) to deal with the additional complexity of internal disease- there can be a number of simultaneous diseases, and data is generally flawed and scarce. CADUCEUS has been described as the ""most knowledge-intensive expert system in existence"".",3028744481257279911,related_concept,Expert system,2024-06-23 21:17:09.728,['MYCIN'],['MYCIN'],set(),set(),0,0.8,1.4013149514473018
473,932,Rule-based system,http://dbpedia.org/resource/Rule-based_system,http://en.wikipedia.org/wiki/Rule-based_system,"In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way. It is often used in artificial intelligence applications and research. Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets. Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type.",9037704410323449598,related_concept,Expert system,2024-06-23 21:17:09.728,['Rule-based system'],"['Rule-based system', 'Datalog', 'Data', 'Prolog']",set(),set(),0,1.4930555555555556,1.4108456792827286
474,933,Knowledge-based systems,http://dbpedia.org/resource/Knowledge-based_systems,http://en.wikipedia.org/wiki/Knowledge-based_systems,"A knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and refers to many different kinds of systems. The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly and a reasoning system that allows it to derive new knowledge. Thus, a knowledge-based system has two distinguishing features: a knowledge base and an inference engine. The first part, the knowledge base, represents facts about the world, often in some form of subsumption ontology (rather than implicitly embedded in procedural code, in the way a conventional computer program does). Other common approaches in addition to a subsumption ontology include frames, conceptual graphs, and logical assertions. The second part, the inference engine, allows new knowledge to be inferred. Most commonly, it can take the form of IF-THEN rules coupled with forward chaining or backward chaining approaches. Other approaches include the use of automated theorem provers, logic programming, blackboard systems, and term rewriting systems such as CHR (Constraint Handling Rules). These more formal approaches are covered in detail in the Wikipedia article on knowledge representation and reasoning.",1239706775977064806,related_concept,Expert system,2024-06-23 21:17:09.728,[],['Knowledge-based systems'],set(),set(),0,1.3978494623655915,1.3765445381371135
475,934,Knowledge base,http://dbpedia.org/resource/Knowledge_base,http://en.wikipedia.org/wiki/Knowledge_base,"A knowledge base (KB) is a technology used to store complex structured and unstructured information used by a computer system. The initial use of the term was in connection with expert systems, which were the first knowledge-based systems.",6416700406991378816,main_concept,Expert system,2024-06-23 21:17:09.728,[],"['AI', 'Knowledge management']",set(),set(),0,3.66,1.3725127497102219
476,936,Expert system,http://dbpedia.org/resource/Expert_system,http://en.wikipedia.org/wiki/Expert_system,"In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.",326561337813828368,main_concept,,2024-06-23 21:17:09.728,"['Inference engine', 'AI', 'Expert system', 'Inference']","['AI', 'Expert system', 'MYCIN', 'Heuristic', 'Prolog', 'Data', 'Inference', 'Recurrent neural network']",set(),set(),0,3.2883435582822087,1.4015557852979441
477,937,Proportionality (mathematics),http://dbpedia.org/resource/Proportionality_(mathematics),http://en.wikipedia.org/wiki/Proportionality_(mathematics),"In mathematics, two sequences of numbers, often experimental data, are proportional or directly proportional if their corresponding elements have a constant ratio, which is called the coefficient of proportionality or proportionality constant. Two sequences are inversely proportional if corresponding elements have a constant product, also called the coefficient of proportionality. This definition is commonly extended to related varying quantities, which are often called variables. This meaning of variable is not the common meaning of the term in mathematics (see variable (mathematics)); these two different concepts share the same name for historical reasons. Two functions and are proportional if their ratio is a constant function. If several pairs of variables share the same direct proportionality constant, the equation expressing the equality of these ratios is called a proportion, e.g., a/b = x/y = ⋯ = k (for details see Ratio).Proportionality is closely related to linearity.",4231110447652038142,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],[],set(),set(),0,8.483870967741936,1.3776573290795975
478,938,Hamiltonian system,http://dbpedia.org/resource/Hamiltonian_system,http://en.wikipedia.org/wiki/Hamiltonian_system,"A Hamiltonian system is a dynamical system governed by Hamilton's equations. In physics, this dynamical system describes the evolution of a physical system such as a planetary system or an electron in an electromagnetic field. These systems can be studied in both Hamiltonian mechanics and dynamical systems theory.",7278248013123376772,related_concept,Nonlinear system,2024-06-23 21:17:09.728,['Hamiltonian system'],['Hamiltonian system'],set(),set(),0,1.7692307692307692,4.559285654476129e-304
479,939,Superposition principle,http://dbpedia.org/resource/Superposition_principle,http://en.wikipedia.org/wiki/Superposition_principle,"The superposition principle, also known as superposition property, states that, for all linear systems, the net response caused by two or more stimuli is the sum of the responses that would have been caused by each stimulus individually. So that if input A produces response X and input B produces response Y then input (A + B) produces response (X + Y). A function that satisfies the superposition principle is called a linear function. Superposition can be defined by two simpler properties: additivity and homogeneityfor scalar a. This principle has many applications in physics and engineering because many physical systems can be modeled as linear systems. For example, a beam can be modeled as a linear system where the input stimulus is the load on the beam and the output response is the deflection of the beam. The importance of linear systems is that they are easier to analyze mathematically; there is a large body of mathematical techniques, frequency domain linear transform methods such as Fourier and Laplace transforms, and linear operator theory, that are applicable. Because physical systems are generally only approximately linear, the superposition principle is only an approximation of the true physical behavior. The superposition principle applies to any linear system, including algebraic equations, linear differential equations, and systems of equations of those forms. The stimuli and responses could be numbers, functions, vectors, vector fields, time-varying signals, or any other object that satisfies certain axioms. Note that when vectors or vector fields are involved, a superposition is interpreted as a vector sum. If the superposition holds, then it automatically also holds for all linear operations applied on these functions (due to definition), such as gradients, differentials or integrals (if they exist).",5880800789142806577,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],['Superposition principle'],set(),set(),0,3.7606837606837606,1.1920843801452794
480,940,Nonlinear system identification,http://dbpedia.org/resource/Nonlinear_system_identification,http://en.wikipedia.org/wiki/Nonlinear_system_identification,"System identification is a method of identifying or measuring the mathematical model of a system from measurements of the system inputs and outputs. The applications of system identification include any system where the inputs and outputs can be measured and include industrial processes, control systems, economic data, biology and the life sciences, medicine, social systems and many more. A nonlinear system is defined as any system that is not linear, that is any system that does not satisfy the superposition principle. This negative definition tends to obscure that there are very many different types of nonlinear systems. Historically, system identification for nonlinear systems has developed by focusing on specific classes of system and can be broadly categorized into five basic approaches, each defined by a model class: 1. 
* Volterra series models, 2. 
* Block-structured models, 3. 
* Neural network models, 4. 
* NARMAX models, and 5. 
* State-space models. There are four steps to be followed for system identification: data gathering, model postulate, parameter identification and model validation. Data gathering is considered as the first and essential part in identification terminology, used as the input for the model which is prepared later. It consists of selecting an appropriate data set, pre-processing and processing. It involves the implementation of the known algorithms together with the transcription of flight tapes, data storage and data management, calibration, processing, analysis and presentation. Moreover, model validation is necessary to gain confidence in, or reject, a particular model. In particular, the parameter estimation and the model validation are integral parts of the system identification. Validation refers to the process of confirming the conceptual model and demonstrating an adequate correspondence between the computational results of the model and the actual data.",7051181214385694333,related_concept,Nonlinear system,2024-06-23 21:17:09.728,"['Data', 'Neural network']","['Data', 'Neural network', 'Artificial neural network', 'Theorem']",set(),set(),0,0.8444444444444444,1.0676665668862797
481,941,Linear system,http://dbpedia.org/resource/Linear_system,http://en.wikipedia.org/wiki/Linear_system,"In systems theory, a linear system is a mathematical model of a system based on the use of a linear operator.Linear systems typically exhibit features and properties that are much simpler than the nonlinear case.As a mathematical abstraction or idealization, linear systems find important applications in automatic control theory, signal processing, and telecommunications. For example, the propagation medium for wireless communication systems can often bemodeled by linear systems.",109755472016606373,related_concept,Nonlinear system,2024-06-23 21:17:09.728,['Linear system'],['Linear system'],set(),set(),0,4.03921568627451,1.1654432434462634
482,942,Polynomial,http://dbpedia.org/resource/Polynomial,http://en.wikipedia.org/wiki/Polynomial,"In mathematics, a polynomial is an expression consisting of indeterminates (also called variables) and coefficients, that involves only the operations of addition, subtraction, multiplication, and positive-integer powers of variables. An example of a polynomial of a single indeterminate x is x2 − 4x + 7. An example with three indeterminates is x3 + 2xyz2 − yz + 1. Polynomials appear in many areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated scientific problems; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, which are central concepts in algebra and algebraic geometry.",4124006999489180347,related_concept,Nonlinear system,2024-06-23 21:17:09.728,['Polynomial'],"['Polynomial', 'Root-finding algorithm', 'Theorem']",set(),set(),0,7.877118644067797,4.6976327364792186e-304
483,943,Initial condition,http://dbpedia.org/resource/Initial_condition,http://en.wikipedia.org/wiki/Initial_condition,"In mathematics and particularly in dynamic systems, an initial condition, in some contexts called a seed value, is a value of an evolving variable at some point in time designated as the initial time (typically denoted t = 0). For a system of order k (the number of time lags in discrete time, or the order of the largest derivative in continuous time) and dimension n (that is, with n different evolving variables, which together can be denoted by an n-dimensional coordinate vector), generally nk initial conditions are needed in order to trace the system's variables forward through time. In both differential equations in continuous time and difference equations in discrete time, initial conditions affect the value of the dynamic variables (state variables) at any future time. In continuous time, the problem of finding a closed form solution for the state variables as a function of time and of the initial conditions is called the initial value problem. A corresponding problem exists for discrete time situations. While a closed form solution is not always possible to obtain, future values of a discrete time system can be found by iterating forward one time period per iteration, though rounding error may make this impractical over long horizons.",4367965738768822022,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],"['Nonlinear system', ""Newton's method""]",set(),set(),0,4.54054054054054,4.565790173913572e-304
484,944,Chaos theory,http://dbpedia.org/resource/Chaos_theory,http://en.wikipedia.org/wiki/Chaos_theory,"Chaos theory is an interdisciplinary area of scientific study and branch of mathematics focused on underlying patterns and deterministic laws of dynamical systems that are highly sensitive to initial conditions, and were once thought to have completely random states of disorder and irregularities. Chaos theory states that within the apparent randomness of chaotic complex systems, there are underlying patterns, interconnection, constant feedback loops, repetition, self-similarity, fractals, and self-organization. The butterfly effect, an underlying principle of chaos, describes how a small change in one state of a deterministic nonlinear system can result in large differences in a later state (meaning that there is sensitive dependence on initial conditions). A metaphor for this behavior is that a butterfly flapping its wings in Brazil can cause a tornado in Texas. Small differences in initial conditions, such as those due to errors in measurements or due to rounding errors in numerical computation, can yield widely diverging outcomes for such dynamical systems, rendering long-term prediction of their behavior impossible in general. This can happen even though these systems are deterministic, meaning that their future behavior follows a unique evolution and is fully determined by their initial conditions, with no random elements involved. In other words, the deterministic nature of these systems does not make them predictable. This behavior is known as deterministic chaos, or simply chaos. The theory was summarized by Edward Lorenz as: Chaos: When the present determines the future, but the approximate present does not approximately determine the future. Chaotic behavior exists in many natural systems, including fluid flow, heartbeat irregularities, weather, and climate. It also occurs spontaneously in some systems with artificial components, such as the stock market and road traffic. This behavior can be studied through the analysis of a chaotic mathematical model, or through analytical techniques such as recurrence plots and Poincaré maps. Chaos theory has applications in a variety of disciplines, including meteorology, anthropology, sociology, environmental science, computer science, engineering, economics, ecology, and pandemic crisis management. The theory formed the basis for such fields of study as complex dynamical systems, edge of chaos theory, and self-assembly processes.",9207521428586837186,related_concept,Nonlinear system,2024-06-23 21:17:09.728,['Chaos theory'],"['Chaos theory', 'Theorem', 'Euclidean plane', 'Euclidean geometry', 'Hamiltonian system', 'Geometry', 'AI']",set(),set(),0,4.544921875,0.9916471079142719
485,945,Differential equation,http://dbpedia.org/resource/Differential_equation,http://en.wikipedia.org/wiki/Differential_equation,"In mathematics, a differential equation is an equation that relates one or more unknown functions and their derivatives. In applications, the functions generally represent physical quantities, the derivatives represent their rates of change, and the differential equation defines a relationship between the two. Such relations are common; therefore, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology. Mainly the study of differential equations consists of the study of their solutions (the set of functions that satisfy each equation), and of the properties of their solutions. Only the simplest differential equations are solvable by explicit formulas; however, many properties of solutions of a given differential equation may be determined without computing them exactly. Often when a closed-form expression for the solutions is not available, solutions may be approximated numerically using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.",3988011320243966487,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],"['Differential equation', 'Stochastic', 'Ordinary differential equation']",set(),set(),0,5.573134328358209,4.6312417492480085e-304
486,946,Logistic map,http://dbpedia.org/resource/Logistic_map,http://en.wikipedia.org/wiki/Logistic_map,"The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often referred to as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a 1976 paper by the biologist Robert May, in part as a discrete-time demographic model analogous to the logistic equation written down by Pierre François Verhulst.Mathematically, the logistic map is written where xn is a number between zero and one, that represents the ratio of existing population to the maximum possible population. This nonlinear difference equation is intended to capture two effects: 
* reproduction where the population will increase at a rate proportional to the current population when the population size is small. 
* starvation (density-dependent mortality) where the growth rate will decrease at a rate proportional to the value obtained by taking the theoretical ""carrying capacity"" of the environment less the current population. The usual values of interest for the parameter are those in the interval [0, 4], so that xn remains bounded on [0, 1]. The r = 4 case of the logistic map is a nonlinear transformation of both the bit-shift map and the μ = 2 case of the tent map. If r > 4 this leads to negative population sizes. (This problem does not appear in the older Ricker model, which also exhibits chaotic dynamics.) One can also consider values of r in the interval [−2, 0], so that xn remains bounded on [−0.5, 1.5].",1789722409506730230,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2173913043478262,0.9544712254060064
487,947,System of linear equations,http://dbpedia.org/resource/System_of_linear_equations,http://en.wikipedia.org/wiki/System_of_linear_equations,"In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. For example, is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by the ordered triple since it makes all three equations valid. The word ""system"" indicates that the equations are to be considered collectively, rather than individually. In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system. Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the ""best"" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.",2655870484741473745,related_concept,Nonlinear system,2024-06-23 21:17:09.728,['Computation'],"['Computation', 'Linear system']",set(),set(),0,3.2157894736842105,4.688741415852092e-304
488,948,Ordinary differential equation,http://dbpedia.org/resource/Ordinary_differential_equation,http://en.wikipedia.org/wiki/Ordinary_differential_equation,"In mathematics, an ordinary differential equation (ODE) is a differential equation whose unknown(s) consists of one (or more) function(s) of one variable and involves the derivatives of those functions. The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable.",5090834490128934901,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],"['Ordinary differential equation', 'Differential equation']",set(),set(),0,3.4775086505190314,4.639159590380874e-304
489,949,Root-finding algorithm,http://dbpedia.org/resource/Root-finding_algorithm,http://en.wikipedia.org/wiki/Root-finding_algorithm,,6707620764377056926,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],"[""Newton's method""]",set(),set(),0,1.7627118644067796,0.5287023924774997
490,950,Systems of polynomial equations,http://dbpedia.org/resource/Systems_of_polynomial_equations,http://en.wikipedia.org/wiki/Systems_of_polynomial_equations,,5905120463175640547,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],"[""Newton's method""]",set(),set(),0,0.375,0.4102049306522427
491,951,Partial differential equation,http://dbpedia.org/resource/Partial_differential_equation,http://en.wikipedia.org/wiki/Partial_differential_equation,"In mathematics, a partial differential equation (PDE) is an equation which imposes relations between the various partial derivatives of a multivariable function. The function is often thought of as an ""unknown"" to be solved for, similarly to how x is thought of as an unknown number to be solved for in an algebraic equation like x2 − 3x + 2 = 0. However, it is usually impossible to write down explicit formulas for solutions of partial differential equations. There is, correspondingly, a vast amount of modern mathematical and scientific research on methods to numerically approximate solutions of certain partial differential equations using computers. Partial differential equations also occupy a large sector of pure mathematical research, in which the usual questions are, broadly speaking, on the identification of general qualitative features of solutions of various partial differential equations, such as existence, uniqueness, regularity, and stability. Among the many open questions are the existence and smoothness of solutions to the Navier–Stokes equations, named as one of the Millennium Prize Problems in 2000. Partial differential equations are ubiquitous in mathematically oriented scientific fields, such as physics and engineering. For instance, they are foundational in the modern scientific understanding of sound, heat, diffusion, electrostatics, electrodynamics, thermodynamics, fluid dynamics, elasticity, general relativity, and quantum mechanics (Schrödinger equation, Pauli equation, etc). They also arise from many purely mathematical considerations, such as differential geometry and the calculus of variations; among other notable applications, they are the fundamental tool in the proof of the Poincaré conjecture from geometric topology. Partly due to this variety of sources, there is a wide spectrum of different types of partial differential equations, and methods have been developed for dealing with many of the individual equations which arise. As such, it is usually acknowledged that there is no ""general theory"" of partial differential equations, with specialist knowledge being somewhat divided between several essentially distinct subfields. Ordinary differential equations form a subclass of partial differential equations, corresponding to functions of a single variable. Stochastic partial differential equations and nonlocal equations are, as of 2020, particularly widely studied extensions of the ""PDE"" notion. More classical topics, on which there is still much active research, include elliptic and parabolic partial differential equations, fluid mechanics, Boltzmann equations, and dispersive partial differential equations.",6669121845702218246,related_concept,Nonlinear system,2024-06-23 21:17:09.728,"['Ordinary differential equation', 'Partial differential equation', 'Stochastic']","['Partial differential equation', 'Ordinary differential equation', 'Stochastic', 'Computation', ""Euler's method"", 'Deep learning']",set(),set(),0,5.323170731707317,0.685639881414419
492,952,Linear combination,http://dbpedia.org/resource/Linear_combination,http://en.wikipedia.org/wiki/Linear_combination,"In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of x and y would be any expression of the form ax + by, where a and b are constants). The concept of linear combinations is central to linear algebra and related fields of mathematics.Most of this article deals with linear combinations in the context of a vector space over a field, with some generalizations given at the end of the article.",5672848376575057807,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.736434108527132,1.3579145656547362
493,953,Simultaneous equations,http://dbpedia.org/resource/Simultaneous_equations,http://en.wikipedia.org/wiki/Simultaneous_equations,,9035802529415375416,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],[],set(),set(),0,10.153846153846153,0.494786511252593
494,954,Randomness,http://dbpedia.org/resource/Randomness,http://en.wikipedia.org/wiki/Randomness,"In common usage, randomness is the apparent or actual lack of pattern or predictability in events. A random sequence of events, symbols or steps often has no order and does not follow an intelligible pattern or combination. Individual random events are, by definition, unpredictable, but if the probability distribution is known, the frequency of different outcomes over repeated events (or ""trials"") is predictable. For example, when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will tend to occur twice as often as 4. In this view, randomness is not haphazardness; it is a measure of uncertainty of an outcome. Randomness applies to concepts of chance, probability, and information entropy. The fields of mathematics, probability, and statistics use formal definitions of randomness. In statistics, a random variable is an assignment of a numerical value to each possible outcome of an event space. This association facilitates the identification and the calculation of probabilities of the events. Random variables can appear in random sequences. A random process is a sequence of random variables whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. These and other constructs are extremely useful in probability theory and the various applications of randomness. Randomness is most often used in statistics to signify well-defined statistical properties. Monte Carlo methods, which rely on random input (such as from random number generators or pseudorandom number generators), are important techniques in science, particularly in the field of computational science. By analogy, quasi-Monte Carlo methods use quasi-random number generators. Random selection, when narrowly associated with a simple random sample, is a method of selecting items (often called units) from a population where the probability of choosing a specific item is the proportion of those items in the population. For example, with a bowl containing just 10 red marbles and 90 blue marbles, a random selection mechanism would choose a red marble with probability 1/10. Note that a random selection mechanism that selected 10 marbles from this bowl would not necessarily result in 1 red and 9 blue. In situations where a population consists of items that are distinguishable, a random selection mechanism requires equal probabilities for any item to be chosen. That is, if the selection process is such that each member of a population, say research subjects, has the same probability of being chosen, then we can say the selection process is random. According to Ramsey theory, pure randomness is impossible, especially for large structures. Mathematician Theodore Motzkin suggested that ""while disorder is more probable in general, complete disorder is impossible"". Misunderstanding this can lead to numerous conspiracy theories. Cristian S. Calude stated that ""given the impossibility of true randomness, the effort is directed towards studying degrees of randomness"". It can be proven that there is infinite hierarchy (in terms of quality or strength) of forms of randomness.",8245109234487285933,related_concept,Nonlinear system,2024-06-23 21:17:09.728,"['Randomness', 'Random variable']","['Randomness', 'Random variable', 'Statistics', 'Algorithmic information theory', 'Algorithm', 'Computation', 'Mathematics']",{'Randomness'},set(),0,1.3528505392912173,4.588209578888149e-304
495,955,Linearization,http://dbpedia.org/resource/Linearization,http://en.wikipedia.org/wiki/Linearization,"In mathematics, linearization is finding the linear approximation to a function at a given point. The linear approximation of a function is the first order Taylor expansion around the point of interest. In the study of dynamical systems, linearization is a method for assessing the local stability of an equilibrium point of a system of nonlinear differential equations or discrete dynamical systems. This method is used in fields such as engineering, physics, economics, and ecology.",931483230266558691,related_concept,Nonlinear system,2024-06-23 21:17:09.728,[],['Linearization'],set(),set(),0,2.7547169811320753,1.129778214442907
496,956,Dynamical system,http://dbpedia.org/resource/Dynamical_system,http://en.wikipedia.org/wiki/Dynamical_system,"In mathematics, a dynamical system is a system in which a function describes the time dependence of a point in an ambient space. Examples include the mathematical models that describe the swinging of a clock pendulum, the flow of water in a pipe, the random motion of particles in the air, and the number of fish each springtime in a lake. The most general definition unifies several concepts in mathematics such as ordinary differential equations and ergodic theory by allowing different choices of the space and how time is measured. Time can be measured by integers, by real or complex numbers or can be a more general algebraic object, losing the memory of its physical origin, and the space may be a manifold or simply a set, without the need of a smooth space-time structure defined on it. At any given time, a dynamical system has a state representing a point in an appropriate state space. This state is often given by a tuple of real numbers or by a vector in a geometrical manifold. The evolution rule of the dynamical system is a function that describes what future states follow from the current state. Often the function is deterministic, that is, for a given time interval only one future state follows from the current state. However, some systems are stochastic, in that random events also affect the evolution of the state variables. In physics, a dynamical system is described as a ""particle or ensemble of particles whose state varies over time and thus obeys differential equations involving time derivatives"". In order to make a prediction about the system's future behavior, an analytical solution of such equations or their integration over time through computer simulation is realized. The study of dynamical systems is the focus of dynamical systems theory, which has applications to a wide variety of fields such as mathematics, physics, biology, chemistry, engineering, economics, history, and medicine. Dynamical systems are a fundamental part of chaos theory, logistic map dynamics, bifurcation theory, the self-assembly and self-organization processes, and the edge of chaos concept.",7805773998990022666,related_concept,Nonlinear system,2024-06-23 21:17:09.728,['Dynamical system'],"['Dynamical system', 'Theorem', 'Hamiltonian system', 'Linear system', 'Chaos theory']",set(),set(),0,3.7613293051359515,4.549669821447682e-304
497,957,Nonlinear system,http://dbpedia.org/resource/Nonlinear_system,http://en.wikipedia.org/wiki/Nonlinear_system,"In mathematics and science, a nonlinear system is a system in which the change of the output is not proportional to the change of the input. Nonlinear problems are of interest to engineers, biologists, physicists, mathematicians, and many other scientists because most systems are inherently nonlinear in nature. Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems. Typically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one.In other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it. As nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos, and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology. Some authors use the term nonlinear science for the study of nonlinear systems. This term is disputed by others: Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals. — Stanisław Ulam",6549920031359767897,main_concept,,2024-06-23 21:17:09.728,[],"['Evolution', 'Machine learning', 'Evolutionary robotics', 'Genetic programming', 'Genetic algorithm', 'Partial differential equation', 'Ordinary differential equation', 'Root-finding algorithm', ""Newton's method""]",set(),set(),0,2.202054794520548,4.651020231015346e-304
498,963,Data set,http://dbpedia.org/resource/Data_set,http://en.wikipedia.org/wiki/Data_set,"A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set. Data sets can also consist of a collection of documents or files. In the open data discipline, data set is the unit to measure the information released in a public open data repository. The European data.europa.eu portal aggregates more than a million data sets. Some other issues (real-time data sources, non-relational data sets, etc.) increases the difficulty to reach a consensus about it.",1397925711118440602,related_concept,Arithmetic mean,2024-06-23 21:17:09.728,"['Data', 'Data set']","['Data', 'Data set', 'SPSS']",set(),set(),0,8.596153846153847,0.9711827503864028
499,965,Standard error (statistics),http://dbpedia.org/resource/Standard_error_(statistics),http://en.wikipedia.org/wiki/Standard_error_(statistics),,5312558764712007475,related_concept,Arithmetic mean,2024-06-23 21:17:09.728,[],"['Poisson distribution', 'Boot']",set(),set(),0,0.2722063037249284,0.5176892417627067
500,974,Fréchet mean,http://dbpedia.org/resource/Fréchet_mean,http://en.wikipedia.org/wiki/Fréchet_mean,"In mathematics and statistics, the Fréchet mean is a generalization of centroids to metric spaces, giving a single representative point or central tendency for a cluster of points. It is named after Maurice Fréchet. Karcher mean is the renaming of the Riemannian Center of Mass construction developed by Karsten Grove and Hermann Karcher. On the real numbers, the arithmetic mean, median, geometric mean, and harmonic mean can all be interpreted as Fréchet means for different distance functions.",4160294245049084193,related_concept,Arithmetic mean,2024-06-23 21:17:09.728,[],"['Fréchet mean', 'Euclidean distance']",set(),set(),0,1.1153846153846154,0.7265121296849273
501,979,Enterprise search,http://dbpedia.org/resource/Enterprise_search,http://en.wikipedia.org/wiki/Enterprise_search,"Enterprise search is the practice of making content from multiple enterprise-type sources, such as databases and intranets, searchable to a defined audience. ""Enterprise search"" is used to describe the software of search information within an enterprise (though the search function and its results may still be public). Enterprise search can be contrasted with web search, which applies search technology to documents on the open web, and desktop search, which applies search technology to the content on a single computer. Enterprise search systems index data and documents from a variety of sources such as: file systems, intranets, document management systems, e-mail, and databases. Many enterprise search systems integrate structured and unstructured data in their collections. Enterprise search systems also use access controls to enforce a security policy on their users. Enterprise search can be seen as a type of vertical search of an enterprise.",8145761744607129960,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Enterprise search'],['Enterprise search'],set(),set(),0,4.0285714285714285,0.5614816050026551
502,980,Probabilistic relevance model (BM25),http://dbpedia.org/resource/Probabilistic_relevance_model_(BM25),http://en.wikipedia.org/wiki/Probabilistic_relevance_model_(BM25),,4926310810391695325,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.21052631578947367,0.5515352276713
503,981,Geographic information retrieval,http://dbpedia.org/resource/Geographic_information_retrieval,http://en.wikipedia.org/wiki/Geographic_information_retrieval,"Geographic information retrieval (GIR) or geographical information retrieval systems are search tools for searching the Web, enterprise documents, and mobile local search that combine traditional text-based queries with location querying, such as a map or placenames. Like traditional information retrieval systems, GIR systems index text and information from structured and unstructured documents, and also augment those indices with geographic information. The development and engineering of GIR systems aims to build systems that can reliably answer queries that include a geographic dimension, such as ""What wars were fought in Greece?"" or ""restaurants in Beirut"". Semantic similarity and word-sense disambiguation are important components of GIR. To identify place names, GIR systems often rely on natural language processing or other metadata to associate text documents with locations. Such georeferencing, geotagging, and geoparsing tools often need databases of location names, known as gazetteers.",4926417646774285797,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Geographic information retrieval'],['Geographic information retrieval'],set(),set(),0,0.45614035087719296,3.264414921511863e-304
504,983,Cross-language information retrieval,http://dbpedia.org/resource/Cross-language_information_retrieval,http://en.wikipedia.org/wiki/Cross-language_information_retrieval,"Cross-language information retrieval (CLIR) is a subfield of information retrieval dealing with retrieving information written in a language different from the language of the user's query. The term ""cross-language information retrieval"" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term ""multilingual information retrieval"" refers more generally both to technology for retrieval of multilingual collections and to technology which has been moved to handle material in one language to another. The term Multilingual Information Retrieval (MLIR) involves the study of systems that accept queries for information in various languages and return objects (text, and other media) of various languages, translated into the user's language. Cross-language information retrieval refers more specifically to the use case where users formulate their information need in one language and the system retrieves relevant documents in another. To do so, most CLIR systems use various translation techniques. CLIR techniques can be classified into different categories based on different translation resources: 
* Dictionary-based CLIR techniques 
* Parallel corpora based CLIR techniques 
* Comparable corpora based CLIR techniques 
* Machine translator based CLIR techniques CLIR systems have improved so much that the most accurate multi-lingual and cross-lingual systems today are nearly as effective as monolingual systems. Other related information access tasks, such as media monitoring, information filtering and routing, sentiment analysis, and information extraction require more sophisticated models and typically more processing and analysis of the information items of interest. Much of that processing needs to be aware of the specifics of the target languages it is deployed in. Mostly, the various mechanisms of variation in human language pose coverage challenges for information retrieval systems: texts in a collection may treat a topic of interest but use terms or expressions which do not match the expression of information need given by the user. This can be true even in a mono-lingual case, but this is especially true in cross-lingual information retrieval, where users may know the target language only to some extent. The benefits of CLIR technology for users with poor to moderate competence in the target language has been found to be greater than for those who are fluent. Specific technologies in place for CLIR services include morphological analysis to handle inflection, decompounding or compound splitting to handle compound terms, and translations mechanisms to translate a query from one language to another. The first workshop on CLIR was held in Zürich during the SIGIR-96 conference. Workshops have been held yearly since 2000 at the meetings of the Cross Language Evaluation Forum (CLEF). Researchers also convene at the annual Text Retrieval Conference (TREC) to discuss their findings regarding different systems and methods of information retrieval, and the conference has served as a point of reference for the CLIR subfield. Google Search had a cross-language search feature that was removed in 2013.",6603989819843995161,related_concept,Information retrieval,2024-06-23 21:17:09.728,"['Google Search', 'Cross-language information retrieval']","['Cross-language information retrieval', 'Google Search']",set(),set(),0,3.608695652173913,0.5524910076136978
505,984,Probabilistic relevance model,http://dbpedia.org/resource/Probabilistic_relevance_model,http://en.wikipedia.org/wiki/Probabilistic_relevance_model,"The probabilistic relevance model was devised by Stephen E. Robertson and Karen Spärck Jones as a framework for probabilistic models to come. It is a formalism of information retrieval useful to derive ranking functions used by search engines and web search engines in order to rank matching documents according to their relevance to a given search query. It is a theoretical model estimating the probability that a document dj is relevant to a query q. The model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query q. Such an ideal answer set is called R and should maximize the overall probability of relevance to that user. The prediction is that documents in this set R are relevant to the query, while documents not present in the set are non-relevant.",7270551608020761947,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.1666666666666665,0.7365066097273459
506,985,Music information retrieval,http://dbpedia.org/resource/Music_information_retrieval,http://en.wikipedia.org/wiki/Music_information_retrieval,"Music information retrieval (MIR) is the interdisciplinary science of retrieving information from music. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in academic musicology, psychoacoustics, psychology, signal processing, informatics, machine learning, optical music recognition, computational intelligence or some combination of these.",3398697439501405791,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Music information retrieval'],['Music information retrieval'],set(),set(),0,0.9252336448598131,0.409354368895164
507,986,Uncertain inference,http://dbpedia.org/resource/Uncertain_inference,http://en.wikipedia.org/wiki/Uncertain_inference,Uncertain inference was first described by C. J. van Rijsbergen as a way to formally define a query and document relationship in Information retrieval. This formalization is a logical implication with an attached measure of uncertainty.,4999339484877347458,related_concept,Information retrieval,2024-06-23 21:17:09.728,"['Uncertain inference', 'Information retrieval']","['Uncertain inference', 'Information retrieval']",{'Inference'},set(),0,1.25,1.365876568076309
508,987,Standard Boolean model,http://dbpedia.org/resource/Standard_Boolean_model,http://en.wikipedia.org/wiki/Standard_Boolean_model,,3529985009701168374,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],['Bayesian epistemology'],set(),set(),0,1.0833333333333333,0.23939557342612283
509,988,Legal information retrieval,http://dbpedia.org/resource/Legal_information_retrieval,http://en.wikipedia.org/wiki/Legal_information_retrieval,"Legal information retrieval is the science of information retrieval applied to legal text, including legislation, case law, and scholarly works. Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means. Legal information retrieval is a part of the growing field of legal informatics. In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used boolean search methods (exact matches of specified terms) on full text legal documents have been shown to have an average recall rate as low as 20 percent, meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents. This may result in failing to retrieve important or precedential cases. In some jurisdictions this may be especially problematic, as legal professionals are ethically obligated to be reasonably informed as to relevant legal documents. Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high recall rate) and reducing the number of irrelevant documents (a high precision rate). This is a difficult task, as the legal field is prone to jargon, polysemes (words that have different meanings when used in a legal context), and constant change. Techniques used to achieve these goals generally fall into three categories: boolean retrieval, manual classification of legal text, and natural language processing of legal text.",8065654804722448982,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Legal information retrieval'],['Legal information retrieval'],set(),set(),0,1.0689655172413792,0.5299495897732135
510,989,3D retrieval,http://dbpedia.org/resource/3D_retrieval,http://en.wikipedia.org/wiki/3D_retrieval,,4017976373958378216,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],['3D retrieval'],set(),set(),0,0.1,0.4227984317664738
511,990,Information system,http://dbpedia.org/resource/Information_system,http://en.wikipedia.org/wiki/Information_system,"An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology. Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making. A computer information system is a system that is composed of people and computers that processes or interprets information. The term is also sometimes used to simply refer to a computer system with software installed. ""Information systems"" is also an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks. In many organizations, the department or unit responsible for information systems and data processing is known as ""information services"". Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes. Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes. Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information. As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action. Information systems are the primary focus of study for organizational informatics.",5551438769552969317,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Information system'],"['Information system', 'Data', 'Information technology', 'AI', 'Expert system']",set(),set(),0,1.9593679458239277,1.320334620989397
512,991,Learning to rank,http://dbpedia.org/resource/Learning_to_rank,http://en.wikipedia.org/wiki/Learning_to_rank,"Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.",7360169039066260135,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Learning to rank'],"['Learning to rank', 'Mathematics']",set(),set(),0,1.433179723502304,1.1593927830317803
513,993,Ranking (information retrieval),http://dbpedia.org/resource/Ranking_(information_retrieval),http://en.wikipedia.org/wiki/Ranking_(information_retrieval),"Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the ""best"" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results.",3077704392732863727,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],['PageRank'],set(),set(),0,2.3666666666666667,3.31949412929578e-304
514,994,Information science,http://dbpedia.org/resource/Information_science,http://en.wikipedia.org/wiki/Information_science,"Information science (also known as information studies) is an academic field which is primarily concerned with analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information. Practitioners within and outside the field study the application and the usage of knowledge in organizations in addition to the interaction between people, organizations, and any existing information systems with the aim of creating, replacing, improving, or understanding the information systems. Historically, information science (informatics) is associated with computer science, data science, psychology, technology, library science, healthcare, and intelligence agencies. However, information science also incorporates aspects of diverse fields such as archival science, cognitive science, commerce, law, linguistics, museology, management, mathematics, philosophy, public policy, and social sciences.",8571786080810311591,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Information science'],"['Information science', 'Classification', 'Information retrieval', 'Knowledge representation']",set(),set(),0,3.5447154471544717,1.280673491240804
515,995,Image retrieval,http://dbpedia.org/resource/Image_retrieval,http://en.wikipedia.org/wiki/Image_retrieval,"An image retrieval system is a computer system used for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning, keywords, title or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social web applications and the semantic web have inspired the development of several web-based image annotation tools. The first microcomputer-based image database retrieval system was developed at MIT, in the 1990s, by Banireddy Prasaad, Amar Gupta, Hoo-min Toong, and Stuart Madnick. A 2008 survey article documented progresses after 2007. All image retrieval systems as of 2021 were designed for 2D images, not 3D ones.",4073469098728822591,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1,0.6015907186120087
516,996,Information filtering,http://dbpedia.org/resource/Information_filtering,http://en.wikipedia.org/wiki/Information_filtering,,3914079053983051329,related_concept,Information retrieval,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.2926829268292683,0.34028747900302364
517,997,Full-text search,http://dbpedia.org/resource/Full-text_search,http://en.wikipedia.org/wiki/Full-text_search,"In text retrieval, full-text search refers to techniques for searching a single computer-stored document or a collection in a full-text database. Full-text search is distinguished from searches based on metadata or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references). In a full-text search, a search engine examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques became common in online bibliographic databases in the 1990s. Many websites and application programs (such as word processing software) provide full-text-search capabilities. Some web search engines, such as AltaVista, employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.",996968984497515138,related_concept,Information retrieval,2024-06-23 21:17:09.728,['Full-text search'],"['Full-text search', 'AI', 'PageRank', 'Search engine']",set(),set(),0,1.074766355140187,3.233120969285498e-304
518,999,PRESS statistic,http://dbpedia.org/resource/PRESS_statistic,http://en.wikipedia.org/wiki/PRESS_statistic,"In statistics, the predicted residual error sum of squares (PRESS) is a form of cross-validation used in regression analysis to provide a summary measure of the fit of a model to a sample of observations that were not themselves used to estimate the model. It is calculated as the sums of squares of the prediction residuals for those observations. A fitted model having been produced, each observation in turn is removed and the model is refitted using the remaining observations. The out-of-sample predicted value is calculated for the omitted observation in each case, and the PRESS statistic is calculated as the sum of the squares of all the resulting prediction errors: Given this procedure, the PRESS statistic can be calculated for a number of candidate model structures for the same dataset, with the lowest values of PRESS indicating the best structures. Models that are over-parameterised (over-fitted) would tend to give small residuals for observations included in the model-fitting but large residuals for observations that are excluded.PRESS statistic has been extensively used in Lazy Learning and locally linear learning to speed-up the assessment and the selection of the neighbourhood size.",8599359065026539286,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,['PRESS statistic'],['PRESS statistic'],set(),set(),0,1.2,1.394244397145879
519,1001,Feature selection,http://dbpedia.org/resource/Feature_selection,http://en.wikipedia.org/wiki/Feature_selection,"In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: 
* simplification of models to make them easier to interpret by researchers/users, 
* shorter training times, 
* to avoid the curse of dimensionality, 
* improve data's compatibility with a learning model class, 
* encode inherent symmetries present in the input space. The central premise when using a feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant and irrelevant are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated. Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples.",4320678130485632006,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,"['Feature extraction', 'DNA microarray', 'Feature selection']","['DNA microarray', 'Feature selection', 'Feature extraction', ""Mallows's Cp"", 'Akaike information criterion', 'AI']",set(),set(),0,0.8367346938775511,1.390105742466923
520,1005,Statistical model validation,http://dbpedia.org/resource/Statistical_model_validation,http://en.wikipedia.org/wiki/Statistical_model_validation,"In statistics, model validation is the task of evaluating whether a chosen statistical model is appropriate or not. Oftentimes in statistical inference, inferences from models that appear to fit their data may be flukes, resulting in a misunderstanding by researchers of the actual relevance of their model. To combat this, model validation is used to test whether a statistical model can hold up to permutations in the data. This topic is not to be confused with the closely related task of model selection, the process of discriminating between multiple candidate models: model validation does not concern so much the conceptual design of models as it tests only the consistency between a chosen model and its stated outputs. There are many ways to validate a model. Residual plots plot the difference between the actual data and the model's predictions: correlations in the residual plots may indicate a flaw in the model. Cross validation is a method of model validation that iteratively refits the model, each time leaving out just a small sample and comparing whether the samples left out are predicted by the model: there are many kinds of cross validation. Predictive simulation is used to compare simulated data to actual data. External validation involves fitting the model to new data. Akaike information criterion estimates the quality of a model.",2662785730515326918,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,['Akaike information criterion'],['Akaike information criterion'],set(),set(),0,0.8571428571428571,1.3948671060209747
521,1006,Stationary bootstrap,http://dbpedia.org/resource/Stationary_bootstrap,http://en.wikipedia.org/wiki/Stationary_bootstrap,,6873244126949254369,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,[],"['Boot', 'Bootstrapping', 'Poisson distribution', 'Bayesian estimator', 'Histogram', 'Bootstrap aggregating', 'U-statistic']",set(),set(),0,0.004796163069544364,0.3549834760618809
522,1010,Resampling (statistics),http://dbpedia.org/resource/Resampling_(statistics),http://en.wikipedia.org/wiki/Resampling_(statistics),"In statistics, resampling is the creation of new samples based on one observed sample.Resampling methods are: 1. 
* Permutation tests (also re-randomization tests) 2. 
* Bootstrapping 3. 
* Cross validation",1371355746545270143,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,"['Boot', 'Bootstrapping']","['Boot', 'Bootstrapping']",set(),set(),0,1.7843137254901962,5.983029108064641e-304
523,1013,Hyperparameter (machine learning),http://dbpedia.org/resource/Hyperparameter_(machine_learning),http://en.wikipedia.org/wiki/Hyperparameter_(machine_learning),"In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training. Hyperparameters can be classified as model hyperparameters, that cannot be inferred while fitting the machine to the training set because they refer to the model selection task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process. An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and batch size as well as mini-batch size. Batch size can refer to the full data sample where mini-batch size would be a smaller sample set. Different model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, LASSO is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm.",692441227210889006,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,['Hyperparameter'],"['Hyperparameter', 'Gradient']",set(),set(),0,0.27647058823529413,1.3150378489404773
524,1018,Predictive modelling,http://dbpedia.org/resource/Predictive_modelling,http://en.wikipedia.org/wiki/Predictive_modelling,"Predictive modelling uses statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place. In many cases the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example given an email determining how likely that it is spam. Models can use one or more classifiers in trying to determine the probability of a set of data belonging to another set. For example, a model might be used to determine whether an email is spam or ""ham"" (non-spam). Depending on definitional boundaries, predictive modelling is synonymous with, or largely overlapping with, the field of machine learning, as it is more commonly referred to in academic or research and development contexts. When deployed commercially, predictive modelling is often referred to as predictive analytics. Predictive modelling is often contrasted with causal modelling/analysis. In the former, one may be entirely satisfied to make use of indicators of, or proxies for, the outcome of interest. In the latter, one seeks to determine true cause-and-effect relationships. This distinction has given rise to a burgeoning literature in the fields of research methods and statistics and to the common statement that ""correlation does not imply causation"".",406438903839487785,related_concept,Cross-validation (statistics),2024-06-23 21:17:09.728,['Predictive modelling'],"['Predictive modelling', 'Parametric model']",set(),set(),0,2.2989690721649483,1.4048827472778462
525,1024,Mean absolute difference,http://dbpedia.org/resource/Mean_absolute_difference,http://en.wikipedia.org/wiki/Mean_absolute_difference,"The mean absolute difference (univariate) is a measure of statistical dispersion equal to the average absolute difference of two independent values drawn from a probability distribution. A related statistic is the , which is the mean absolute difference divided by the arithmetic mean, and equal to twice the Gini coefficient.The mean absolute difference is also known as the absolute mean difference (not to be confused with the absolute value of the mean signed difference) and the Gini mean difference (GMD). The mean absolute difference is sometimes denoted by Δ or as MD.",8880971042786036424,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,[],"['Confidence interval', 'Bernoulli distribution']",set(),set(),0,1.2,4.132966903936508e-304
526,1028,Measurement uncertainty,http://dbpedia.org/resource/Measurement_uncertainty,http://en.wikipedia.org/wiki/Measurement_uncertainty,"In metrology, measurement uncertainty is the expression of the statistical dispersion of the values attributed to a measured quantity. All measurements are subject to uncertainty and a measurement result is complete only when it is accompanied by a statement of the associated uncertainty, such as the standard deviation. By international agreement, this uncertainty has a probabilistic basis and reflects incomplete knowledge of the quantity value. It is a non-negative parameter. The measurement uncertainty is often taken as the standard deviation of a state-of-knowledge probability distribution over the possible values that could be attributed to a measured quantity. Relative uncertainty is the measurement uncertainty relative to the magnitude of a particular single choice for the value for the measured quantity, when this choice is nonzero. This particular single choice is usually called the measured value, which may be optimal in some well-defined sense (e.g., a mean, median, or mode). Thus, the relative measurement uncertainty is the measurement uncertainty divided by the absolute value of the measured value, when the measured value is not zero.",6278498214483207140,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,[],['Measurement uncertainty'],set(),set(),0,3.029850746268657,1.3739083832415633
527,1029,Coefficient of variation,http://dbpedia.org/resource/Coefficient_of_variation,http://en.wikipedia.org/wiki/Coefficient_of_variation,"In probability theory and statistics, the coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. It is often expressed as a percentage, and is defined as the ratio of the standard deviation to the mean (or its absolute value, ). The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay. It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R, by economists and investors in economic models, and in neuroscience.",3962483690128537337,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,['ANOVA'],"['ANOVA', 'Mean', 'Statistical inference', 'Normalization (statistics)']",set(),set(),0,1.8188976377952757,1.1536684966457709
528,1032,Summary statistics,http://dbpedia.org/resource/Summary_statistics,http://en.wikipedia.org/wiki/Summary_statistics,"In descriptive statistics, summary statistics are used to summarize a set of observations, in order to communicate the largest amount of information as simply as possible. Statisticians commonly try to describe the observations in 
* a measure of location, or central tendency, such as the arithmetic mean 
* a measure of statistical dispersion like the standard mean absolute deviation 
* a measure of the shape of the distribution like skewness or kurtosis 
* if more than one variable is measured, a measure of statistical dependence such as a correlation coefficient A common collection of order statistics used as summary statistics are the five-number summary, sometimes extended to a seven-number summary, and the associated box plot. Entries in an analysis of variance table can also be regarded as summary statistics.",4621131814866019935,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.24477611940298508,1.3367171774912603
529,1034,Quartile coefficient of dispersion,http://dbpedia.org/resource/Quartile_coefficient_of_dispersion,http://en.wikipedia.org/wiki/Quartile_coefficient_of_dispersion,"In statistics, the quartile coefficient of dispersion is a descriptive statistic which measures dispersion and which is used to make comparisons within and between data sets. Since it is based on quantile information, it is less sensitive to outliers than measures such as the Coefficient of variation. As such, it is one of several Robust measures of scale. The statistic is easily computed using the first (Q1) and third (Q3) quartiles for each data set. The quartile coefficient of dispersion is:",5287757940144099378,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,"['Coefficient of variation', 'Robust measures of scale']",[],set(),set(),0,1.8181818181818181,4.271711481030518e-304
530,1035,Average absolute deviation,http://dbpedia.org/resource/Average_absolute_deviation,http://en.wikipedia.org/wiki/Average_absolute_deviation,"The average absolute deviation (AAD) of a data set is the average of the absolute deviations from a central point. It is a summary statistic of statistical dispersion or variability. In the general form, the central point can be a mean, median, mode, or the result of any other measure of central tendency or any reference value related to the given data set. AAD includes the mean absolute deviation and the median absolute deviation (both abbreviated as MAD).",6466725563382417651,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,[],['Average absolute deviation'],set(),set(),0,1.5907780979827089,0.6428882064717082
531,1037,Qualitative variation,http://dbpedia.org/resource/Qualitative_variation,http://en.wikipedia.org/wiki/Qualitative_variation,"An index of qualitative variation (IQV) is a measure of statistical dispersion in nominal distributions. There are a variety of these, but they have been relatively little-studied in the statistics literature. The simplest is the variation ratio, while more complex indices include the information entropy.",4898370671102762944,related_concept,Statistical dispersion,2024-06-23 21:17:09.728,[],"['Manhattan distance', 'ANOVA']",set(),set(),0,0.30097087378640774,0.9983158995145633
532,1041,Count data,http://dbpedia.org/resource/Count_data,http://en.wikipedia.org/wiki/Count_data,"In statistics, count data is a statistical data type describing countable quantities, data which can take only the counting numbers, non-negative integer values {0, 1, 2, 3, ...}, and where these integers arise from counting rather than ranking. The statistical treatment of count data is distinct from that of binary data, in which the observations can take only two values, usually represented by 0 and 1, and from ordinal data, which may also consist of integers but where the individual values fall on an arbitrary scale and only the relative ranking is important.",4680646592886605621,related_concept,Poisson regression,2024-06-23 21:17:09.728,[],['Poisson distribution'],set(),set(),0,11.777777777777779,1.3876229431504572
533,1043,Quasi-likelihood,http://dbpedia.org/resource/Quasi-likelihood,http://en.wikipedia.org/wiki/Quasi-likelihood,"In statistics, quasi-likelihood methods are used to estimate parameters in a statistical model when exact likelihood methods, for example maximum likelihood estimation, are computationally infeasible. Due to the wrong likelihood being used, quasi-likelihood estimators lose asymptotic efficiency compared to, e.g., maximum likelihood estimators. Under broadly applicable conditions, quasi-likelihood estimators are consistent and asymptotically normal. The asymptotic covariance matrix can be obtained using the so-called . Examples of quasi-likelihood methods are the generalized estimating equations and pairwise likelihood approaches.",7806180623116079271,related_concept,Poisson regression,2024-06-23 21:17:09.728,[],['Quasi-likelihood'],set(),set(),0,1.0,1.355256610463359
534,1052,Log-linear model,http://dbpedia.org/resource/Log-linear_model,http://en.wikipedia.org/wiki/Log-linear_model,"A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form , in which the fi(X) are quantities that are functions of the variable X, in general a vector of values, while c and the wi stand for the model parameters. The term may specifically be used for: 
* A log-linear plot or graph, which is a type of semi-log plot. 
* Poisson regression for contingency tables, a type of generalized linear model. The specific applications of log-linear models are where the output quantity lies in the range 0 to ∞, for values of the independent variables X, or more immediately, the transformed quantities fi(X) in the range −∞ to +∞. This may be contrasted to logistic models, similar to the logistic function, for which the output quantity lies in the range 0 to 1. Thus the contexts where these models are useful or realistic often depends on the range of the values being modelled.",7163208835134715341,related_concept,Poisson regression,2024-06-23 21:17:09.728,['Poisson regression'],[],set(),set(),0,1.8333333333333333,1.3308367056055128
535,1054,Negative binomial distribution,http://dbpedia.org/resource/Negative_binomial_distribution,http://en.wikipedia.org/wiki/Negative_binomial_distribution,"In probability theory and statistics, the negative binomial distribution is a discrete probability distribution that models the number of failures in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of successes (denoted ) occurs. For example, we can define rolling a 6 on a die as a success, and rolling any other number as a failure, and ask how many failure rolls will occur before we see the third success. In such a case, the probability distribution of the number of failures that appear will be a negative binomial distribution. An alternative formulation is to model the number of total trials (instead of the number of failures). In fact, for a specified (non-random) number of successes (r), the number of failures (n - r) are random because the total trials (n) are random. For example, we could use the negative binomial distribution to model the number of days n (random) a certain machine works (specified by r) before it breaks down. The Pascal distribution (after Blaise Pascal) and Polya distribution (for George Pólya) are special cases of the negative binomial distribution. A convention among engineers, climatologists, and others is to use ""negative binomial"" or ""Pascal"" for the case of an integer-valued stopping-time parameter and use ""Polya"" for the real-valued case. For occurrences of associated discrete events, like tornado outbreaks, the Polya distributions can be used to give more accurate models than the Poisson distribution by allowing the mean and variance to be different, unlike the Poisson. The negative binomial distribution has a variance , with the distribution becoming identical to Poisson in the limit for a given mean . This can make the distribution a useful overdispersed alternative to the Poisson distribution, for example for a robust modification of Poisson regression. In epidemiology, it has been used to model disease transmission for infectious diseases where the likely number of onward infections may vary considerably from individual to individual and from setting to setting. More generally, it may be appropriate where events have positively correlated occurrences causing a larger variance than if the occurrences were independent, due to a positive covariance term. The term ""negative binomial"" is likely due to the fact that a certain binomial coefficient that appears in the formula for the probability mass function of the distribution can be written more simply with negative numbers.",5124547052983653642,related_concept,Poisson regression,2024-06-23 21:17:09.728,"['Poisson distribution', 'Poisson regression']","['Poisson distribution', 'Poisson regression', ""Newton's method"", 'Probability', 'Probability distribution']",set(),set(),0,1.453968253968254,1.3147227720695651
536,1055,Iteratively reweighted least squares,http://dbpedia.org/resource/Iteratively_reweighted_least_squares,http://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares,"The method of iteratively reweighted least squares (IRLS) is used to solve certain optimization problems with objective functions of the form of a p-norm: by an iterative method in which each step involves solving a weighted least squares problem of the form: IRLS is used to find the maximum likelihood estimates of a generalized linear model, and in robust regression to find an M-estimator, as a way of mitigating the influence of outliers in an otherwise normally-distributed data set. For example, by minimizing the least absolute errors rather than the least square errors. One of the advantages of IRLS over linear programming and convex programming is that it can be used with Gauss–Newton and Levenberg–Marquardt numerical algorithms.",6804438658320936983,related_concept,Poisson regression,2024-06-23 21:17:09.728,['M-estimator'],['M-estimator'],set(),set(),0,1.793103448275862,1.0758980192573508
537,1056,Zero-inflated model,http://dbpedia.org/resource/Zero-inflated_model,http://en.wikipedia.org/wiki/Zero-inflated_model,"In statistics, a zero-inflated model is a statistical model based on a zero-inflated probability distribution, i.e. a distribution that allows for frequent zero-valued observations.",4679325934941529238,related_concept,Poisson regression,2024-06-23 21:17:09.728,[],"['Count data', 'Zero-inflated model', 'Poisson distribution', 'Poisson regression', 'Data']",set(),set(),0,0.08943089430894309,1.3339633166354303
538,1057,Ridge regression,http://dbpedia.org/resource/Ridge_regression,http://en.wikipedia.org/wiki/Ridge_regression,"Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and engineering. Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems. it is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters. In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff). The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers “RIDGE regressions: biased estimation of nonorthogonal problems” and “RIDGE regressions: applications in nonorthogonal problems”. This was the result of ten years of research into the field of ridge analysis. Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables—by creating a ridge regression estimator (RR). This provides a more precise ridge parameters estimate, as its variance and mean square estimator are often smaller than the least square estimators previously derived.",8617725388504093679,related_concept,Poisson regression,2024-06-23 21:17:09.728,['Ridge regression'],"['Ridge regression', 'Kriging', 'Ordinary least squares', ""Bayes' theorem""]",set(),set(),0,0.46,4.933665023018659e-304
539,1058,Fixed-effect Poisson model,http://dbpedia.org/resource/Fixed-effect_Poisson_model,http://en.wikipedia.org/wiki/Fixed-effect_Poisson_model,"In statistics, fixed-effect Poisson models are used for static panel data when the outcome variable is count data. Hausman, Hall, and Griliches pioneered the method in the mid 1980s. Their outcome of interest was the number of patents filed by firms, where they wanted to develop methods to control for the firm fixed effects. Linear panel data models use the linear additivity of the fixed effects to difference them out and circumvent the . Even though Poisson models are inherently nonlinear, the use of the linear index and the exponential link function lead to multiplicative separability, more specifically E[yit ∨ xi1... xiT, ci ] = m(xit, ci, b0 ) = exp(ci + xit b0 ) = ai exp(xit b0 ) = μti (1) This formula looks very similar to the standard Poisson premultiplied by the term ai. As the conditioning set includes the observables over all periods, we are in the static panel data world and are imposing . Hausman, Hall, and Griliches then use Andersen's conditional Maximum Likelihood methodology to estimate b0. Using ni = Σ yit allows them to obtain the following nice distributional result of yi yi ∨ ni, xi, ci ∼ Multinomial (ni, p1 (xi, b0), ..., pT (xi, b0 )) (2) where At this point, the estimation of the fixed-effect Poisson model is transformed in a useful way and can be estimated by maximum-likelihood estimation techniques for multinomial log likelihoods. This is computationally not necessarily very restrictive, but the distributional assumptions up to this point are fairly stringent. Wooldridge provided evidence that these models have nice robustness properties as long as the conditional mean assumption (i.e. equation 1) holds. Chamberlain also provided for these estimators under slightly weaker exogeneity assumptions. However, these bounds are practically difficult to attain, as the proposed methodology needs high-dimensional nonparametric regressions for attaining these bounds.",6351892707068813102,related_concept,Poisson regression,2024-06-23 21:17:09.728,[],['Poisson regression'],set(),set(),0,0.8461538461538461,1.3722830695686534
540,1060,Poisson distribution,http://dbpedia.org/resource/Poisson_distribution,http://en.wikipedia.org/wiki/Poisson_distribution,"In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. It is named after French mathematician Siméon Denis Poisson (/ˈpwɑːsɒn/; French pronunciation: ​[pwasɔ̃]). The Poisson distribution can also be used for the number of events in other specified interval types such as distance, area, or volume. For instance, a call center receives an average of 180 calls per hour, 24 hours a day. The calls are independent; receiving one does not change the probability of when the next one will arrive. The number of calls received during any minute has a Poisson probability distribution with mean 3: the most likely numbers are 2 and 3 but 1 and 4 are also likely and there is a small probability of it being as low as zero and a very small probability it could be 10. Another example is the number of decay events that occur from a radioactive source during a defined observation period.",2558302256691409464,related_concept,Poisson regression,2024-06-23 21:17:09.728,['Poisson distribution'],"['Poisson distribution', 'Kullback–Leibler divergence', 'Probability', 'Bayesian inference', 'Poisson regression', 'MATLAB']",set(),set(),0,2.471563981042654,0.8709052992197199
541,1066,Vector quantization,http://dbpedia.org/resource/Vector_quantization,http://en.wikipedia.org/wiki/Vector_quantization,"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms. The density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensional data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation. Vector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.",365309459450831503,related_concept,K-means clustering,2024-06-23 21:17:09.728,['Vector quantization'],"['Vector quantization', 'Simulated annealing', 'Mean']",set(),set(),0,1.2333333333333334,1.3557564252676646
542,1068,Mean shift,http://dbpedia.org/resource/Mean_shift,http://en.wikipedia.org/wiki/Mean_shift,"Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.",8590341461976611560,related_concept,K-means clustering,2024-06-23 21:17:09.728,"['Mean shift', 'Mean']","['Mean shift', 'Mean', 'Expectation–maximization algorithm']",set(),set(),0,1.7549668874172186,0.877741702437022
543,1069,K-medians clustering,http://dbpedia.org/resource/K-medians_clustering,http://en.wikipedia.org/wiki/K-medians_clustering,"In statistics, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the squared 2-norm distance metric (which k-means does.) This relates directly to the k-median problem with respect to the 1-norm, which is the problem of finding k centers such that the clusters formed by them are the most compact. Formally, given a set of data points x, the k centers ci are to be chosen so as to minimize the sum of the distances from each x to the nearest ci. The criterion function formulated in this way is sometimes a better criterion than that used in the k-means clustering algorithm, in which the sum of the squared distances is used. The sum of distances is widely used in applications such as the facility location problem. The proposed algorithm uses Lloyd-style iteration which alternates between an expectation (E) and maximization (M) step, making this an expectation–maximization algorithm. In the E step, all objects are assigned to their nearest median. In the M step, the medians are recomputed by using the median in each single dimension.",9079241272716174300,related_concept,K-means clustering,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.55,3.315077335491957e-304
544,1079,K q-flats,http://dbpedia.org/resource/K_q-flats,http://en.wikipedia.org/wiki/K_q-flats,"In data mining and machine learning, -flats algorithm is an iterative method which aims to partition observations into clusters where each cluster is close to a -flat, where is a given integer. It is a generalization of the -means algorithm. In -means algorithm,clusters are formed in the way that each cluster is close to one point, which is a -flat. -flats algorithm gives better clustering result than -means algorithmfor some data set.",7738161504895277855,related_concept,K-means clustering,2024-06-23 21:17:09.728,[],"[""Lloyd's algorithm"", 'Classification']",set(),set(),0,3.5625,3.3690391768724316e-304
545,1083,Truncated mean,http://dbpedia.org/resource/Truncated_mean,http://en.wikipedia.org/wiki/Truncated_mean,"A truncated mean or trimmed mean is a statistical measure of central tendency, much like the mean and median. It involves the calculation of the mean after discarding given parts of a probability distribution or sample at the high and low end, and typically discarding an equal amount of both. This number of points to be discarded is usually given as a percentage of the total number of points, but may also be given as a fixed number of points. For most statistical applications, 5 to 25 percent of the ends are discarded. For example, given a set of 8 points, trimming by 12.5% would discard the minimum and maximum value in the sample: the smallest and largest values, and would compute the mean of the remaining 6 points. The 25% trimmed mean (when the lowest 25% and the highest 25% are discarded) is known as the interquartile mean. The median can be regarded as a fully truncated mean and is most robust. As with other trimmed estimators, the main advantage of the trimmed mean is robustness and higher efficiency for mixed distributions and heavy-tailed distribution (like the Cauchy distribution), at the cost of lower efficiency for some other less heavily tailed distributions (such as the normal distribution). For intermediate distributions the differences between the efficiency of the mean and the median are not very big, e.g. for the student-t distribution with 2 degrees of freedom the variances for mean and median are nearly equal.",1111533965860117624,related_concept,Central tendency,2024-06-23 21:17:09.728,['Cauchy distribution'],"['Cauchy distribution', 'Winsorized mean', ""Student's t-test""]",set(),set(),0,1.2564102564102564,4.073610798509893e-304
546,1085,Central moment,http://dbpedia.org/resource/Central_moment,http://en.wikipedia.org/wiki/Central_moment,"In probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable's mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterized. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location. Sets of central moments can be defined for both univariate and multivariate distributions.",9099924551923346945,related_concept,Central tendency,2024-06-23 21:17:09.728,['Central moment'],"['Central moment', 'Cauchy distribution']",set(),set(),0,2.6739130434782608,1.3170436101261114
547,1086,Midrange,http://dbpedia.org/resource/Midrange,http://en.wikipedia.org/wiki/Midrange,,296395542917317640,related_concept,Central tendency,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.23076923076923078,0.551181594312963
548,1088,Mode (statistics),http://dbpedia.org/resource/Mode_(statistics),http://en.wikipedia.org/wiki/Mode_(statistics),"The mode is the value that appears most often in a set of data values. If X is a discrete random variable, the mode is the value x (i.e, X = x) at which the probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled. Like the statistical mean and median, the mode is a way of expressing, in a (usually) single number, important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions. The mode is not necessarily unique to a given discrete distribution, since the probability mass function may take the same maximum value at several points x1, x2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently. When the probability density function of a continuous distribution has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal (as opposed to unimodal). A mode of a continuous probability distribution is often considered to be any value x at which its probability density function has a locally maximum value, so any peak is a mode. In symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. For samples, if it is known that they are drawn from a symmetric unimodal distribution, the sample mean can be used as an estimate of the population mode.",4518694316619914523,related_concept,Central tendency,2024-06-23 21:17:09.728,[],['MATLAB'],set(),set(),0,2.790633608815427,1.315591221519686
549,1089,Deviation (statistics),http://dbpedia.org/resource/Deviation_(statistics),http://en.wikipedia.org/wiki/Deviation_(statistics),"In mathematics and statistics, deviation is a measure of difference between the observed value of a variable and some other value, often that variable's mean. The sign of the deviation reports the direction of that difference (the deviation is positive when the observed value exceeds the reference value). The magnitude of the value indicates the size of the difference.",7078117349982464153,related_concept,Central tendency,2024-06-23 21:17:09.728,[],"['Summary statistics', 'Variance', 'Mean', 'Statistics', 'Studentized residual']",set(),set(),0,1.8222222222222222,1.2745801176681963
550,1090,Winsorized mean,http://dbpedia.org/resource/Winsorized_mean,http://en.wikipedia.org/wiki/Winsorized_mean,"A winsorized mean is a winsorized statistical measure of central tendency, much like the mean and median, and even more similar to the truncated mean. It involves the calculation of the mean after winsorizing -- replacing given parts of a probability distribution or sample at the high and low end with the most extreme remaining values, typically doing so for an equal amount of both extremes; often 10 to 25 percent of the ends are replaced. The winsorized mean can equivalently be expressed as a weighted average of the truncated mean and the quantiles at which it is limited, which corresponds to replacing parts with the corresponding quantiles.",3009801004421193248,related_concept,Central tendency,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.9655172413793104,0.717973889367848
551,1092,Mean,http://dbpedia.org/resource/Mean,http://en.wikipedia.org/wiki/Mean,"There are several kinds of mean in mathematics, especially in statistics. Each mean serves to summarize a given group of data, often to better understand the overall value (magnitude and sign) of a given data set. For a data set, the arithmetic mean, also known as ""arithmetic average"", is a measure of central tendency of a finite set of numbers: specifically, the sum of the values divided by the number of values. The arithmetic mean of a set of numbers x1, x2, ..., xn is typically denoted using an overhead bar, . If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is the sample mean to distinguish it from the mean, or expected value, of the underlying distribution, the population mean (denoted or ). Outside probability and statistics, a wide range of other notions of mean are often used in geometry and mathematical analysis; examples are given below.",6544048166719960573,related_concept,Central tendency,2024-06-23 21:17:09.728,[],"['Poisson distribution', 'Fréchet mean']",set(),set(),0,4.3164556962025316,0.8748033702600511
552,1094,Geometric mean,http://dbpedia.org/resource/Geometric_mean,http://en.wikipedia.org/wiki/Geometric_mean,"In mathematics, the geometric mean is a mean or average which indicates a central tendency of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the nth root of the product of n numbers, i.e., for a set of numbers a1, a2, ..., an, the geometric mean is defined as or, equivalently, as the arithmetic mean in logscale: For instance, the geometric mean of two numbers, say 2 and 8, is just the square root of their product, that is, . As another example, the geometric mean of the three numbers 4, 1, and 1/32 is the cube root of their product (1/8), which is 1/2, that is, . The geometric mean applies only to positive numbers. The geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as a set of growth figures: values of the human population or interest rates of a financial investment over time. It also applies to benchmarking, where it is particularly useful for computing means of speedup ratios: since the mean of 0.5x (half as fast) and 2x (twice as fast) will be 1 (i.e., no speedup overall). The geometric mean can be understood in terms of geometry. The geometric mean of two numbers, and , is the length of one side of a square whose area is equal to the area of a rectangle with sides of lengths and . Similarly, the geometric mean of three numbers, , , and , is the length of one edge of a cube whose volume is the same as that of a cuboid with sides whose lengths are equal to the three given numbers. The geometric mean is one of the three classical Pythagorean means, together with the arithmetic mean and the harmonic mean. For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see Inequality of arithmetic and geometric means.)",8453979673334661548,related_concept,Central tendency,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.1379310344827585,1.0757722392040696
553,1097,Empirical measure,http://dbpedia.org/resource/Empirical_measure,http://en.wikipedia.org/wiki/Empirical_measure,"In probability theory, an empirical measure is a random measure arising from a particular realization of a (usually finite) sequence of random variables. The precise definition is found below. Empirical measures are relevant to mathematical statistics. The motivation for studying empirical measures is that it is often impossible to know the true underlying probability measure . We collect observations and compute relative frequencies. We can estimate , or a related distribution function by means of the empirical measure or empirical distribution function, respectively. These are uniformly good estimates under certain conditions. Theorems in the area of empirical processes provide rates of this convergence.",7075069102517442147,related_concept,Central tendency,2024-06-23 21:17:09.728,"['Empirical measure', 'Theorem']","['Empirical measure', 'Theorem']",set(),set(),0,1.5,1.3452077951238466
554,1098,Averages,http://dbpedia.org/resource/Averages,http://en.wikipedia.org/wiki/Averages,,2513996053740924300,related_concept,Central tendency,2024-06-23 21:17:09.728,[],['Central tendency'],set(),set(),0,0.11688311688311688,0.5736288941674655
555,1102,Arithmetic mean,http://dbpedia.org/resource/Arithmetic_mean,http://en.wikipedia.org/wiki/Arithmetic_mean,"In mathematics and statistics, the arithmetic mean ( /ˌærɪθˈmɛtɪk ˈmiːn/ air-ith-MET-ik) or arithmetic average, or just the mean or the average (when the context is clear), is the sum of a collection of numbers divided by the count of numbers in the collection. The collection is often a set of results of an experiment or an observational study, or frequently a set of results from a survey. The term ""arithmetic mean"" is preferred in some contexts in mathematics and statistics, because it helps distinguish it from other means, such as the geometric mean and the harmonic mean. In addition to mathematics and statistics, the arithmetic mean is used frequently in many diverse fields such as economics, anthropology and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population. While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). For skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not coincide with one's notion of ""middle"", and robust statistics, such as the median, may provide better description of central tendency.",1804383347428643990,main_concept,Central tendency,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.4251336898395723,4.00726265737572e-304
556,1104,Memetic algorithm,http://dbpedia.org/resource/Memetic_algorithm,http://en.wikipedia.org/wiki/Memetic_algorithm,"A memetic algorithm (MA) in computer science and operations research, is an extension of the traditional genetic algorithm. It may provide a sufficiently good solution to an optimization problem. It uses a local search technique to reduce the likelihood of premature convergence. Memetic algorithms represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MAs are also referred to in the literature as Baldwinian evolutionary algorithms (EAs), Lamarckian EAs, cultural algorithms, or genetic local search.",1341991885895010133,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,['Memetic algorithm'],"['Memetic algorithm', 'Co-evolution']",set(),set(),0,1.4433962264150944,0.6788638571383199
557,1106,Gaussian adaptation,http://dbpedia.org/resource/Gaussian_adaptation,http://en.wikipedia.org/wiki/Gaussian_adaptation,"Gaussian adaptation (GA), also called normal or natural adaptation (NA) is an evolutionary algorithm designed for the maximization of manufacturing yield due to statistical deviation of component values of signal processing systems. In short, GA is a stochastic adaptive process where a number of samples of an n-dimensional vector x[xT = (x1, x2, ..., xn)] are taken from a multivariate Gaussian distribution, N(m, M), having mean m and moment matrix M. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are m* and M*. The outcome of x as a pass sample is determined by a function s(x), 0 < s(x) < q ≤ 1, such that s(x) is the probability that x will be selected as a pass sample. The average probability of finding pass samples (yield) is Then the theorem of GA states: For any s(x) and for any value of P < q, there always exist a Gaussian p. d. f. [ probability density function ] that is adapted for maximum dispersion. The necessary conditions for a local optimum are m = m* and M proportional to M*. The dual problem is also solved: P is maximized while keeping the dispersion constant (Kjellström, 1991). Proofs of the theorem may be found in the papers by Kjellström, 1970, and Kjellström & Taxén, 1981. Since dispersion is defined as the exponential of entropy/disorder/average information it immediately follows that the theorem is valid also for those concepts. Altogether, this means that Gaussian adaptation may carry out a simultaneous maximisation of yield and average information (without any need for the yield or the average information to be defined as criterion functions). The theorem is valid for all regions of acceptability and all Gaussian distributions. It may be used by cyclic repetition of random variation and selection (like the natural evolution). In every cycle a sufficiently large number of Gaussian distributed points are sampled and tested for membership in the region of acceptability. The centre of gravity of the Gaussian, m, is then moved to the centre of gravity of the approved (selected) points, m*. Thus, the process converges to a state of equilibrium fulfilling the theorem. A solution is always approximate because the centre of gravity is always determined for a limited number of points. It was used for the first time in 1969 as a pure optimization algorithm making the regions of acceptability smaller and smaller (in analogy to simulated annealing, Kirkpatrick 1983). Since 1970 it has been used for both ordinary optimization and yield maximization.",2712807043917162498,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,['Gaussian adaptation'],"['Gaussian adaptation', 'Mean', 'Computer simulation']",set(),set(),0,0.84,1.3067632679461298
558,1107,Genetic representation,http://dbpedia.org/resource/Genetic_representation,http://en.wikipedia.org/wiki/Genetic_representation,"In computer programming, genetic representation is a way of presenting solutions/individuals in evolutionary computation methods. Genetic representation can encode appearance, behavior, physical qualities of individuals. Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation. Terminology is often analogous with natural genetics. The block of computer memory that represents one candidate solution is called an individual. The data in that block is called a chromosome. Each chromosome consists of genes. The possible values of a particular gene are called alleles. A programmer may represent all the individuals of a population using binary encoding, permutational encoding, encoding by tree, or any one of several other representations. Genetic algorithms (GAs) typically linear representations; these are often, but not always, binary. Holland's original description of GA used arrays of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size. This facilitates simple crossover operation. Variable length representations were also explored in Genetic algorithms, but crossover implementation is more complex in this case. Evolution strategy uses linear real-valued representations, e.g. an array of real values. It uses mostly gaussian mutation and blending/averaging crossover. Genetic programming (GP) pioneered tree-like representations and developed genetic operators suitable for such representations. Tree-like representations are used in GP to represent and evolve functional programs with desired properties. Human-based genetic algorithm (HBGA) offers a way to avoid solving hard representation problems by outsourcing all genetic operators to outside agents, in this case, humans. The algorithm has no need for knowledge of a particular fixed genetic representation as long as there are enough external agents capable of handling those representations, allowing for free-form and evolving genetic representations.",8556858686067934720,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,"['Evolution', 'Genetic representation', 'Genetic programming', 'Genetic algorithm']","['Genetic representation', 'Genetic algorithm', 'Evolution', 'Genetic programming']",set(),set(),0,0.6122448979591837,3.284111640445811e-304
559,1109,Metaheuristic,http://dbpedia.org/resource/Metaheuristic,http://en.wikipedia.org/wiki/Metaheuristic,"In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a subset of solutions which is otherwise too large to be completely enumerated or otherwise explored. Metaheuristics may make relatively few assumptions about the optimization problem being solved and so may be usable for a variety of problems. Compared to optimization algorithms and iterative methods, metaheuristics do not guarantee that a globally optimal solution can be found on some class of problems. Many metaheuristics implement some form of stochastic optimization, so that the solution found is dependent on the set of random variables generated. In combinatorial optimization, by searching over a large set of feasible solutions, metaheuristics can often find good solutions with less computational effort than optimization algorithms, iterative methods, or simple heuristics. As such, they are useful approaches for optimization problems. Several books and survey papers have been published on the subject. Most literature on metaheuristics is experimental in nature, describing empirical results based on computer experiments with the algorithms. But some formal theoretical results are also available, often on convergence and the possibility of finding the global optimum. Many metaheuristic methods have been published with claims of novelty and practical efficacy. While the field also features high-quality research, many of the publications have been of poor quality; flaws include vagueness, lack of conceptual elaboration, poor experiments, and ignorance of previous literature.",8709476579600262815,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,['Metaheuristic'],"['Metaheuristic', 'Memetic algorithm', 'Hypercube', 'Evolution', 'Algorithm', 'Heuristic']",set(),set(),0,2.3313253012048194,0.9143639091474259
560,1110,Estimation of distribution algorithm,http://dbpedia.org/resource/Estimation_of_distribution_algorithm,http://en.wikipedia.org/wiki/Estimation_of_distribution_algorithm,"Estimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding an uninformative prior over admissible solutions and ending with the model that generates only the global optima. EDAs belong to the class of evolutionary algorithms. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an implicit distribution defined by one or more variation operators, whereas EDAs use an explicit probability distribution encoded by a Bayesian network, a multivariate normal distribution, or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to LISP style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions. The general procedure of an EDA is outlined in the following: t := 0initialize model M(0) to represent uniform distribution over admissible solutionswhile (termination criteria not met) do P := generate N>0 candidate solutions by sampling M(t) F := evaluate all candidate solutions in P M(t + 1) := adjust_model(P, F, M(t)) t := t + 1 Using explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis. Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-specific neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem. For example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.",2650599604869032855,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,"['Estimation of distribution algorithm', 'Bayesian network']","['Estimation of distribution algorithm', 'Bayesian network']",set(),set(),0,1.4482758620689655,1.3214184265099245
561,1114,Cultural algorithm,http://dbpedia.org/resource/Cultural_algorithm,http://en.wikipedia.org/wiki/Cultural_algorithm,"Cultural algorithms (CA) are a branch of evolutionary computation where there is a knowledge component that is called the belief space in addition to the population component. In this sense, cultural algorithms can be seen as an extension to a conventional genetic algorithm. Cultural algorithms were introduced by Reynolds (see references).",4215022401030299027,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,['Cultural algorithm'],['Cultural algorithm'],set(),set(),0,1.037037037037037,3.349493543314609e-304
562,1116,Differential evolution,http://dbpedia.org/resource/Differential_evolution,http://en.wikipedia.org/wiki/Differential_evolution,"In evolutionary computation, differential evolution (DE) is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. Such methods are commonly known as metaheuristics as they make few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as DE do not guarantee an optimal solution is ever found. DE is used for multidimensional real-valued functions but does not use the gradient of the problem being optimized, which means DE does not require the optimization problem to be differentiable, as is required by classic optimization methods such as gradient descent and quasi-newton methods. DE can therefore also be used on optimization problems that are not even continuous, are noisy, change over time, etc. DE optimizes a problem by maintaining a population of candidate solutions and creating new candidate solutions by combining existing ones according to its simple formulae, and then keeping whichever candidate solution has the best score or fitness on the optimization problem at hand. In this way, the optimization problem is treated as a black box that merely provides a measure of quality given a candidate solution and the gradient is therefore not needed. DE was introduced by Storn and Price in the 1990s. Books have been published on theoretical and practical aspects of using DE in parallel computing, multiobjective optimization, constrained optimization, and the books also contain surveys of application areas. Surveys on the multi-faceted research aspects of DE can be found in journal articles .",2869933185621929281,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,[],"['Evolution', 'Differential evolution']",set(),set(),0,1.4,1.3140095587010354
563,1117,Selection (genetic algorithm),http://dbpedia.org/resource/Selection_(genetic_algorithm),http://en.wikipedia.org/wiki/Selection_(genetic_algorithm),"Selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator). A generic selection procedure may be implemented as follows: 1. 
* The fitness function is evaluated for each individual, providing fitness values, which are then normalized. Normalization means dividing the fitness value of each individual by the sum of all fitness values, so that the sum of all resulting fitness values equals 1. 2. 
* Accumulated normalized fitness values are computed: the accumulated fitness value of an individual is the sum of its own fitness value plus the fitness values of all the previous individuals; the accumulated fitness of the last individual should be 1, otherwise something went wrong in the normalization step. 3. 
* A random number R between 0 and 1 is chosen. 4. 
* The selected individual is the first one whose accumulated normalized value is greater than or equal to R. For many problems the above algorithm might be computationally demanding. A simpler and faster alternative uses the so-called stochastic acceptance. If this procedure is repeated until there are enough selected individuals, this selection method is called fitness proportionate selection or roulette-wheel selection. If instead of a single pointer spun multiple times, there are multiple, equally spaced pointers on a wheel that is spun once, it is called stochastic universal sampling.Repeatedly selecting the best individual of a randomly chosen subset is tournament selection. Taking the best half, third or another proportion of the individuals is truncation selection. There are other selection algorithms that do not consider all individuals for selection, but only those with a fitness value that is higher than a given (arbitrary) constant. Other algorithms select from a restricted pool where only a certain percentage of the individuals are allowed, based on fitness value. Retaining the best individuals in a generation unchanged in the next generation, is called elitism or elitist selection. It is a successful (slight) variant of the general process of constructing a new population.",5523154239014420699,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,[],['Probability'],set(),set(),0,1.0246913580246915,0.9839619494378353
564,1119,List of genetic algorithm applications,http://dbpedia.org/resource/List_of_genetic_algorithm_applications,http://en.wikipedia.org/wiki/List_of_genetic_algorithm_applications,This is a list of genetic algorithm (GA) applications.,5150637568084722588,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.38392857142857145,3.2988818223314316e-304
565,1120,Search algorithm,http://dbpedia.org/resource/Search_algorithm,http://en.wikipedia.org/wiki/Search_algorithm,"In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values. algorithms are Although search engines use search algorithms, they belong to the study of information retrieval, not algorithmics. The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes. Search algorithms can be classified based on their mechanism of searching into three types of algorithms: linear, binary, and hashing. Linear search algorithms check every record for the one associated with a target key in a linear fashion. Binary, or half-interval, searches repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order. Digital search algorithms work based on the properties of digits in data structures by using numerical keys. Finally, hashing directly maps keys to records based on a hash function. Algorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. In simple terms, the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.",2624051052236920091,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,"['Search algorithm', 'Algorithm']","['Search algorithm', 'Algorithm', ""Dijkstra's algorithm"", ""Kruskal's algorithm"", ""Prim's algorithm""]",set(),set(),0,3.410958904109589,1.0955146799611724
566,1121,Stochastic optimization,http://dbpedia.org/resource/Stochastic_optimization,http://en.wikipedia.org/wiki/Stochastic_optimization,"Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization.Stochastic optimization methods generalize deterministic methods for deterministic problems.",8634574606411855465,related_concept,Genetic algorithm,2024-06-23 21:17:09.728,"['Stochastic optimization', 'Stochastic']","['Stochastic optimization', 'Stochastic']",set(),set(),0,2.1454545454545455,1.3007916867465872
567,1127,Kolmogorov complexity,http://dbpedia.org/resource/Kolmogorov_complexity,http://en.wikipedia.org/wiki/Kolmogorov_complexity,"In algorithmic information theory (a subfield of computer science and mathematics), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest computer program (in a predetermined programming language) that produces the object as output. It is a measure of the computational resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff–Kolmogorov–Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963 and is a generalization of classical information theory. The notion of Kolmogorov complexity can be used to state and prove impossibility results akin to Cantor's diagonal argument, Gödel's incompleteness theorem, and Turing's halting problem.In particular, no program P computing a lower bound for each text's Kolmogorov complexity can return a value essentially larger than P's own length (see section ); hence no single program can compute the exact Kolmogorov complexity for infinitely many texts.",4230637722781455468,related_concept,Minimum description length,2024-06-23 21:17:09.728,['Kolmogorov complexity'],"['Kolmogorov complexity', 'Theorem', 'Algorithmic information theory', 'Algorithm', 'Inference', 'Probability']",set(),set(),0,1.5338645418326693,0.9385836908398312
568,1130,Entropy (information theory),http://dbpedia.org/resource/Entropy_(information_theory),http://en.wikipedia.org/wiki/Entropy_(information_theory),"In information theory, the entropy of a random variable is the average level of ""information"", ""surprise"", or ""uncertainty"" inherent to the variable's possible outcomes. Given a discrete random variable , which takes values in the alphabet and is distributed according to : where denotes the sum over the variable's possible values. The choice of base for , the logarithm, varies for different applications. Base 2 gives the unit of bits (or ""shannons""), while base e gives ""natural units"" nat, and base 10 gives units of ""dits"", ""bans"", or ""hartleys"". An equivalent definition of entropy is the expected value of the self-information of a variable. The concept of information entropy was introduced by Claude Shannon in his 1948 paper ""A Mathematical Theory of Communication"", and is also referred to as Shannon entropy. Shannon's theory defines a data communication system composed of three elements: a source of data, a communication channel, and a receiver. The ""fundamental problem of communication"" – as expressed by Shannon – is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel. Shannon considered various ways to encode, compress, and transmit messages from a data source, and proved in his famous source coding theorem that the entropy represents an absolute mathematical limit on how well data from the source can be losslessly compressed onto a perfectly noiseless channel. Shannon strengthened this result considerably for noisy channels in his noisy-channel coding theorem. Entropy in information theory is directly analogous to the entropy in statistical thermodynamics. The analogy results when the values of the random variable designate energies of microstates, so Gibbs formula for the entropy is formally identical to Shannon's formula. Entropy has relevance to other areas of mathematics such as combinatorics and machine learning. The definition can be derived from a set of axioms establishing that entropy should be a measure of how ""surprising"" the average outcome of a variable is. For a continuous random variable, differential entropy is analogous to entropy.",6085514988202677543,related_concept,Minimum description length,2024-06-23 21:17:09.728,[],"['Mean', 'Kolmogorov complexity', 'Kullback–Leibler divergence', 'Machine learning', 'Decision tree learning', 'Decision tree', 'Bayesian inference', 'Classification']",set(),set(),0,1.7814207650273224,5.092739244698295e-304
569,1133,Universal code (data compression),http://dbpedia.org/resource/Universal_code_(data_compression),http://en.wikipedia.org/wiki/Universal_code_(data_compression),"In data compression, a universal code for integers is a prefix code that maps the positive integers onto binary codewords, with the additional property that whatever the true probability distribution on integers, as long as the distribution is monotonic (i.e., p(i) ≥ p(i + 1) for all positive i), the expected lengths of the codewords are within a constant factor of the expected lengths that the for that probability distribution would have assigned. A universal code is asymptotically optimal if the ratio between actual and optimal expected lengths is bounded by a function of the information entropy of the code that, in addition to being bounded, approaches 1 as entropy approaches infinity. In general, most prefix codes for integers assign longer codewords to larger integers. Such a code can be used to efficiently communicate a message drawn from a set of possible messages, by simply ordering the set of messages by decreasing probability and then sending the index of the intended message. Universal codes are generally not used for precisely known probability distributions, and no universal code is known to be optimal for any distribution used in practice. A universal code should not be confused with , in which the data compression method need not be a fixed prefix code and the ratio between actual and optimal expected lengths must approach one. However, note that an asymptotically optimal universal code can be used on independent identically-distributed sources, by using increasingly large blocks, as a method of universal source coding.",639719345945765398,related_concept,Minimum description length,2024-06-23 21:17:09.728,[],"['Data', 'Kullback–Leibler divergence']",set(),set(),0,0.9444444444444444,5.139342856432152e-304
570,1134,Inductive inference,http://dbpedia.org/resource/Inductive_inference,http://en.wikipedia.org/wiki/Inductive_inference,,7958258461021405692,related_concept,Minimum description length,2024-06-23 21:17:09.728,[],"['Analytics', 'Inference', 'Probability', 'AI', 'Bayesian inference']",set(),set(),0,0.0807061790668348,0.45454833281694423
571,1136,Solomonoff's theory of inductive inference,http://dbpedia.org/resource/Solomonoff's_theory_of_inductive_inference,http://en.wikipedia.org/wiki/Solomonoff's_theory_of_inductive_inference,"Solomonoff's theory of inductive inference is a mathematical proof that if a universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset. This formalization of Occam's razor for induction was introduced by Ray Solomonoff, based on probability theory and theoretical computer science. In essence, Solomonoff's induction derives the posterior probability of any computable theory, given a sequence of observed data. This posterior probability is derived from Bayes rule and some universal prior, that is, a prior that assigns a positive probability to any computable theory.",743521293426053216,related_concept,Minimum description length,2024-06-23 21:17:09.728,"[""Solomonoff's theory of inductive inference""]","[""Solomonoff's theory of inductive inference"", 'Bayesianism', 'Kolmogorov complexity', ""Bayes' theorem"", 'Kullback–Leibler divergence', 'AI']",set(),set(),0,0.875,1.3154989512799908
572,1137,Inductive probability,http://dbpedia.org/resource/Inductive_probability,http://en.wikipedia.org/wiki/Inductive_probability,"Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world. There are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods. Deduction establishes new facts based on existing facts. Inference establishes new facts from data. Its basis is Bayes' theorem. Information describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters. But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements. Occam's razor says the ""simplest theory, consistent with the data is most likely to be correct"". The ""simplest theory"" is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct.",9207231032357594695,related_concept,Minimum description length,2024-06-23 21:17:09.728,"['Inductive probability', 'Inference', ""Bayes' theorem""]","['Inductive probability', 'Inference', ""Bayes' theorem"", 'Probability', 'Bayesian inference', 'Overfitting', ""Solomonoff's theory of inductive inference"", 'Kolmogorov complexity', 'Theorem', 'Inductive inference']",{'Inference'},set(),0,0.7222222222222222,1.3040299553782009
573,1138,Kolmogorov structure function,http://dbpedia.org/resource/Kolmogorov_structure_function,http://en.wikipedia.org/wiki/Kolmogorov_structure_function,"In 1973, Andrey Kolmogorov proposed a non-probabilistic approach to statistics and model selection. Let each datum be a finite binary string and a model be a finite set of binary strings. Consider model classes consisting of models of given maximal Kolmogorov complexity.The Kolmogorov structure function of an individual data string expresses the relation between the complexity level constraint on a model class and the least log-cardinality of a model in the class containing the data. The structure function determines all stochastic properties of the individual data string: for every constrained model class it determines the individual best-fitting model in the class irrespective of whether the true model is in the model class considered or not. In the classical case we talk about a set of data with a probability distribution, and the properties are those of the expectations. In contrast, here we deal with individual data strings and the properties of the individual string focused on. In this setting, a property holds with certainty rather than with high probability as in the classical case. The Kolmogorov structure function precisely quantifies the goodness-of-fit of an individual model with respect to individual data. The Kolmogorov structure function is used in the algorithmic information theory, also known as the theory of Kolmogorov complexity, for describing the structure of a string by use of models of increasing complexity.",1309534957323938492,related_concept,Minimum description length,2024-06-23 21:17:09.728,"['Kolmogorov complexity', 'Kolmogorov structure function']","['Kolmogorov complexity', 'Kolmogorov structure function', 'Minimum description length']",set(),set(),0,1.1304347826086956,1.161687526683824
574,1140,Minimum message length,http://dbpedia.org/resource/Minimum_message_length,http://en.wikipedia.org/wiki/Minimum_message_length,"Minimum message length (MML) is a Bayesian information-theoretic method for statistical model comparison and selection. It provides a formal information theory restatement of Occam's Razor: even when models are equal in their measure of fit-accuracy to the observed data, the one generating the most concise explanation of data is more likely to be correct (where the explanation consists of the statement of the model, followed by the lossless encoding of the data using the stated model). MML was invented by Chris Wallace, first appearing in the seminal paper ""An information measure for classification"". MML is intended not just as a theoretical construct, but as a technique that may be deployed in practice. It differs from the related concept of Kolmogorov complexity in that it does not require use of a Turing-complete language to model data.",109187632401261506,related_concept,Minimum description length,2024-06-23 21:17:09.728,"['Kolmogorov complexity', 'Minimum message length']","['Kolmogorov complexity', 'Minimum message length']",set(),set(),0,0.20052083333333334,5.258187856074549e-304
575,1141,Algorithmic information theory,http://dbpedia.org/resource/Algorithmic_information_theory,http://en.wikipedia.org/wiki/Algorithmic_information_theory,"Algorithmic information theory (AIT) is a branch of theoretical computer science that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. In other words, it is shown within algorithmic information theory that computational incompressibility ""mimics"" (except for a constant that only depends on the chosen universal programming language) the relations or inequalities found in information theory. According to Gregory Chaitin, it is ""the result of putting Shannon's information theory and Turing's computability theory into a cocktail shaker and shaking vigorously."" Besides the formalization of a universal measure for irreducible information content of computably generated objects, some main achievements of AIT were to show that: in fact algorithmic complexity follows (in the self-delimited case) the same inequalities (except for a constant) that entropy does, as in classical information theory; randomness is incompressibility; and, within the realm of randomly generated software, the probability of occurrence of any data structure is of the order of the shortest program that generates it when running on a universal machine. AIT principally studies measures of irreducible information content of strings (or other data structures). Because most mathematical objects can be described in terms of strings, or as the limit of a sequence of strings, it can be used to study a wide variety of mathematical objects, including integers. One of the main motivations behind AIT is the very study of the information carried by mathematical objects as in the field of metamathematics, e.g., as shown by the incompleteness results mentioned below. Other main motivations came from surpassing the limitations of classical information theory for single and fixed objects, formalizing the concept of randomness, and finding a meaningful probabilistic inference without prior knowledge of the probability distribution (e.g., whether it is independent and identically distributed, Markovian, or even stationary). In this way, AIT is known to be basically founded upon three main mathematical concepts and the relations between them: algorithmic complexity, algorithmic randomness, and algorithmic probability.",3052152135353292654,related_concept,Minimum description length,2024-06-23 21:17:09.728,"['AI', 'Algorithmic information theory', 'Algorithm']","['AI', 'Algorithmic information theory', 'Algorithm', 'Kolmogorov complexity', ""Gödel's incompleteness theorems"", 'Inference']",set(),set(),0,0.46981627296587924,5.2398880824301195e-304
576,1143,Bayesian Information Criterion,http://dbpedia.org/resource/Bayesian_Information_Criterion,http://en.wikipedia.org/wiki/Bayesian_Information_Criterion,,7110496986554694480,related_concept,Minimum description length,2024-06-23 21:17:09.728,[],"['Akaike information criterion', 'AI', 'Bayes factor', 'F-test']",set(),set(),0,0.010471204188481676,0.3390467239273487
577,1145,Minimum description length,http://dbpedia.org/resource/Minimum_description_length,http://en.wikipedia.org/wiki/Minimum_description_length,"Minimum Description Length (MDL) is a model selection principle where the shortest description of the data is the best model. MDL methods learn through a data compression perspective and are sometimes described as mathematical applications of Occam's razor. The MDL principle can be extended to other forms of inductive inference and learning, for example to estimation and sequential prediction, without explicitly identifying a single model of the data. MDL has its origins mostly in information theory and has been further developed within the general fields of statistics, theoretical computer science and machine learning, and more narrowly computational learning theory. Historically, there are different, yet interrelated, usages of the definite noun phrase ""the minimum description length principle"" that vary in what is meant by description: 
* Within Jorma Rissanen's theory of learning, a central concept of information theory, models are statistical hypotheses and descriptions are defined as universal codes. 
* Rissanen's 1978 pragmatic first attempt to automatically derive short descriptions, relates to the Bayesian Information Criterion (BIC). 
* Within Algorithmic Information Theory, where the description length of a data sequence is the length of the smallest program that outputs that data set. In this context, it is also known as 'idealized' MDL principle and it is closely related to Solomonoff's theory of inductive inference, which is that the best model of a data set is represented by its shortest self-extracting archive.",4099809120386864313,main_concept,,2024-06-23 21:17:09.728,"[""Solomonoff's theory of inductive inference"", 'Bayesian Information Criterion', 'Algorithm']","['Kolmogorov complexity', 'Probability', 'Algorithm', 'Jeffreys prior', 'Bayesian inference', 'Kolmogorov structure function']",set(),set(),0,0.3071253071253071,5.365297378047964e-304
578,1153,Goodman–Nguyen–Van Fraassen algebra,http://dbpedia.org/resource/Goodman–Nguyen–Van_Fraassen_algebra,http://en.wikipedia.org/wiki/Goodman–Nguyen–Van_Fraassen_algebra,,4657797552855577170,related_concept,Conditional probability,2024-06-23 21:17:09.728,[],"['AI', 'Probability', 'Inference', 'Axiom']",set(),set(),0,0.15384615384615385,0.35103933744285437
579,1157,Probability interpretations,http://dbpedia.org/resource/Probability_interpretations,http://en.wikipedia.org/wiki/Probability_interpretations,"The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical, tendency of something to occur, or is it a measure of how strongly one believes it will occur, or does it draw on both these elements? In answering such questions, mathematicians interpret the probability values of probability theory. There are two broad categories of probability interpretations which can be called ""physical"" and ""evidential"" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or ""relative frequency"", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer). Evidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom). Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of ""frequentist"" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the frequency interpretation when it makes sense (although not as a definition), but there's less agreement regarding physical probabilities. Bayesians consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference. The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word ""frequentist"" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, ""frequentist probability"" is just another name for physical (or objective) probability. Those who promote Bayesian inference view ""frequentist statistics"" as an approach to statistical inference that is based on the frequency interpretation of probability, usually relying on the law of large numbers and characterized by what is called 'Null Hypothesis Significance Testing' (NHST). Also the word ""objective"", as applied to probability, sometimes means exactly what ""physical"" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities. It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis. — (Savage, 1954, p 2)",5003856150608545179,related_concept,Conditional probability,2024-06-23 21:17:09.728,"['Bayesian inference', 'Hypothesis']","['Bayesian inference', 'Hypothesis', 'Probability', 'Probability theory']",set(),set(),0,1.3416666666666666,4.689123019393525e-304
580,1160,Elementary event,http://dbpedia.org/resource/Elementary_event,http://en.wikipedia.org/wiki/Elementary_event,"In probability theory, an elementary event, also called an atomic event or sample point, is an event which contains only a single outcome in the sample space. Using set theory terminology, an elementary event is a singleton. Elementary events and their corresponding outcomes are often written interchangeably for simplicity, as such an event corresponding to precisely one outcome. The following are examples of elementary events: 
* All sets where if objects are being counted and the sample space is (the natural numbers). 
* if a coin is tossed twice. where stands for heads and for tails. 
* All sets where is a real number. Here is a random variable with a normal distribution and This example shows that, because the probability of each elementary event is zero, the probabilities assigned to elementary events do not determine a continuous probability distribution.",543232500433035296,related_concept,Conditional probability,2024-06-23 21:17:09.728,['Elementary event'],['Elementary event'],set(),set(),0,1.4307692307692308,4.601761431169637e-304
581,1163,Probability axioms,http://dbpedia.org/resource/Probability_axioms,http://en.wikipedia.org/wiki/Probability_axioms,"The Kolmogorov axioms are the foundations of probability theory introduced by Russian mathematician Andrey Kolmogorov in 1933. These axioms remain central and have direct contributions to mathematics, the physical sciences, and real-world probability cases. An alternative approach to formalising probability, favoured by some Bayesians, is given by Cox's theorem.",8387592598090151171,related_concept,Conditional probability,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.0930232558139537,1.3451932750138118
582,1167,Bayesian optimization,http://dbpedia.org/resource/Bayesian_optimization,http://en.wikipedia.org/wiki/Bayesian_optimization,Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.,4443609231201320328,related_concept,Machine learning,2024-06-23 21:17:09.728,['Bayesian optimization'],"['Bayesian optimization', 'Estimator', 'Optimization problem', 'Broyden–Fletcher–Goldfarb–Shanno algorithm']",set(),set(),0,0.4,1.1188036482891448
583,1170,Meta-learning (computer science),http://dbpedia.org/resource/Meta-learning_(computer_science),http://en.wikipedia.org/wiki/Meta-learning_(computer_science),"Meta learningis a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn. Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood. By using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for Jürgen Schmidhuber's early work (1987) and Yoshua Bengio et al.'s work (1991), considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta learning system using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.",4149744321267309643,related_concept,Machine learning,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.3953488372093024,4.7935861791055954e-304
584,1176,Computational statistics,http://dbpedia.org/resource/Computational_statistics,http://en.wikipedia.org/wiki/Computational_statistics,"Computational statistics, or statistical computing, is the bond between statistics and computer science. It means statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education. As in traditional statistics the goal is to transform raw data into knowledge, but the focus lies on computer intensive statistical methods, such as cases with very large sample size and non-homogeneous data sets. The terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as ""the application of computer science to statistics"",and 'computational statistics' as ""aiming at the design of algorithm for implementingstatistical methods on computers, including the ones unthinkable before the computerage (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems"" [sic]. The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.",2442802363611109497,related_concept,Machine learning,2024-06-23 21:17:09.728,"['Computation', 'Computational statistics']","['Computation', 'Computational statistics', 'Maximum likelihood estimation', 'Boot', 'Bootstrapping']",set(),set(),0,2.380952380952381,1.3992672167717606
585,1177,Feature learning,http://dbpedia.org/resource/Feature_learning,http://en.wikipedia.org/wiki/Feature_learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Feature learning can be either supervised, unsupervised or self-supervised. 
* In supervised feature learning, features are learned using labeled input data. Labeled data includes input-label pairs where the input is given to the model and it must produce the ground truth label as the correct answer. This can be leveraged to generate feature representations with the model which result in high label prediction accuracy. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning. 
* In unsupervised feature learning, features are learned with unlabeled input data by analyzing the relationship between points in the dataset. Examples include dictionary learning, independent component analysis, matrix factorization and various forms of clustering. 
* In self-supervised feature learning, features are learned using unlabeled data like unsupervised learning, however input-label pairs are constructed from each data point, which enables learning the structure of the data through supervised methods such as gradient descent. Classical examples include word embeddings and autoencoders. SSL has since been applied to many modalities through the use of deep neural network architectures such as CNNs and Transformers.",299729163330053310,related_concept,Machine learning,2024-06-23 21:17:09.728,['Feature learning'],"['Feature learning', 'Neural network', 'K-means clustering', 'Principal component analysis', 'Boot']",set(),set(),0,1.3915094339622642,4.842852662694897e-304
586,1178,Symbolic artificial intelligence,http://dbpedia.org/resource/Symbolic_artificial_intelligence,http://en.wikipedia.org/wiki/Symbolic_artificial_intelligence,"In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems), symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems. Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the middle 1990s. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checker's Playing Program led to unrealistic expectations and promises and was followed by the First AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations. Neural networks, a sub-symbolic approach, had been pursued from early days and was to reemerge strongly in 2012. Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: ""Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks."" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning.",5594415229677475126,related_concept,Machine learning,2024-06-23 21:17:09.728,"['Data', 'AI', 'Neural network']","['AI', 'Data', 'Neural network', 'Prolog', 'Hypothesis', 'MYCIN', 'Expert system', 'Inference', 'Backtracking', 'Semantic network', 'Theorem', 'Knowledge-based systems', 'Calculus', 'Natural language processing', 'Machine learning']",set(),set(),0,0.5358090185676393,1.3974444703081268
587,1180,Machine learning control,http://dbpedia.org/resource/Machine_learning_control,http://en.wikipedia.org/wiki/Machine_learning_control,"Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theorywhich solves optimal control problems with methods of machine learning.Key applications are complex nonlinear systemsfor which linear control theory methods are not applicable.",4144958415605837887,related_concept,Machine learning,2024-06-23 21:17:09.728,"['Machine learning control', 'Machine learning']","['Machine learning control', 'Machine learning']",set(),set(),0,0.9523809523809523,0.5559222681914433
588,1181,Natural language processing,http://dbpedia.org/resource/Natural_language_processing,http://en.wikipedia.org/wiki/Natural_language_processing,"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.",1652059827607904075,related_concept,Machine learning,2024-06-23 21:17:09.728,['Natural language processing'],"['Natural language processing', 'AI', 'Machine learning']",set(),set(),0,5.779329608938547,1.4133224386041707
589,1183,Semi-supervised learning,http://dbpedia.org/resource/Semi-supervised_learning,http://en.wikipedia.org/wiki/Semi-supervised_learning,,1499309679081281591,main_concept,Machine learning,2024-06-23 21:17:09.728,[],"['Semi-supervised learning', 'Generative model', 'Iterative']",set(),set(),0,1.2262295081967214,0.4090078616403923
590,1188,Sparse PCA,http://dbpedia.org/resource/Sparse_PCA,http://en.wikipedia.org/wiki/Sparse_PCA,"Sparse principal component analysis (sparse PCA) is a specialised technique used in statistical analysis and, in particular, in the analysis of multivariate data sets. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by introducing sparsity structures to the input variables. A particular disadvantage of ordinary PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables. Contemporary datasets often have the number of input variables comparable with or even much larger than the number of samples. It has been shown that if does not converge to zero, the classical PCA is not consistent. But sparse PCA can retain consistency even if",2014466084214575476,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Sparse PCA'],['Sparse PCA'],set(),set(),0,1.0,4.1871861070549655e-304
591,1190,Covariance matrix,http://dbpedia.org/resource/Covariance_matrix,http://en.wikipedia.org/wiki/Covariance_matrix,"In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance–covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector. Any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself). Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the and directions contain all of the necessary information; a matrix would be necessary to fully characterize the two-dimensional variation. The covariance matrix of a random vector is typically denoted by or .",8203400884941822177,related_concept,Principal component analysis,2024-06-23 21:17:09.728,[],"['Probability', 'Heuristic']",set(),set(),0,0.9675675675675676,5.747751737101342e-304
592,1192,Low-rank approximation,http://dbpedia.org/resource/Low-rank_approximation,http://en.wikipedia.org/wiki/Low-rank_approximation,"In mathematics, low-rank approximation is a minimization problem, in which the cost function measures the fit between a given matrix (the data) and an approximating matrix (the optimization variable), subject to a constraint that the approximating matrix has reduced rank. The problem is used for mathematical modeling and data compression. The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure. Low-rank approximation is closely related to: 
* principal component analysis, 
* factor analysis, 
* total least squares, 
* latent semantic analysis 
* orthogonal regression, and 
* dynamic mode decomposition.",7818587658620639777,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Low-rank approximation'],['Low-rank approximation'],set(),set(),0,1.0869565217391304,4.242653643406903e-304
593,1193,Empirical orthogonal functions,http://dbpedia.org/resource/Empirical_orthogonal_functions,http://en.wikipedia.org/wiki/Empirical_orthogonal_functions,"In statistics and signal processing, the method of empirical orthogonal function (EOF) analysis is a decomposition of a signal or data set in terms of orthogonal basis functions which are determined from the data. The term is also interchangeable with the geographically weighted Principal components analysis in geophysics. The i th basis function is chosen to be orthogonal to the basis functions from the first through i − 1, and to minimize the residual variance. That is, the basis functions are chosen to be different from each other, and to account for as much variance as possible. The method of EOF analysis is similar in spirit to harmonic analysis, but harmonic analysis typically uses predetermined orthogonal functions, for example, sine and cosine functions at fixed frequencies. In some cases the two methods may yield essentially the same results. The basis functions are typically found by computing the eigenvectors of the covariance matrix of the data set. A more advanced technique is to form a kernel out of the data, using a fixed kernel. The basis functions from the eigenvectors of the kernel matrix are thus non-linear in the location of the data (see Mercer's theorem and the kernel trick for more information).",622176737757137839,related_concept,Principal component analysis,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.96875,4.214521591139574e-304
594,1194,Partial least squares regression,http://dbpedia.org/resource/Partial_least_squares_regression,http://en.wikipedia.org/wiki/Partial_least_squares_regression,"Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). Partial least squares was introduced by the Swedish statistician Herman O. A. Wold, who then developed it with his son, Svante Wold. An alternative term for PLS is projection to latent structures, but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience, and anthropology.",3251790418179344831,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Partial least squares regression'],"['Partial least squares regression', 'Algorithm']",set(),set(),0,1.2448979591836735,4.963330806352235e-304
595,1195,Robust principal component analysis,http://dbpedia.org/resource/Robust_principal_component_analysis,http://en.wikipedia.org/wiki/Robust_principal_component_analysis,"Robust Principal Component Analysis (RPCA) is a modification of the widely used statistical procedure of principal component analysis (PCA) which works well with respect to grossly corrupted observations. A number of different approaches exist for Robust PCA, including an idealized version of Robust PCA, which aims to recover a low-rank matrix L0 from highly corrupted measurements M = L0 +S0. This decomposition in low-rank and sparse matrices can be achieved by techniques such as Principal Component Pursuit method (PCP), Stable PCP, Quantized PCP, Block based PCP, and Local PCP. Then, optimization methods are used such as the Augmented Lagrange Multiplier Method (ALM), Alternating Direction Method (ADM), Fast Alternating Minimization (FAM), Iteratively Reweighted Least Squares (IRLS ) or alternating projections (AP).",7865482767081583331,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Iterative'],['Iterative'],set(),set(),0,1.4583333333333333,4.1952445212358955e-304
596,1197,Multilinear subspace learning,http://dbpedia.org/resource/Multilinear_subspace_learning,http://en.wikipedia.org/wiki/Multilinear_subspace_learning,"Multilinear subspace learning is an approach to dimensionality reduction. Dimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor. Here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D). The mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. When observations are retained in the same organizational structure as the sensor provides them; as matrices or higher order tensors, their representations are computed by performing N multiple linear projections. Multilinear subspace learning algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA).",5882610543070128021,related_concept,Principal component analysis,2024-06-23 21:17:09.728,"['Dimension', 'Multilinear subspace learning', 'Dimensionality reduction']","['Dimension', 'Multilinear subspace learning', 'Dimensionality reduction', 'Linear subspace']",set(),set(),0,3.0,4.221712309577925e-304
597,1199,Functional principal component analysis,http://dbpedia.org/resource/Functional_principal_component_analysis,http://en.wikipedia.org/wiki/Functional_principal_component_analysis,"Functional principal component analysis (FPCA) is a statistical method for investigating the dominant modes of variation of functional data. Using this method, a random function is represented in the eigenbasis, which is an orthonormal basis of the Hilbert space L2 that consists of the eigenfunctions of the autocovariance operator. FPCA represents functional data in the most parsimonious way, in the sense that when using a fixed number of basis functions, the eigenfunction basis explains more variation than any other basis expansion. FPCA can be applied for representing random functions, or in functional regression and classification.",5563029321846055138,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Functional principal component analysis'],"['Functional principal component analysis', 'Principal component analysis']",set(),set(),0,0.9354838709677419,4.196930734462624e-304
598,1200,Principal component regression,http://dbpedia.org/resource/Principal_component_regression,http://en.wikipedia.org/wiki/Principal_component_regression,"In statistics, principal component regression (PCR) is a regression analysis technique that is based on principal component analysis (PCA). More specifically, PCR is used for estimating the unknown regression coefficients in a standard linear regression model. In PCR, instead of regressing the dependent variable on the explanatory variables directly, the principal components of the explanatory variables are used as regressors. One typically uses only a subset of all the principal components for regression, making PCR a kind of regularized procedure and also a type of shrinkage estimator. Often the principal components with higher variances (the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance-covariance matrix of the explanatory variables) are selected as regressors. However, for the purpose of predicting the outcome, the principal components with low variances may also be important, in some cases even more important. One major use of PCR lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear. PCR can aptly deal with such situations by excluding some of the low-variance principal components in the regression step. In addition, by usually regressing on only a subset of all the principal components, PCR can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model. This can be particularly useful in settings with high-dimensional covariates. Also, through appropriate selection of the principal components to be used for regression, PCR can lead to efficient prediction of the outcome based on the assumed model.",5551204453789367322,related_concept,Principal component analysis,2024-06-23 21:17:09.728,[],"['Data', 'Data pre-processing']",set(),set(),0,0.8731343283582089,0.8706126390996425
599,1201,L1-norm principal component analysis,http://dbpedia.org/resource/L1-norm_principal_component_analysis,http://en.wikipedia.org/wiki/L1-norm_principal_component_analysis,"L1-norm principal component analysis (L1-PCA) is a general method for multivariate data analysis.L1-PCA is often preferred over standard L2-norm principal component analysis (PCA) when the analyzed data may contain outliers (faulty values or corruptions). Both L1-PCA and standard PCA seek a collection of orthogonal directions (principal components) that define a subspace wherein data representation is maximized according to the selected criterion.Standard PCA quantifies data representation as the aggregate of the L2-norm of the data point projections into the subspace, or equivalently the aggregate Euclidean distance of the original points from their subspace-projected representations.L1-PCA uses instead the aggregate of the L1-norm of the data point projections into the subspace. In PCA and L1-PCA, the number of principal components (PCs) is lower than the rank of the analyzed matrix, which coincides with the dimensionality of the space defined by the original data points.Therefore, PCA or L1-PCA are commonly employed for dimensionality reduction for the purpose of data denoising or compression.Among the advantages of standard PCA that contributed to its high popularity are low-cost computational implementation by means of singular-value decomposition (SVD) and statistical optimality when the data set is generated by a true multivariate normal data source. However, in modern big data sets, data often include corrupted, faulty points, commonly referred to as outliers.Standard PCA is known to be sensitive to outliers, even when they appear as a small fraction of the processed data.The reason is that the L2-norm formulation of L2-PCA places squared emphasis on the magnitude of each coordinate of each data point, ultimately overemphasizing peripheral points, such as outliers. On the other hand, following an L1-norm formulation, L1-PCA places linear emphasis on the coordinates of each data point, effectively restraining outliers.",5981410154158709988,related_concept,Principal component analysis,2024-06-23 21:17:09.728,"['L1-norm principal component analysis', 'Euclidean distance']","['L1-norm principal component analysis', 'Euclidean distance', 'Theorem']",set(),set(),0,0.2413793103448276,4.179033946194257e-304
600,1202,Correspondence analysis,http://dbpedia.org/resource/Correspondence_analysis,http://en.wikipedia.org/wiki/Correspondence_analysis,"Correspondence analysis (CA) is a multivariate statistical technique proposed by Herman Otto Hartley (Hirschfeld) and later developed by Jean-Paul Benzécri. It is conceptually similar to principal component analysis, but applies to categorical rather than continuous data. In a similar manner to principal component analysis, it provides a means of displaying or summarising a set of data in two-dimensional graphical form. Its aim is to display in a biplot any structure hidden in the multivariate setting of the data table. As such it is a technique from the field of multivariate ordination. Since the variant of CA described here can be applied either with a focus on the rows or on the columns it should in fact be called simple (symmetric) correspondence analysis. It is traditionally applied to the contingency table of a pair of nominal variables where each cell contains either a count or a zero value. If more than two categorical variables are to be summarized, a variant called multiple correspondence analysis should be chosen instead. CA may also be applied to binary data given the presence/absence coding represents simplified count data i.e. a 1 describes a positive count and 0 stands for a count of zero. Depending on the scores used CA preserves the chi-square distance between either the rows or the columns of the table. Because CA is a descriptive technique, it can be applied to tables regardless of a significant chisquared test. Although the statistic used in inferential statistics and the chi-square distance are computationally related they should not be confused since the latter works as a multivariate statistical distance measure in CA while the statistic is in fact a scalar not a metric.",5158167435959473768,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Correspondence analysis'],"['Correspondence analysis', 'Factor analysis']",set(),set(),0,1.3275862068965518,1.2703377204831174
601,1203,Canonical correlation analysis,http://dbpedia.org/resource/Canonical_correlation_analysis,http://en.wikipedia.org/wiki/Canonical_correlation_analysis,,7325177183610342388,related_concept,Principal component analysis,2024-06-23 21:17:09.728,[],['Covariance'],set(),set(),0,0.04149377593360996,0.40775260921142015
602,1204,Weighted least squares,http://dbpedia.org/resource/Weighted_least_squares,http://en.wikipedia.org/wiki/Weighted_least_squares,"Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which knowledge of the variance of observations is incorporated into the regression.WLS is also a specialization of generalized least squares.",3463867265782179447,related_concept,Principal component analysis,2024-06-23 21:17:09.728,['Weighted least squares'],"['Weighted least squares', 'Normal distribution', ""Student's t-distribution""]",set(),set(),0,1.6170212765957446,4.972235989471663e-304
603,1205,High dimensional data,http://dbpedia.org/resource/High_dimensional_data,http://en.wikipedia.org/wiki/High_dimensional_data,,450878593936686416,related_concept,Principal component analysis,2024-06-23 21:17:09.728,[],['Estimator'],set(),set(),0,0.013404825737265416,0.7091174149260345
604,1207,Principal component analysis,http://dbpedia.org/resource/Principal_component_analysis,http://en.wikipedia.org/wiki/Principal_component_analysis,"Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science. The principal components of a collection of points in a real coordinate space are a sequence of unit vectors, where the -th vector is the direction of a line that best fits the data while being orthogonal to the first vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. In data analysis, the first principal component of a set of variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set. PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The -th principal component can be taken as a direction orthogonal to the first principal components that maximizes the variance of the projected data. For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.",5934303661733579146,main_concept,,2024-06-23 21:17:09.728,"['Factor analysis', 'Principal component analysis']","['Principal component analysis', 'Factor analysis', 'Dimension', 'Dimensionality reduction', 'Nonlinear dimensionality reduction', 'Mean', 'Euclidean distance', 'Kullback–Leibler divergence', 'Gradient', 'LOBPCG', 'Cluster analysis', 'Statistics', 'Correspondence analysis', 'Sparse PCA', 'Outlier', 'Robust principal component analysis']",set(),set(),0,1.9768451519536903,0.7921876892575226
605,1208,Generalization error,http://dbpedia.org/resource/Generalization_error,http://en.wikipedia.org/wiki/Generalization_error,"For supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error or the risk) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data. Because learning algorithms are evaluated on finite samples, the evaluation of a learning algorithm may be sensitive to sampling error. As a result, measurements of prediction error on the current data may not provide much information about predictive ability on new data. Generalization error can be minimized by avoiding overfitting in the learning algorithm. The performance of a machine learning algorithm is visualized by plots that show values of estimates of the generalization error through the learning process, which are called learning curves.",4730277460374039519,related_concept,Random forest,2024-06-23 21:17:09.728,['Generalization error'],"['Generalization error', 'Overfitting']",set(),set(),0,0.6540880503144654,1.3497334562067815
606,1209,Ensemble learning,http://dbpedia.org/resource/Ensemble_learning,http://en.wikipedia.org/wiki/Ensemble_learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.",5146063291977588998,related_concept,Random forest,2024-06-23 21:17:09.728,[],"['Supervised learning', 'Ensemble learning', 'Gradient', 'Euclidean distance', 'Bayes classifier', 'Naive Bayes classifier', ""Bayes' theorem"", 'Boot', 'Random forest', 'Inference', 'Akaike information criterion', 'AI', 'Classification']",set(),set(),0,1.6050420168067228,1.029933105172859
607,1211,Cross-validation (statistics),http://dbpedia.org/resource/Cross-validation_(statistics),http://en.wikipedia.org/wiki/Cross-validation_(statistics),"Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem). One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance. In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.",1118526819560327834,main_concept,Random forest,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.3375634517766497,1.3931947788248542
608,1212,Bootstrap aggregating,http://dbpedia.org/resource/Bootstrap_aggregating,http://en.wikipedia.org/wiki/Bootstrap_aggregating,"Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.",426576745448891710,related_concept,Random forest,2024-06-23 21:17:09.728,"['Boot', 'Bootstrap aggregating']","['Boot', 'Bootstrap aggregating', 'K-nearest neighbor', 'Random forest', 'Prediction']",set(),set(),0,1.9883720930232558,1.3180963091509696
609,1213,Random subspace method,http://dbpedia.org/resource/Random_subspace_method,http://en.wikipedia.org/wiki/Random_subspace_method,"In machine learning the random subspace method, also called attribute bagging or feature bagging, is an ensemble learning method that attempts to reduce the correlation between estimators in an ensemble by training them on random samples of features instead of the entire feature set.",7340075165943076097,related_concept,Random forest,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.206896551724138,1.360439961097306
610,1214,Overfitting,http://dbpedia.org/resource/Overfitting,http://en.wikipedia.org/wiki/Overfitting,"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure. Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance. The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data. To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.",932910669230693553,related_concept,Random forest,2024-06-23 21:17:09.728,[],"['Overfitting', ""Freedman's paradox"", 'Generalization error']",set(),set(),0,1.5853658536585367,1.3916867511004596
611,1216,Kernel methods,http://dbpedia.org/resource/Kernel_methods,http://en.wikipedia.org/wiki/Kernel_methods,,2618487169870718014,related_concept,Random forest,2024-06-23 21:17:09.728,[],"['Kernel methods', 'Algorithm', 'Prediction']",set(),set(),0,0.455,0.43329936171707684
612,1217,Training set,http://dbpedia.org/resource/Training_set,http://en.wikipedia.org/wiki/Training_set,,869015975773070684,related_concept,Random forest,2024-06-23 21:17:09.728,[],['Bayes classifier'],set(),set(),0,0.36824324324324326,4.2248265587780573e-305
613,1219,Out-of-bag error,http://dbpedia.org/resource/Out-of-bag_error,http://en.wikipedia.org/wiki/Out-of-bag_error,"Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging). Bagging uses subsampling with replacement to create training samples for the model to learn from. OOB error is the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample. Bootstrap aggregating allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations that were not used in the building of the next base learner.",3859225524187592086,related_concept,Random forest,2024-06-23 21:17:09.728,"['Boot', 'Bootstrap aggregating']","['Boot', 'Bootstrap aggregating', 'Out-of-bag error']",set(),set(),0,0.3795620437956204,1.2959137969461119
614,1220,Bias-variance dilemma,http://dbpedia.org/resource/Bias-variance_dilemma,http://en.wikipedia.org/wiki/Bias-variance_dilemma,,4412697545304614607,related_concept,Random forest,2024-06-23 21:17:09.728,[],"['Accuracy', 'Dimension', 'Dimensionality reduction']",set(),set(),0,0.09278350515463918,4.2189159838529105e-305
615,1229,Gradient,http://dbpedia.org/resource/Gradient,http://en.wikipedia.org/wiki/Gradient,"In vector calculus, the gradient of a scalar-valued differentiable function f of several variables is the vector field (or vector-valued function) whose value at a point is the ""direction and rate of fastest increase"". If the gradient of a function is non-zero at a point p, the direction of the gradient is the direction in which the function increases most quickly from p, and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to maximize a function by gradient ascent. In coordinate-free terms, the gradient of a function may be defined by: where df is the total infinitesimal change in f for an infinitesimal displacement , and is seen to be maximal when is in the direction of the gradient . The nabla symbol , written as an upside-down triangle and pronounced ""del"", denotes the vector differential operator. When a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of at . That is, for , its gradient is defined at the point in n-dimensional space as the vector The gradient is dual to the total derivative : the value of the gradient at a point is a tangent vector – a vector at each point; while the value of the derivative at a point is a cotangent vector – a linear functional on vectors. They are related in that the dot product of the gradient of f at a point p with another tangent vector v equals the directional derivative of f at p of the function along v; that is, . The gradient admits multiple generalizations to more general functions on manifolds; see .",9207223955644279255,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],['Computation'],set(),set(),0,4.520900321543408,4.658445061293304e-304
616,1230,Learning rate,http://dbpedia.org/resource/Learning_rate,http://en.wikipedia.org/wiki/Learning_rate,"In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model ""learns"". In the adaptive control literature, the learning rate is commonly referred to as gain. In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum. In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms. When conducting line searches, mini-batch sub-sampling (MBSS) affect the characteristics of the loss function along which the learning rate needs to be resolved. Static MBSS keeps the mini-batch fixed along a search direction, resulting in a smooth loss function along the search direction. Dynamic MBSS updates the mini-batch at every function evaluation, resulting in a point-wise discontinuous loss function along the search direction. Line searches that adaptively resolve learning rates for static MBSS loss functions include the parabolic approximation line (PAL) search. Line searches that adaptively resolve learning rates for dynamic MBSS loss functions include probabilistic line searches, gradient-only line searches (GOLS) and quadratic approximations.",4747239088288380879,related_concept,Gradient descent,2024-06-23 21:17:09.728,"['Line search', ""Newton's method""]","[""Newton's method""]",set(),set(),0,0.3741935483870968,1.2116693283172375
617,1231,Proximal gradient method,http://dbpedia.org/resource/Proximal_gradient_method,http://en.wikipedia.org/wiki/Proximal_gradient_method,"Proximal gradient methods are a generalized form of projection used to solve non-differentiable convex optimization problems. Many interesting problems can be formulated as convex optimization problems of the form where are convex functions defined from where some of the functions are non-differentiable. This rules out conventional smooth optimization techniques likeSteepest descent method, conjugate gradient method etc. Proximal gradient methods can be used instead. These methods proceed by splitting, in that the functions are used individually so as to yield an easily implementable algorithm.They are called proximal because each non smooth function among is involved via its proximityoperator. Iterative Shrinkage thresholding algorithm, projected Landweber, projectedgradient, alternating projections, alternating-direction method of multipliers, alternatingsplit Bregman are special instances of proximal algorithms. For the theory of proximal gradient methods from the perspective of and with applications to statistical learning theory, see proximal gradient methods for learning.",7236675061274361082,related_concept,Gradient descent,2024-06-23 21:17:09.728,"['Proximal gradient method', 'Iterative']","['Proximal gradient method', 'Iterative']",set(),set(),0,0.7586206896551724,4.63443976402798e-304
618,1233,Broyden–Fletcher–Goldfarb–Shanno algorithm,http://dbpedia.org/resource/Broyden–Fletcher–Goldfarb–Shanno_algorithm,http://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm,"In numerical optimization, the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems. Like the related Davidon–Fletcher–Powell method, BFGS determines the descent direction by preconditioning the gradient with curvature information. It does so by gradually improving an approximation to the Hessian matrix of the loss function, obtained only from gradient evaluations (or approximate gradient evaluations) via a generalized secant method. Since the updates of the BFGS curvature matrix do not require matrix inversion, its computational complexity is only , compared to in Newton's method. Also in common use is L-BFGS, which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B variant handles simple box constraints. The algorithm is named after Charles George Broyden, Roger Fletcher, Donald Goldfarb and David Shanno.",399840087598250029,related_concept,Gradient descent,2024-06-23 21:17:09.728,"[""Newton's method""]","[""Newton's method"", 'Bayesian inference']",set(),set(),0,1.2173913043478262,4.564013303018481e-304
619,1234,Mirror descent,http://dbpedia.org/resource/Mirror_descent,http://en.wikipedia.org/wiki/Mirror_descent,"In mathematics, mirror descent is an iterative optimization algorithm for finding a local minimum of a differentiable function. It generalizes algorithms such as gradient descent and multiplicative weights.",855426686044077743,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],"['Mirror descent', 'Euclidean distance']",set(),set(),0,1.3263157894736841,1.0638065705378599
620,1235,Convex programming,http://dbpedia.org/resource/Convex_programming,http://en.wikipedia.org/wiki/Convex_programming,,8263344625996186402,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],"[""Newton's method""]",set(),set(),0,0.15283842794759825,0.22070261681745826
621,1237,Euler's method,http://dbpedia.org/resource/Euler's_method,http://en.wikipedia.org/wiki/Euler's_method,,8811872631856452567,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],"[""Euler's method""]",set(),set(),0,0.17391304347826086,0.41059441771903205
622,1238,Stochastic gradient descent,http://dbpedia.org/resource/Stochastic_gradient_descent,http://en.wikipedia.org/wiki/Stochastic_gradient_descent,"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in trade for a lower convergence rate. While the basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning.",8118923462963563,related_concept,Gradient descent,2024-06-23 21:17:09.728,"['Stochastic gradient descent', 'Stochastic']","['Stochastic gradient descent', 'Stochastic', 'M-estimator', 'Backpropagation', 'Mean', 'Gradient', 'Limited-memory BFGS', 'Poisson regression', 'Least squares', 'Rprop', 'Backtracking line search', 'Backtracking', 'Fisher information']",set(),set(),0,1.2016348773841963,4.668220932963362e-304
623,1239,Line search,http://dbpedia.org/resource/Line_search,http://en.wikipedia.org/wiki/Line_search,"In optimization, the line search strategy is one of two basic iterative approaches to find a local minimum of an objective function . The other approach is trust region. The line search approach first finds a descent direction along which the objective function will be reduced and then computes a step size that determines how far should move along that direction. The descent direction can be computed by various methods, such as gradient descent or quasi-Newton method. The step size can be determined either exactly or inexactly.",6045930349619369235,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.6261682242990654,1.337925196860053
624,1240,Limited-memory BFGS,http://dbpedia.org/resource/Limited-memory_BFGS,http://en.wikipedia.org/wiki/Limited-memory_BFGS,"Limited-memory BFGS (L-BFGS or LM-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS) using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning. The algorithm's target problem is to minimize over unconstrained values of the real-vector where is a differentiable scalar function. Like the original BFGS, L-BFGS uses an estimate of the inverse Hessian matrix to steer its search through variable space, but where BFGS stores a dense approximation to the inverse Hessian (n being the number of variables in the problem), L-BFGS stores only a few vectors that represent the approximation implicitly. Due to its resulting linear memory requirement, the L-BFGS method is particularly well suited for optimization problems with many variables. Instead of the inverse Hessian Hk, L-BFGS maintains a history of the past m updates of the position x and gradient ∇f(x), where generally the history size m can be small (often ). These updates are used to implicitly do operations requiring the Hk-vector product.",3178844717556747561,related_concept,Gradient descent,2024-06-23 21:17:09.728,['Limited-memory BFGS'],"['Broyden–Fletcher–Goldfarb–Shanno algorithm', 'Limited-memory BFGS']",set(),set(),0,1.3421052631578947,4.605335559844425e-304
625,1241,Mathematical optimization,http://dbpedia.org/resource/Mathematical_optimization,http://en.wikipedia.org/wiki/Mathematical_optimization,"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries. In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.",8417543146752320187,related_concept,Gradient descent,2024-06-23 21:17:09.728,"['Optimization problem', 'Mathematical optimization']","['Optimization problem', 'Mathematical optimization', 'Bayesian optimization', ""Newton's method"", 'Operations research']",set(),set(),0,2.9310344827586206,1.0695708937564767
626,1243,Hill climbing,http://dbpedia.org/resource/Hill_climbing,http://en.wikipedia.org/wiki/Hill_climbing,"In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by making an incremental change to the solution. If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found. For example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will likely be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained. Hill climbing finds optimal solutions for convex problems – for other problems it will find only local optima (solutions that cannot be improved upon by any neighboring configurations), which are not necessarily the best possible solution (the global optimum) out of all possible solutions (the search space). Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search. To attempt to avoid getting stuck in local optima, one could use restarts (i.e. repeated local search), or more complex schemes based on iterations (like iterated local search), or on memory (like reactive search optimization and tabu search), or on memory-less stochastic modifications (like simulated annealing). The relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Different choices for next nodes and starting nodes are used in related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems, so long as a small number of increments typically converges on a good solution (the optimal solution or a close approximation). At the other extreme, bubble sort can be viewed as a hill climbing algorithm (every adjacent element exchange decreases the number of disordered element pairs), yet this approach is far from efficient for even modest N, as the number of exchanges required grows quadratically. Hill climbing is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends.",7004563319597503084,related_concept,Gradient descent,2024-06-23 21:17:09.728,['Hill climbing'],"['Hill climbing', 'Stochastic']",set(),set(),0,2.261904761904762,4.3599156058515485e-304
627,1245,Backtracking line search,http://dbpedia.org/resource/Backtracking_line_search,http://en.wikipedia.org/wiki/Backtracking_line_search,"In (unconstrained) mathematical optimization, a backtracking line search is a line search method to determine the amount to move along a given search direction. Its use requires that the objective function is differentiable and that its gradient is known. The method involves starting with a relatively large estimate of the step size for movement along the line search direction, and iteratively shrinking the step size (i.e., ""backtracking"") until a decrease of the objective function is observed that adequately corresponds to the amount of decrease that is expected, based on the step size and the local gradient of the objective function. The stopping criterion is known as the Armijo–Goldstein condition. Backtracking line search is typically used for gradient descent (GD), but it can also be used in other contexts. For example, it can be used with Newton's method if the Hessian matrix is positive definite.",2720416374031893015,related_concept,Gradient descent,2024-06-23 21:17:09.728,"['Backtracking line search', ""Newton's method"", 'Backtracking']","['Backtracking line search', ""Newton's method"", 'Backtracking', 'Algorithm', 'Stochastic gradient descent', 'Stochastic']",set(),set(),0,0.6129032258064516,0.9986907875481994
628,1246,Forward–backward algorithm,http://dbpedia.org/resource/Forward–backward_algorithm,http://en.wikipedia.org/wiki/Forward–backward_algorithm,"The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e. it computes, for all hidden state variables , the distribution . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm. The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.",5802958035788603821,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],"['Viterbi algorithm', ""Bayes' theorem""]",set(),set(),0,2.4074074074074074,4.734523164743159e-304
629,1248,Conjugate gradient method,http://dbpedia.org/resource/Conjugate_gradient_method,http://en.wikipedia.org/wiki/Conjugate_gradient_method,"In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is positive-definite. The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition. Large sparse systems often arise when numerically solving partial differential equations or optimization problems. The conjugate gradient method can also be used to solve unconstrained optimization problems such as energy minimization. It is commonly attributed to Magnus Hestenes and Eduard Stiefel, who programmed it on the Z4, and extensively researched it. The biconjugate gradient method provides a generalization to non-symmetric matrices. Various nonlinear conjugate gradient methods seek minima of nonlinear optimization problems.",4672813494738385473,related_concept,Gradient descent,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.9801980198019802,4.590400517884129e-304
630,1250,Generative model,http://dbpedia.org/resource/Generative_model,http://en.wikipedia.org/wiki/Generative_model,"In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following : 1. 
* A generative model is a statistical model of the joint probability distribution on given observable variable X and target variable Y; 2. 
* A discriminative model is a model of the conditional probability of the target Y, given an observation x; and 3. 
* Classifiers computed without using a probability model are also referred to loosely as ""discriminative"". The distinction between these last two classes is not consistently made; refers to these three classes as generative learning, conditional learning, and discriminative learning, but only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model. Standard examples of each, all of which are linear classifiers, are: 
* generative classifiers: 
* naive Bayes classifier and 
* linear discriminant analysis 
* discriminative model: 
* logistic regression In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, (discriminative model), and base classification on that; or one can estimate the joint distribution (generative model), from that compute the conditional probability , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.",7714114689810964689,related_concept,Decision tree,2024-06-23 21:17:09.728,['Bayes classifier'],[],set(),set(),0,0.9973045822102425,1.40224482883647
631,1252,Boosting (machine learning),http://dbpedia.org/resource/Boosting_(machine_learning),http://en.wikipedia.org/wiki/Boosting_(machine_learning),"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): ""Can a set of weak learners create a single strong learner?"" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting. When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. ""Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner]."" Algorithms that achieve hypothesis boosting quickly became simply known as ""boosting"". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.",5934409985128637850,related_concept,Decision tree,2024-06-23 21:17:09.728,['Algorithm'],"['Algorithm', 'AdaBoost', 'BrownBoost', 'LPBoost', 'Bayes classifier', 'Naive Bayes classifier']",set(),set(),0,1.4929577464788732,0.518588664106261
632,1254,Probability,http://dbpedia.org/resource/Probability,http://en.wikipedia.org/wiki/Probability,"Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (""heads"" and ""tails"") are both equally probable; the probability of ""heads"" equals the probability of ""tails""; and since no other outcomes are possible, the probability of either ""heads"" or ""tails"" is 1/2 (which could also be written as 0.5 or 50%). These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.",2365427711184428523,related_concept,Decision tree,2024-06-23 21:17:09.728,"['Probability', 'Probability theory']","['Probability', 'Probability theory', 'Conditional probability']",{'Probability'},set(),0,8.0,0.7126744132034275
633,1255,Decision analysis,http://dbpedia.org/resource/Decision_analysis,http://en.wikipedia.org/wiki/Decision_analysis,"Decision analysis (DA) is the discipline comprising the philosophy, methodology, and professional practice necessary to address important decisions in a formal manner. Decision analysis includes many procedures, methods, and tools for identifying, clearly representing, and formally assessing important aspects of a decision; for prescribing a recommended course of action by applying the maximum expected-utility axiom to a well-formed representation of the decision; and for translating the formal representation of a decision and its corresponding recommendation into insight for the decision maker, and other corporate and non-corporate stakeholders.",6649508524797168197,related_concept,Decision tree,2024-06-23 21:17:09.728,['Decision analysis'],"['Decision analysis', 'Bayesian inference']",set(),set(),0,1.935483870967742,1.4366964887351072
634,1256,"Behavior tree (artificial intelligence, robotics and control)","http://dbpedia.org/resource/Behavior_tree_(artificial_intelligence,_robotics_and_control)","http://en.wikipedia.org/wiki/Behavior_tree_(artificial_intelligence,_robotics_and_control)","A behavior tree is a mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. Behavior trees present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make behavior trees less error prone and very popular in the game developer community. Behavior trees have been shown to generalize several other control architectures.",8596272024000154063,related_concept,Decision tree,2024-06-23 21:17:09.728,[],['AI'],set(),set(),0,2.09375,1.087349280141873
635,1257,Decision table,http://dbpedia.org/resource/Decision_table,http://en.wikipedia.org/wiki/Decision_table,Decision tables are a concise visual representation for specifying which actions to perform depending on given conditions. They are algorithms whose output is a set of actions. The information expressed in decision tables could also be represented as decision trees or in a programming language as a series of if-then-else and switch-case statements.,3272227349518351331,related_concept,Decision tree,2024-06-23 21:17:09.728,['Decision table'],"['Business process', 'Business process modeling', 'Decision table']",set(),set(),0,3.5357142857142856,1.4350328072299612
636,1258,Decision matrix,http://dbpedia.org/resource/Decision_matrix,http://en.wikipedia.org/wiki/Decision_matrix,"A decision matrix is a list of values in rows and columns that allows an analyst to systematically identify, analyze, and rate the performance of relationships between sets of values and information. Elements of a decision matrix show decisions based on certain decision criteria. The matrix is useful for looking at large masses of decision factors and assessing each factor's relative significance by weighting them by importance.",9108826437395875524,related_concept,Decision tree,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1282051282051282,1.4360653150029912
637,1259,Information gain in decision trees,http://dbpedia.org/resource/Information_gain_in_decision_trees,http://en.wikipedia.org/wiki/Information_gain_in_decision_trees,,3581084536411086611,related_concept,Decision tree,2024-06-23 21:17:09.728,[],['Kullback–Leibler divergence'],set(),set(),0,1.5964912280701755,0.5526893468714003
638,1260,Ordinal priority approach,http://dbpedia.org/resource/Ordinal_priority_approach,http://en.wikipedia.org/wiki/Ordinal_priority_approach,Ordinal priority approach (OPA) is a multiple-criteria decision analysis method that aids in solving the group decision-making problems based on preference relations.,580348345098689139,related_concept,Decision tree,2024-06-23 21:17:09.728,['Ordinal priority approach'],"['Ordinal priority approach', 'MATLAB', 'Information technology']",set(),set(),0,0.32727272727272727,1.4416598148195152
639,1262,Random forest,http://dbpedia.org/resource/Random_forest,http://en.wikipedia.org/wiki/Random_forest,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance. The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg. An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance. Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.",4906330166298674383,main_concept,Decision tree,2024-06-23 21:17:09.728,['Random forest'],"['Random forest', 'Decision tree', 'Classification', 'Mean', 'Bayes classifier', 'Prediction']",set(),set(),0,2.7204301075268815,1.0380700111797645
640,1263,Decision support system,http://dbpedia.org/resource/Decision_support_system,http://en.wikipedia.org/wiki/Decision_support_system,"A decision support system (DSS) is an information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both. While academics have perceived DSS as a tool to support decision making processes, DSS users see DSS as a tool to facilitate organizational processes. Some authors have extended the definition of DSS to include any system that might support decision making and some DSS include a decision-making software component; Sprague (1980) defines a properly termed DSS as follows: 1. 
* DSS tends to be aimed at the less well structured, underspecified problem that upper level managers typically face; 2. 
* DSS attempts to combine the use of models or analytic techniques with traditional data access and retrieval functions; 3. 
* DSS specifically focuses on features which make them easy to use by non-computer-proficient people in an interactive mode; and 4. 
* DSS emphasizes flexibility and adaptability to accommodate changes in the environment and the decision making approach of the user. DSSs include knowledge-based systems. A properly designed DSS is an interactive software-based system intended to help decision makers compile useful information from a combination of raw data, documents, and personal knowledge, or business models to identify and solve problems and make decisions. Typical information that a decision support application might gather and present includes: 
* inventories of information assets (including legacy and relational data sources, cubes, data warehouses, and data marts), 
* comparative sales figures between one period and the next, 
* projected revenue figures based on product sales assumptions.",8791424071165801958,related_concept,Decision tree,2024-06-23 21:17:09.728,['Decision support system'],"['Decision support system', 'AI']",set(),set(),0,2.949748743718593,1.3598160055526678
641,1264,Operations research,http://dbpedia.org/resource/Operations_research,http://en.wikipedia.org/wiki/Operations_research,"Operations research (British English: operational research), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. It is considered to be a subfield of mathematical sciences. The term management science is occasionally used as a synonym. Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlap with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.",3220821046762794181,related_concept,Decision tree,2024-06-23 21:17:09.728,['Operations research'],"['Operations research', 'Econometric']",set(),set(),0,4.15817223198594,1.429188749148544
642,1265,Odds algorithm,http://dbpedia.org/resource/Odds_algorithm,http://en.wikipedia.org/wiki/Odds_algorithm,"The odds algorithm (or Bruss algorithm) is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Their solution follows from the odds strategy, and the importance of the odds strategy lies in its optimality, as explained below. The odds algorithm applies to a class of problems called last-success problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a ""specific event""). This identification must be done at the time of observation. No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of ""stopping"" to take a well-defined action. Such problems are encountered in several situations.",542426317912072418,related_concept,Decision tree,2024-06-23 21:17:09.728,[],['Theorem'],set(),set(),0,2.642857142857143,1.4398142467928359
643,1266,Decision tree model,http://dbpedia.org/resource/Decision_tree_model,http://en.wikipedia.org/wiki/Decision_tree_model,"In computational complexity the decision tree model is the model of computation in which an algorithm is considered to be basically a decision tree, i.e., a sequence of queries or tests that are done adaptively, so the outcome of the previous tests can influence the test is performed next. Typically, these tests have a small number of outcomes (such as a yes–no question) and can be performed quickly (say, with unit computational cost), so the worst-case time complexity of an algorithm in the decision tree model corresponds to the depth of the corresponding decision tree. This notion of computational complexity of a problem or an algorithm in the decision tree model is called its decision tree complexity or query complexity. Decision trees models are instrumental in establishing lower bounds for complexity theory for certain classes of computational problems and algorithms. Several variants of decision tree models have been introduced, depending on the computational model and type of query algorithms are allowed to perform. For example, a decision tree argument is used to show that a comparison sort of items must take comparisons. For comparison sorts, a query is a comparison of two items , with two outcomes (assuming no items are equal): either or . Comparison sorts can be expressed as a decision tree in this model, since such sorting algorithms only perform these types of queries.",1254044176086218599,related_concept,Decision tree,2024-06-23 21:17:09.728,['Decision tree'],"['Decision tree', 'Algorithm', 'Boolean function']",set(),set(),0,2.1842105263157894,1.4288967655760505
644,1267,Decision cycle,http://dbpedia.org/resource/Decision_cycle,http://en.wikipedia.org/wiki/Decision_cycle,"A decision cycle is a sequence of steps used by an entity on a repeated basis to reach and implement decisions and to learn from the results. The ""decision cycle"" phrase has a history of use to broadly categorize various methods of making decisions, going upstream to the need, downstream to the outcomes, and cycling around to connect the outcomes to the needs.",829283309307514946,related_concept,Decision tree,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1875,1.4365950501337168
645,1268,Decision list,http://dbpedia.org/resource/Decision_list,http://en.wikipedia.org/wiki/Decision_list,"Decision lists are a representation for Boolean functions which can be easily learnable from examples. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form. The language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree. Learning decision lists can be used for .",8127876963419282355,related_concept,Decision tree,2024-06-23 21:17:09.728,"['Boolean function', 'Decision list']","['Boolean function', 'Decision list']",set(),set(),0,5.8,1.4329964711695677
646,1271,Universal approximation theorem,http://dbpedia.org/resource/Universal_approximation_theorem,http://en.wikipedia.org/wiki/Universal_approximation_theorem,"In the mathematical theory of artificial neural networks, universal approximation theorems are results that establish the density of an algorithmically generated class of functions within a given function space of interest. Typically, these results concern the approximation capabilities of the feedforward architecture on the space of continuous functions between two Euclidean spaces, and the approximation is with respect to the compact convergence topology. However, there are also a variety of results between non-Euclidean spaces and other commonly used architectures and, more generally, algorithmically generated sets of functions, such as the convolutional neural network (CNN) architecture, radial basis-functions, or neural networks with specific properties. Most universal approximation theorems can be parsed into two classes. The first quantifies the approximation capabilities of neural networks with an arbitrary number of artificial neurons (""arbitrary width"" case) and the second focuses on the case with an arbitrary number of hidden layers, each containing a limited number of artificial neurons (""arbitrary depth"" case). In addition to these two classes, there are also universal approximation theorems for neural networks with bounded number of hidden layers and a limited number of neurons in each layer (""bounded depth and bounded width"" case). Universal approximation theorems imply that neural networks can represent a wide variety of interesting functions when given appropriate weights. On the other hand, they typically do not provide a construction for the weights, but merely state that such a construction is possible.",6790935060614355041,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,['Universal approximation theorem'],"['Universal approximation theorem', 'Artificial neural network']",set(),set(),0,0.545,0.6023375147644069
647,1272,Early stopping,http://dbpedia.org/resource/Early_stopping,http://en.wikipedia.org/wiki/Early_stopping,"In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.",4886379965925955585,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,['Early stopping'],"['Early stopping', 'Overfitting', 'Machine learning', 'Gradient', 'Gradient descent', 'AdaBoost']",set(),set(),0,2.3214285714285716,1.3897388145290746
648,1273,Computational learning theory,http://dbpedia.org/resource/Computational_learning_theory,http://en.wikipedia.org/wiki/Computational_learning_theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.",1763207954017043129,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,[],['Bayesian inference'],set(),set(),0,1.8491228070175438,1.371335898734536
649,1274,Delta rule,http://dbpedia.org/resource/Delta_rule,http://en.wikipedia.org/wiki/Delta_rule,"In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. For a neuron with activation function , the delta rule for neuron 's th weight is given by , where It holds that and . The delta rule is commonly stated in simplified form for a neuron with a linear activation function as While the delta rule is similar to the perceptron's update rule, the derivation is different. The perceptron uses the Heaviside step function as the activation function , and that means that does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the delta rule impossible.",5393224331148799236,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.826086956521739,1.218227096812817
650,1278,Rprop,http://dbpedia.org/resource/Rprop,http://en.wikipedia.org/wiki/Rprop,"Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992. Similarly to the , Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each ""weight"". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor η−, where η− < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of η+, where η+ > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. η+ is empirically set to 1.2 and η− to 0.5. RPROP is a . Next to the and the Levenberg–Marquardt algorithm, Rprop is one of the fastest weight update mechanisms.",5961819371442776622,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,['Rprop'],['Rprop'],set(),set(),0,2.789473684210526,1.131769168128606
651,1280,Recurrent neural network,http://dbpedia.org/resource/Recurrent_neural_network,http://en.wikipedia.org/wiki/Recurrent_neural_network,"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).",3716881588021688260,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,['Recurrent neural network'],"['Recurrent neural network', 'Hopfield network', 'Classification', 'Neural network', 'Gradient', 'Gradient descent', 'Biological neural network']",set(),set(),0,1.6227390180878554,3.707058943698371e-304
652,1282,Hopfield network,http://dbpedia.org/resource/Hopfield_network,http://en.wikipedia.org/wiki/Hopfield_network,"A Hopfield network (or Ising model of a neural network or Ising–Lenz–Little model) is a form of recurrent artificial neural network and a type of spin glass system popularised by John Hopfield in 1982 as described earlier by Little in 1974 based on Ernst Ising's work with Wilhelm Lenz on the Ising model. Hopfield networks serve as content-addressable (""associative"") memory systems with binary threshold nodes, or with continuous variables. Hopfield networks also provide a model for understanding human memory.",7443714766278297253,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,['Hopfield network'],"['Hopfield network', 'Markov network', 'Perceptron', 'AI', 'Biological neural network']",set(),set(),0,0.3103448275862069,0.5472215153778814
653,1283,Feed forward (control),http://dbpedia.org/resource/Feed_forward_(control),http://en.wikipedia.org/wiki/Feed_forward_(control),"A feed forward (sometimes written feedforward) is an element or pathway within a control system that passes a controlling signal from a source in its external environment to a load elsewhere in its external environment. This is often a command signal from an external operator. A control system which has only feed-forward behavior responds to its control signal in a pre-defined way without responding to the way the load reacts; it is in contrast with a system that also has feedback, which adjusts the input to take account of how it affects the load, and how the load itself may vary unpredictably; the load is considered to belong to the external environment of the system. In a feed-forward system, the control variable adjustment is not error-based. Instead it is based on knowledge about the process in the form of a mathematical model of the process and knowledge about, or measurements of, the process disturbances. Some prerequisites are needed for control scheme to be reliable by pure feed-forward without feedback: the external command or controlling signal must be available, and the effect of the output of the system on the load should be known (that usually means that the load must be predictably unchanging with time). Sometimes pure feed-forward control without feedback is called 'ballistic', because once a control signal has been sent, it cannot be further adjusted; any corrective adjustment must be by way of a new control signal. In contrast, 'cruise control' adjusts the output in response to the load that it encounters, by a feedback mechanism. These systems could relate to control theory, physiology, or computing.",443852819541414870,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.5748031496062993,1.1396217703226865
654,1288,Radial basis function network,http://dbpedia.org/resource/Radial_basis_function_network,http://en.wikipedia.org/wiki/Radial_basis_function_network,"In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.",6685874638015326857,related_concept,Feedforward neural network,2024-06-23 21:17:09.728,"['Radial basis function network', 'Radar']","['Radial basis function network', 'Radar', 'Euclidean distance', 'Algorithm', ""Newton's method""]",set(),set(),0,2.1636363636363636,3.811361401917529e-304
655,1293,Information-theoretic,http://dbpedia.org/resource/Information-theoretic,http://en.wikipedia.org/wiki/Information-theoretic,,3460975257387394903,related_concept,Biclustering,2024-06-23 21:17:09.728,[],"['Mutual information', 'Kullback–Leibler divergence', 'Pseudorandom number generator']",set(),set(),0,0.023529411764705882,0.5433721218295674
656,1295,Kullback–Leibler divergence,http://dbpedia.org/resource/Kullback–Leibler_divergence,http://en.wikipedia.org/wiki/Kullback–Leibler_divergence,"In mathematical statistics, the Kullback–Leibler divergence (also called relative entropy and I-divergence), denoted , is a type of statistical distance: a measure of how one probability distribution P is different from a second, reference probability distribution Q. A simple of the KL divergence of P from Q is the expected excess surprise from using Q as a model when the actual distribution is P. While it is a distance, it is not a metric, the most familiar type of distance: it is not symmetric in the two distributions (in contrast to variation of information), and does not satisfy the triangle inequality. Instead, in terms of information geometry, it is a type of divergence, a generalization of squared distance, and for certain classes of distributions (notably an exponential family), it satisfies a generalized Pythagorean theorem (which applies to squared distances). In the simple case, a relative entropy of 0 indicates that the two distributions in question have identical quantities of information. Relative entropy is a nonnegative function of two distributions or measures. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience and bioinformatics.",4707888711558307119,related_concept,Biclustering,2024-06-23 21:17:09.728,[],"['Kullback–Leibler divergence', 'Bayesian inference', 'Neyman–Pearson lemma', 'Fisher information', 'Inference', 'Theorem', 'Mean', 'Bayesian statistics', ""Bayes' theorem"", 'Akaike information criterion']",set(),set(),0,1.5161290322580645,5.132268773764128e-304
657,1299,Data type,http://dbpedia.org/resource/Data_type,http://en.wikipedia.org/wiki/Data_type,"In computer science and computer programming, a data type (or simply type) is a set of possible values and a set of allowed operations on it. A data type tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type constrains the possible values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored.",1046314446557904665,main_concept,Biclustering,2024-06-23 21:17:09.728,[],"['Data', 'Data type', 'Abstract data type']",set(),set(),0,3.3300492610837438,4.0463923987165096e-304
658,1300,Information content,http://dbpedia.org/resource/Information_content,http://en.wikipedia.org/wiki/Information_content,"In information theory, the information content, self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable. It can be thought of as an alternative way of expressing probability, much like odds or log-odds, but which has particular mathematical advantages in the setting of information theory. The Shannon information can be interpreted as quantifying the level of ""surprise"" of a particular outcome. As it is such a basic quantity, it also appears in several other settings, such as the length of a message needed to transmit the event given an optimal source coding of the random variable. The Shannon information is closely related to entropy, which is the expected value of the self-information of a random variable, quantifying how surprising the random variable is ""on average"". This is the average amount of self-information an observer would expect to gain about a random variable when measuring it. The information content can be expressed in various units of information, of which the most common is the ""bit"" (more correctly called the shannon), as explained below.",7039409203736211761,related_concept,Biclustering,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.6601941747572816,5.073873430555711e-304
659,1305,Iterative,http://dbpedia.org/resource/Iterative,http://en.wikipedia.org/wiki/Iterative,,2088815821595631808,related_concept,Biclustering,2024-06-23 21:17:09.728,[],"[""Newton's method""]",set(),set(),0,2.6,0.6047674578853256
660,1306,Suffix tree,http://dbpedia.org/resource/Suffix_tree,http://en.wikipedia.org/wiki/Suffix_tree,"In computer science, a suffix tree (also called PAT tree or, in an earlier form, position tree) is a compressed trie containing all the suffixes of the given text as their keys and positions in the text as their values. Suffix trees allow particularly fast implementations of many important string operations. The construction of such a tree for the string takes time and space linear in the length of . Once constructed, several operations can be performed quickly, for instance locating a substring in , locating a substring if a certain number of mistakes are allowed, locating matches for a regular expression pattern etc. Suffix trees also provide one of the first linear-time solutions for the longest common substring problem. These speedups come at a cost: storing a string's suffix tree typically requires significantly more space than storing the string itself.",724428518900449304,related_concept,Biclustering,2024-06-23 21:17:09.728,['Suffix tree'],"['Suffix tree', 'Algorithm']",set(),set(),0,1.2097560975609756,1.3654098117548144
661,1307,Text mining,http://dbpedia.org/resource/Text_mining,http://en.wikipedia.org/wiki/Text_mining,"Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources."" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information. A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.The document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.",719384314912933470,related_concept,Biclustering,2024-06-23 21:17:09.728,"['Database', 'Data', 'Text mining']","['Text mining', 'Computation']",set(),set(),0,2.7338403041825097,1.405044996651242
662,1308,Mutual information,http://dbpedia.org/resource/Mutual_information,http://en.wikipedia.org/wiki/Mutual_information,"In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the ""amount of information"" (in units such as shannons (bits), nats or hartleys) obtained about one random variable by observing the other random variable. The concept of mutual information is intimately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected ""amount of information"" held in a random variable. Not limited to real-valued random variables and linear dependence like the correlation coefficient, MI is more general and determines how different the joint distribution of the pair is from the product of the marginal distributions of and . MI is the expected value of the pointwise mutual information (PMI). The quantity was defined and analyzed by Claude Shannon in his landmark paper ""A Mathematical Theory of Communication"", although he did not call it ""mutual information"". This term was coined later by Robert Fano. Mutual Information is also known as information gain.",8843524027263275715,related_concept,Biclustering,2024-06-23 21:17:09.728,[],"['Kullback–Leibler divergence', 'Mutual information', 'Pearson correlation coefficient', 'Kolmogorov complexity', ""Pearson's chi-squared test""]",set(),set(),0,2.088050314465409,5.078346857074863e-304
663,1309,Matrix (mathematics),http://dbpedia.org/resource/Matrix_(mathematics),http://en.wikipedia.org/wiki/Matrix_(mathematics),"In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object. For example, is a matrix with two rows and three columns. This is often referred to as a ""two by three matrix"", a ""2×3-matrix"", or a matrix of dimension 2×3. Without further specifications, matrices represent linear maps, and allow explicit computations in linear algebra. Therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. For example, matrix multiplication represents composition of linear maps. Not all matrices are related to linear algebra. This is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices. This article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such. Square matrices, matrices with the same number of rows and columns, play a major role in matrix theory. Square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. The determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant. In geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. In numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this often involves computing with matrices of huge dimension. Matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis.",3654680997114299606,related_concept,Biclustering,2024-06-23 21:17:09.728,[],"['MATLAB', 'Text mining', 'Partial differential equation', 'Stochastic', 'Descriptive statistics', 'Statistics']",set(),set(),0,4.211438474870017,1.4005941734527148
664,1311,Biclustering,http://dbpedia.org/resource/Biclustering,http://en.wikipedia.org/wiki/Biclustering,"Biclustering, block clustering, Co-clustering or Two-mode clustering is a data mining technique which allows simultaneous clustering of the rows and columns of a matrix.The term was first introduced by Boris Mirkin to name a technique introduced many years earlier, in 1972, by J. A. Hartigan. Given a set of samples represented by an -dimensional feature vector, the entire dataset can be represented as rows in columns (i.e., an matrix). The Biclustering algorithm generates Biclusters, a subset of rows which exhibit similar behavior across a subset of columns, or vice versa.",1780686450771663552,main_concept,,2024-06-23 21:17:09.728,['Biclustering'],"['Biclustering', 'Factor analysis', 'Algorithm', 'Information-theoretic']",{'Bioinformatics'},set(),0,1.6666666666666667,1.2365403342159793
665,1312,Bayes classifier,http://dbpedia.org/resource/Bayes_classifier,http://en.wikipedia.org/wiki/Bayes_classifier,"In statistical classification, the Bayes classifier minimizes the probability of misclassification.",8651263563170542523,related_concept,Naive Bayes classifier,2024-06-23 21:17:09.728,['Bayes classifier'],['Bayes classifier'],set(),set(),0,0.5106382978723404,1.411795582494905
666,1313,AODE,http://dbpedia.org/resource/AODE,http://en.wikipedia.org/wiki/AODE,,7870371524101942544,related_concept,Naive Bayes classifier,2024-06-23 21:17:09.728,[],"['Bayes classifier', 'AODE', 'Estimator']",set(),set(),0,1.0,0.6594751162866997
667,1325,Boosted trees,http://dbpedia.org/resource/Boosted_trees,http://en.wikipedia.org/wiki/Boosted_trees,,6894903431998769307,related_concept,Naive Bayes classifier,2024-06-23 21:17:09.728,[],"['Gradient', 'Algorithm']",set(),set(),0,0.005747126436781609,0.6835706397036271
668,1326,Bayesian spam filtering,http://dbpedia.org/resource/Bayesian_spam_filtering,http://en.wikipedia.org/wiki/Bayesian_spam_filtering,,2496155145483797450,related_concept,Naive Bayes classifier,2024-06-23 21:17:09.728,[],"['Bayes classifier', 'Naive Bayes classifier', ""Bayes' theorem"", 'Bayesian spam filtering', 'Statistics']",set(),set(),0,0.3319838056680162,0.4638467206860324
669,1333,Multinomial distribution,http://dbpedia.org/resource/Multinomial_distribution,http://en.wikipedia.org/wiki/Multinomial_distribution,"In probability theory, the multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for each side of a k-sided dice rolled n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories. When k is 2 and n is 1, the multinomial distribution is the Bernoulli distribution. When k is 2 and n is bigger than 1, it is the binomial distribution. When k is bigger than 2 and n is 1, it is the categorical distribution. The term ""multinoulli"" is sometimes used for the categorical distribution to emphasize this four-way relationship (so n determines the prefix, and k the suffix). The Bernoulli distribution models the outcome of a single Bernoulli trial. In other words, it models whether flipping a (possibly biased) coin one time will result in either a success (obtaining a head) or failure (obtaining a tail). The binomial distribution generalizes this to the number of heads from performing n independent flips (Bernoulli trials) of the same coin. The multinomial distribution models the outcome of n experiments, where the outcome of each trial has a categorical distribution, such as rolling a k-sided dice n times. Let k be a fixed finite number. Mathematically, we have k possible mutually exclusive outcomes, with corresponding probabilities p1, ..., pk, and n independent trials. Since the k outcomes are mutually exclusive and one must occur we have pi ≥ 0 for i = 1, ..., k and . Then if the random variables Xi indicate the number of times outcome number i is observed over the n trials, the vector X = (X1, ..., Xk) follows a multinomial distribution with parameters n and p, where p = (p1, ..., pk). While the trials are independent, their outcomes Xi are dependent because they must be summed to n.",7572554582453289982,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,['Bernoulli distribution'],"['Bernoulli distribution', 'Kullback–Leibler divergence', 'Theorem', ""Pearson's chi-squared test"", 'Euclidean distance', 'Bayesian estimator', 'Jeffreys prior', 'Confidence interval']",set(),set(),0,1.494296577946768,1.305475473466894
670,1335,Random vector,http://dbpedia.org/resource/Random_vector,http://en.wikipedia.org/wiki/Random_vector,,9052235185982279213,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,[],['Random vector'],set(),set(),0,0.9120879120879121,0.6970586728919058
671,1338,Multivariate hypergeometric distribution,http://dbpedia.org/resource/Multivariate_hypergeometric_distribution,http://en.wikipedia.org/wiki/Multivariate_hypergeometric_distribution,,4294177483093807925,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.03461538461538462,0.5930996698653405
672,1342,Negative multinomial distribution,http://dbpedia.org/resource/Negative_multinomial_distribution,http://en.wikipedia.org/wiki/Negative_multinomial_distribution,"In probability theory and statistics, the negative multinomial distribution is a generalization of the negative binomial distribution (NB(x0, p)) to more than two outcomes. As with the univariate negative binomial distribution, if the parameter is a positive integer, the negative multinomial distribution has an urn model interpretation. Suppose we have an experiment that generates m+1≥2 possible outcomes, {X0,...,Xm}, each occurring with non-negative probabilities {p0,...,pm} respectively. If sampling proceeded until n observations were made, then {X0,...,Xm} would have been multinomially distributed. However, if the experiment is stopped once X0 reaches the predetermined value x0 (assuming x0 is a positive integer), then the distribution of the m-tuple {X1,...,Xm} is negative multinomial. These variables are not multinomially distributed because their sum X1+...+Xm is not fixed, being a draw from a negative binomial distribution.",6321715083279206607,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,[],['Log-linear model'],set(),set(),0,1.2455357142857142,1.3174974501050234
673,1345,Chain rule (probability),http://dbpedia.org/resource/Chain_rule_(probability),http://en.wikipedia.org/wiki/Chain_rule_(probability),"In probability theory, the chain rule (also called the general product rule) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities.",1906307503631513156,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,['Bayesian network'],['Bayesian network'],set(),set(),0,1.7142857142857142,1.3464391237104338
674,1346,Pairwise independence,http://dbpedia.org/resource/Pairwise_independence,http://en.wikipedia.org/wiki/Pairwise_independence,"In probability theory, a pairwise independent collection of random variables is a set of random variables any two of which are independent. Any collection of mutually independent random variables is pairwise independent, but some pairwise independent collections are not mutually independent. Pairwise independent random variables with finite variance are uncorrelated. A pair of random variables X and Y are independent if and only if the random vector (X, Y) with joint cumulative distribution function (CDF) satisfies or equivalently, their joint density satisfies That is, the joint distribution is equal to the product of the marginal distributions. Unless it is not clear in context, in practice the modifier ""mutual"" is usually dropped so that independence means mutual independence. A statement such as "" X, Y, Z are independent random variables"" means that X, Y, Z are mutually independent.",4583322735684521412,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,[],['Pairwise independence'],set(),set(),0,1.205128205128205,1.2943643492469648
675,1347,Elliptical distribution,http://dbpedia.org/resource/Elliptical_distribution,http://en.wikipedia.org/wiki/Elliptical_distribution,"In probability and statistics, an elliptical distribution is any member of a broad family of probability distributions that generalize the multivariate normal distribution. Intuitively, in the simplified two and three dimensional case, the joint distribution forms an ellipse and an ellipsoid, respectively, in iso-density plots. In statistics, the normal distribution is used in classical multivariate analysis, while elliptical distributions are used in generalized multivariate analysis, for the study of symmetric distributions with tails that are heavy, like the multivariate t-distribution, or light (in comparison with the normal distribution). Some statistical methods that were originally motivated by the study of the normal distribution have good performance for general elliptical distributions (with finite variance), particularly for spherical distributions (which are defined below). Elliptical distributions are also used in robust statistics to evaluate proposed multivariate-statistical procedures.",2490133133523311046,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,['Elliptical distribution'],"['Elliptical distribution', 'Computation', 'Cauchy distribution']",set(),set(),0,1.3920145190562614,1.2785970443869865
676,1348,Bernoulli distribution,http://dbpedia.org/resource/Bernoulli_distribution,http://en.wikipedia.org/wiki/Bernoulli_distribution,"In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability and the value 0 with probability . Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. Such questions lead to outcomes that are boolean-valued: a single bit whose value is success/yes/true/one with probability p and failure/no/false/zero with probability q. It can be used to represent a (possibly biased) coin toss where 1 and 0 would represent ""heads"" and ""tails"", respectively, and p would be the probability of the coin landing on heads (or vice versa where 1 would represent tails and p would be the probability of tails). In particular, unfair coins would have The Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1.",8377472171565650702,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,['Bernoulli distribution'],['Bernoulli distribution'],set(),set(),0,1.8923611111111112,1.279685088146968
677,1352,Multivariate stable distribution,http://dbpedia.org/resource/Multivariate_stable_distribution,http://en.wikipedia.org/wiki/Multivariate_stable_distribution,"The multivariate stable distribution is a multivariate probability distribution that is a multivariate generalisation of the univariate stable distribution. The multivariate stable distribution defines linear relations between stable distribution marginals. In the same way as for the univariate case, the distribution is defined in terms of its characteristic function. The multivariate stable distribution can also be thought as an extension of the multivariate normal distribution. It has parameter, α, which is defined over the range 0 < α ≤ 2, and where the case α = 2 is equivalent to the multivariate normal distribution. It has an additional skew parameter that allows for non-symmetric distributions, where the multivariate normal distribution is symmetric.",5865370062546786957,related_concept,Joint probability distribution,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2710280373831775,5.822956987386525e-304
678,1354,Independent identically distributed,http://dbpedia.org/resource/Independent_identically_distributed,http://en.wikipedia.org/wiki/Independent_identically_distributed,,1421415735159455126,related_concept,Mixture model,2024-06-23 21:17:09.728,[],"['Statistics', 'Machine learning']",set(),set(),0,0.1124031007751938,0.6304251342821964
679,1355,Concentration parameter,http://dbpedia.org/resource/Concentration_parameter,http://en.wikipedia.org/wiki/Concentration_parameter,"In probability theory and statistics, a concentration parameter is a special kind of numerical parameter of a parametric family of probability distributions. Concentration parameters occur in two kinds of distribution: In the Von Mises–Fisher distribution, and in conjunction with distributions whose domain is a probability distribution, such as the symmetric Dirichlet distribution and the Dirichlet process. The rest of this article focuses on the latter usage. The larger the value of the concentration parameter, the more evenly distributed is the resulting distribution (the more it tends towards the uniform distribution). The smaller the value of the concentration parameter, the more sparsely distributed is the resulting distribution, with most values or ranges of values having a probability near zero (in other words, the more it tends towards a distribution concentrated on a single point, the degenerate distribution defined by the Dirac delta function).",38773125288270016,related_concept,Mixture model,2024-06-23 21:17:09.728,['Concentration parameter'],"['Concentration parameter', 'Mean']",set(),set(),0,0.6129032258064516,1.2432462330067986
680,1359,Copula (statistics),http://dbpedia.org/resource/Copula_(statistics),http://en.wikipedia.org/wiki/Copula_(statistics),,3243979098610153327,related_concept,Mixture model,2024-06-23 21:17:09.728,[],['PDF'],set(),set(),0,0.10657596371882086,0.4918098615313164
681,1361,Categorical distribution,http://dbpedia.org/resource/Categorical_distribution,http://en.wikipedia.org/wiki/Categorical_distribution,"In probability theory and statistics, a categorical distribution (also called a generalized Bernoulli distribution, multinoulli distribution) is a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each category separately specified. There is no innate underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution, (e.g. 1 to K). The K-dimensional categorical distribution is the most general distribution over a K-way event; any other discrete distribution over a size-K sample space is a special case. The parameters specifying the probabilities of each possible outcome are constrained only by the fact that each must be in the range 0 to 1, and all must sum to 1. The categorical distribution is the generalization of the Bernoulli distribution for a categorical random variable, i.e. for a discrete variable with more than two possible outcomes, such as the roll of a die. On the other hand, the categorical distribution is a special case of the multinomial distribution, in that it gives the probabilities of potential outcomes of a single drawing rather than multiple drawings.",7850467236913511300,related_concept,Mixture model,2024-06-23 21:17:09.728,['Bernoulli distribution'],"['Bernoulli distribution', 'Gibbs sampling', 'Bayesian statistics', 'Logically']",set(),set(),0,1.2548262548262548,1.2763928028601486
682,1363,Mixture (probability),http://dbpedia.org/resource/Mixture_(probability),http://en.wikipedia.org/wiki/Mixture_(probability),"In probability theory and statistics, a mixture is a probabilistic combination of two or more probability distributions. The concept arises mostly in two contexts: 
* A mixture defining a new probability distribution from some existing ones, as in a mixture distribution or a compound distribution. Here a major problem often is to derive the properties of the resulting distribution. 
* A mixture used as a statistical model such as is often used for statistical classification. The model may represent the population from which observations arise as a mixture of several components, and the problem is that of a mixture model, in which the task is to infer from which of a discrete set of sub-populations each observation originated.",3819082783183025154,related_concept,Mixture model,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.105263157894737,1.124143114760719
683,1365,Mixture distribution,http://dbpedia.org/resource/Mixture_distribution,http://en.wikipedia.org/wiki/Mixture_distribution,"In probability and statistics, a mixture distribution is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized. The underlying random variables may be random real numbers, or they may be random vectors (each having the same dimension), in which case the mixture distribution is a multivariate distribution. In cases where each of the underlying random variables is continuous, the outcome variable will also be continuous and its probability density function is sometimes referred to as a mixture density. The cumulative distribution function (and the probability density function if it exists) can be expressed as a convex combination (i.e. a weighted sum, with non-negative weights that sum to 1) of other distribution functions and density functions. The individual distributions that are combined to form the mixture distribution are called the mixture components, and the probabilities (or weights) associated with each component are called the mixture weights. The number of components in a mixture distribution is often restricted to being finite, although in some cases the components may be countably infinite in number. More general cases (i.e. an uncountable set of component distributions), as well as the countable case, are treated under the title of compound distributions. A distinction needs to be made between a random variable whose distribution function or density is the sum of a set of components (i.e. a mixture distribution) and a random variable whose value is the sum of the values of two or more underlying random variables, in which case the distribution is given by the convolution operator. As an example, the sum of two jointly normally distributed random variables, each with different means, will still have a normal distribution. On the other hand, a mixture density created as a mixture of two normal distributions with different means will have two peaks provided that the two means are far enough apart, showing that this distribution is radically different from a normal distribution. Mixture distributions arise in many contexts in the literature and arise naturally where a statistical population contains two or more subpopulations. They are also sometimes used as a means of representing non-normal distributions. Data analysis concerning statistical models involving mixture distributions is discussed under the title of mixture models, while the present article concentrates on simple probabilistic and statistical properties of mixture distributions and how these relate to properties of the underlying distributions.",3649521140219757066,related_concept,Mixture model,2024-06-23 21:17:09.728,"['Data', 'Data analysis', 'Mixture distribution']","['Data', 'Data analysis', 'Mixture distribution']",set(),set(),0,4.690140845070423,4.761099289530517e-304
684,1366,Gibbs sampling,http://dbpedia.org/resource/Gibbs_sampling,http://en.wikipedia.org/wiki/Gibbs_sampling,"In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled. Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM). As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired. Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded.",1382778759114682527,related_concept,Mixture model,2024-06-23 21:17:09.728,"['Gibbs sampling', 'Bayesian inference']","['Gibbs sampling', 'Bayesian inference', 'Bayesian network', 'Supervised learning', 'Generalized linear models', 'Bayesian statistics', ""Student's t-distribution""]",set(),set(),0,1.3092105263157894,0.9848065430860515
685,1376,Linear classifier,http://dbpedia.org/resource/Linear_classifier,http://en.wikipedia.org/wiki/Linear_classifier,"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics. An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.",6133469990911752276,related_concept,Statistical classification,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.480769230769231,1.3188199268673606
686,1385,Linear predictor function,http://dbpedia.org/resource/Linear_predictor_function,http://en.wikipedia.org/wiki/Linear_predictor_function,"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as ""weights"".",796319073021334213,related_concept,Statistical classification,2024-06-23 21:17:09.728,[],['Linear regression'],set(),set(),0,0.6923076923076923,4.973999458229274e-304
687,1389,Algorithm,http://dbpedia.org/resource/Algorithm,http://en.wikipedia.org/wiki/Algorithm,"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can perform automated deductions (referred to as automated reasoning) and use mathematical and logical tests to divert the code execution through various routes (referred to as automated decision-making). Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as ""memory"", ""search"" and ""stimulus"". In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result. As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.",5615966079397376943,related_concept,Statistical classification,2024-06-23 21:17:09.728,['Algorithm'],['Algorithm'],set(),set(),0,10.91340206185567,1.4280882210875028
688,1391,Ordinal data,http://dbpedia.org/resource/Ordinal_data,http://en.wikipedia.org/wiki/Ordinal_data,"Ordinal data is a categorical, statistical data type where the variables have natural, ordered categories and the distances between the categories are not known. These data exist on an ordinal scale, one of four levels of measurement described by S. S. Stevens in 1946. The ordinal scale is distinguished from the nominal scale by having a ranking. It also differs from the interval scale and ratio scale by not having category widths that represent equal increments of the underlying attribute.",5960527118447806911,related_concept,Statistical classification,2024-06-23 21:17:09.728,['Ordinal data'],"['Ordinal data', 'ANOVA', 'Classification']",set(),set(),0,0.90625,1.444482081483843
689,1392,Classification,http://dbpedia.org/resource/Classification,http://en.wikipedia.org/wiki/Classification,"Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. Classification is the grouping of related facts into classes. It may also refer to:",168624939292157565,related_concept,Statistical classification,2024-06-23 21:17:09.728,['Classification'],"['Classification', 'Precision and recall', 'Sensitivity and specificity']",set(),set(),0,2.808080808080808,1.3688341161262576
690,1393,Sequence labeling,http://dbpedia.org/resource/Sequence_labeling,http://en.wikipedia.org/wiki/Sequence_labeling,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once. As an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described. Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right. For example, the word ""sets"" can be either a noun or verb. In a phrase like ""he sets the books down"", the word ""he"" is unambiguously a pronoun, and ""the"" unambiguously a determiner, and using either of these labels, ""sets"" can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are. But in other cases, only one of the adjacent words is similarly helpful. In ""he sets and then knocks over the table"", only the word ""he"" to the left is helpful (cf. ""...picks up the sets and then knocks over...""). Conversely, in ""... and also sets the table"" only the word ""the"" to the right is helpful (cf. ""... and also sets of books were ...""). An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left. Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. This leads naturally to the hidden Markov model (HMM), one of the most common statistical models used for sequence labeling. Other common models in use are the maximum entropy Markov model and conditional random field.",997377822386526902,related_concept,Statistical classification,2024-06-23 21:17:09.728,['Sequence labeling'],['Sequence labeling'],set(),set(),0,2.6666666666666665,1.4196408635836564
691,1397,Sufficient statistic,http://dbpedia.org/resource/Sufficient_statistic,http://en.wikipedia.org/wiki/Sufficient_statistic,"In statistics, a statistic is sufficient with respect to a statistical model and its associated unknown parameter if ""no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter"". In particular, a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than the statistic, as to which of those probability distributions is the sampling distribution. A related concept is that of linear sufficiency, which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic, although it is restricted to linear estimators. The Kolmogorov structure function deals with individual finite data; the related notion there is the algorithmic sufficient statistic. The concept is due to Sir Ronald Fisher in 1920. Stephen Stigler noted in 1973 that the concept of sufficiency had fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form (see below), but remained very important in theoretical work.",4619175910714788460,related_concept,Likelihood function,2024-06-23 21:17:09.728,['Kolmogorov structure function'],"['Kolmogorov structure function', 'Poisson distribution', 'Gamma distribution']",set(),set(),0,1.6878453038674033,1.3507700256357693
692,1398,Likelihoodism,http://dbpedia.org/resource/Likelihoodism,http://en.wikipedia.org/wiki/Likelihoodism,,823291015921484958,related_concept,Likelihood function,2024-06-23 21:17:09.728,[],"['Likelihoodism', 'Bayesian statistics', 'Bayesian inference', 'Likelihood principle']",set(),set(),0,0.05,0.43126864838428536
693,1403,Likelihood principle,http://dbpedia.org/resource/Likelihood_principle,http://en.wikipedia.org/wiki/Likelihood_principle,"In statistics, the likelihood principle is the proposition that, given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function. A likelihood function arises from a probability density function considered as a function of its distributional parameterization argument. For example, consider a model which gives the probability density function of observable random variable as a function of a parameter Then for a specific value of the function is a likelihood function of it gives a measure of how ""likely"" any particular value of is, if we know that has the value The density function may be a density with respect to counting measure, i.e. a probability mass function. Two likelihood functions are equivalent if one is a scalar multiple of the other.The likelihood principle is this: All information from the data that is relevant to inferences about the value of the model parameters is in the equivalence class to which the likelihood function belongs. The strong likelihood principle applies this same criterion to cases such as sequential experiments where the sample of data that is available results from applying a stopping rule to the observations earlier in the experiment.",1584593735857037097,related_concept,Likelihood function,2024-06-23 21:17:09.728,[],"['Bayesian statistics', 'Bayes factor', 'Neyman–Pearson lemma', ""Pearson's chi-squared test""]",set(),set(),0,0.7014925373134329,1.2224871729201832
694,1407,Empirical likelihood,http://dbpedia.org/resource/Empirical_likelihood,http://en.wikipedia.org/wiki/Empirical_likelihood,Empirical likelihood (EL) is a nonparametric method that requires fewer assumptions about the error distribution while retaining some of the merits in likelihood-based inference. The estimation method requires that the data are independent and identically distributed (iid). It performs well even when the distribution is asymmetric or censored. EL methods can also handle constraints and prior information on parameters. Art Owen pioneered work in this area with his 1988 paper.,5493322875005106393,related_concept,Likelihood function,2024-06-23 21:17:09.728,['Empirical likelihood'],['Empirical likelihood'],set(),set(),0,0.71875,1.3418746247964302
695,1408,Pseudolikelihood,http://dbpedia.org/resource/Pseudolikelihood,http://en.wikipedia.org/wiki/Pseudolikelihood,"In statistical theory, a pseudolikelihood is an approximation to the joint probability distribution of a collection of random variables. The practical use of this is that it can provide an approximation to the likelihood function of a set of observed data which may either provide a computationally simpler problem for estimation, or may provide a way of obtaining explicit estimates of model parameters. The pseudolikelihood approach was introduced by Julian Besag in the context of analysing data having spatial dependence.",7897206384463907130,related_concept,Likelihood function,2024-06-23 21:17:09.728,[],['Bayesian network'],set(),set(),0,0.8181818181818182,1.3487572920111375
696,1410,Score (statistics),http://dbpedia.org/resource/Score_(statistics),http://en.wikipedia.org/wiki/Score_(statistics),"In statistics, the score (or informant) is the gradient of the log-likelihood function with respect to the parameter vector. Evaluated at a particular point of the parameter vector, the score indicates the steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values. If the log-likelihood function is continuous over the parameter space, the score will vanish at a local maximum or minimum; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function. Since the score is a function of the observations that are subject to sampling error, it lends itself to a test statistic known as score test in which the parameter is held at a particular value. Further, the ratio of two likelihood functions evaluated at two distinct parameter values can be understood as a definite integral of the score function.",2058942554040197121,related_concept,Likelihood function,2024-06-23 21:17:09.728,[],['Fisher information'],set(),set(),0,1.0416666666666667,1.3535093305666357
697,1414,Gamma distribution,http://dbpedia.org/resource/Gamma_distribution,http://en.wikipedia.org/wiki/Gamma_distribution,"In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-square distribution are special cases of the gamma distribution. There are two equivalent parameterizations in common use: 1. 
* With a shape parameter and a scale parameter . 2. 
* With a shape parameter and an inverse scale parameter , called a rate parameter. In each of these forms, both parameters are positive real numbers. The gamma distribution is the maximum entropy probability distribution (both with respect to a uniform base measure and a base measure) for a random variable for which E[X] = kθ = α/β is fixed and greater than zero, and E[ln(X)] = ψ(k) + ln(θ) = ψ(α) − ln(β) is fixed (ψ is the digamma function).",2743953405941469933,related_concept,Likelihood function,2024-06-23 21:17:09.728,[],"['Bayesian statistics', 'Poisson distribution', 'Kullback–Leibler divergence', 'PDF', ""Newton's method"", 'Bayesian inference', 'Poisson regression', 'Algorithm']",set(),set(),0,1.6635802469135803,1.2440027025482499
698,1419,Evaluation of binary classifiers,http://dbpedia.org/resource/Evaluation_of_binary_classifiers,http://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers,"The evaluation of binary classifiers compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated. There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.",1820828015869947094,related_concept,Sensitivity and specificity,2024-06-23 21:17:09.728,[],"['Prevalence', 'Precision and recall', ""Bayes' theorem"", 'F-score', 'Matthews correlation coefficient']",set(),set(),0,0.45263157894736844,1.3150429049036811
699,1421,False positive rate,http://dbpedia.org/resource/False_positive_rate,http://en.wikipedia.org/wiki/False_positive_rate,"In statistics, when performing multiple comparisons, a false positive ratio (also known as fall-out or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification). The false positive rate (or ""false alarm rate"") usually refers to the expectancy of the false positive ratio.",130550881331644331,related_concept,Sensitivity and specificity,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.59375,5.500976461732728e-304
700,1423,Cumulative accuracy profile,http://dbpedia.org/resource/Cumulative_accuracy_profile,http://en.wikipedia.org/wiki/Cumulative_accuracy_profile,"A cumulative accuracy profile (CAP) is a concept utilized in data science to visualize discrimination power. The CAP of a model represents the cumulative number of positive outcomes along the y-axis versus the corresponding cumulative number of a classifying parameter along the x-axis. The output is called a CAP curve. The CAP is distinct from the receiver operating characteristic (ROC) curve, which plots the true-positive rate against the false-positive rate. CAPs are used in robustness evaluations of classification models.",7233726282737021392,related_concept,Sensitivity and specificity,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.105263157894737,1.1073393468626849
701,1425,False positive paradox,http://dbpedia.org/resource/False_positive_paradox,http://en.wikipedia.org/wiki/False_positive_paradox,,6672914640987116891,related_concept,Sensitivity and specificity,2024-06-23 21:17:09.728,[],"[""Bayes' theorem""]",set(),set(),0,0.16184971098265896,0.5512875844792196
702,1427,Detection theory,http://dbpedia.org/resource/Detection_theory,http://en.wikipedia.org/wiki/Detection_theory,"Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the detection machine and of the nervous system of the operator). In the field of electronics, signal recovery is the separation of such patterns from a disguising background. According to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be. The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed. When the detecting system is a human being, characteristics such as experience, expectations, physiological state (e.g., fatigue) and other factors can affect the threshold applied. For instance, a sentry in wartime might be likely to detect fainter stimuli than the same sentry in peacetime due to a lower criterion, however they might also be more likely to treat innocuous stimuli as a threat. Much of the early work in detection theory was done by radar researchers. By 1954, the theory was fully developed on the theoretical side as described by Peterson, Birdsall and Fox and the foundation for the psychological theory was made by Wilson P. Tanner, David M. Green, and John A. Swets, also in 1954.Detection theory was used in 1966 by John A. Swets and David M. Green for psychophysics. Green and Swets criticized the traditional methods of psychophysics for their inability to discriminate between the real sensitivity of subjects and their (potential) response biases. Detection theory has applications in many fields such as diagnostics of any kind, quality control, telecommunications, and psychology. The concept is similar to the signal-to-noise ratio used in the sciences and confusion matrices used in artificial intelligence. It is also usable in alarm management, where it is important to separate important events from background noise.",8025645120540844604,related_concept,Sensitivity and specificity,2024-06-23 21:17:09.728,['Detection theory'],['Detection theory'],set(),set(),0,1.0421348314606742,1.1508428266674153
703,1429,Brier score,http://dbpedia.org/resource/Brier_score,http://en.wikipedia.org/wiki/Brier_score,"The Brier Score is a strictly proper score function or strictly proper scoring rule that measures the accuracy of probabilistic predictions. For unidimensional predictions, it is strictly equivalent to the mean squared error as applied to predicted probabilities. The Brier score is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes or classes. The set of possible outcomes can be either binary or categorical in nature, and the probabilities assigned to this set of outcomes must sum to one (where each individual probability is in the range of 0 to 1). It was proposed by Glenn W. Brier in 1950. The Brier score can be thought of as a cost function. More precisely, across all items in a set of N predictions, the Brier score measures the mean squared difference between: 
* The predicted probability assigned to the possible outcomes for item i 
* The actual outcome Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score, in its most common formulation, takes on a value between zero and one, since this is the square of the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 or 1). In the original (1950) formulation of the Brier score, the range is double, from zero to two. The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but it is inappropriate for ordinal variables which can take on three or more values.",1393754615991847418,related_concept,Sensitivity and specificity,2024-06-23 21:17:09.728,['Brier score'],['Brier score'],set(),set(),0,2.25,1.4392528885301905
704,1437,Foundational crisis of mathematics,http://dbpedia.org/resource/Foundational_crisis_of_mathematics,http://en.wikipedia.org/wiki/Foundational_crisis_of_mathematics,,6635342168159330249,related_concept,Theorem,2024-06-23 21:17:09.728,[],"['Analytics', 'Real number', 'Euclidean geometry', 'Cauchy sequence', 'Peano arithmetic', ""Gödel's incompleteness theorems"", 'Mathematics', 'Consistency', ""Gödel's completeness theorem""]",set(),set(),0,0.029780564263322883,0.5489962311668589
705,1438,Model theory,http://dbpedia.org/resource/Model_theory,http://en.wikipedia.org/wiki/Model_theory,"In mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). The aspects investigated include the number and size of models of a theory, the relationship of different models to each other, and their interaction with the formal language itself. In particular, model theorists also investigate the sets that can be defined in a model of a theory, and the relationship of such definable sets to each other.As a separate discipline, model theory goes back to Alfred Tarski, who first used the term ""Theory of Models"" in publication in 1954.Since the 1970s, the subject has been shaped decisively by Saharon Shelah's stability theory. Compared to other areas of mathematical logic such as proof theory, model theory is often less concerned with formal rigour and closer in spirit to classical mathematics.This has prompted the comment that ""if proof theory is about the sacred, then model theory is about the profane"".The applications of model theory to algebraic and diophantine geometry reflect this proximity to classical mathematics, as they often involve an integration of algebraic and model-theoretic results and techniques. The most prominent scholarly organization in the field of model theory is the Association for Symbolic Logic.",2779849209967924131,related_concept,Theorem,2024-06-23 21:17:09.728,[],"[""Gödel's completeness theorem"", 'Theorem', 'Axiom', 'Model theory']",set(),set(),0,1.9423459244532804,3.664513085222346e-304
706,1439,Theory (mathematical logic),http://dbpedia.org/resource/Theory_(mathematical_logic),http://en.wikipedia.org/wiki/Theory_(mathematical_logic),"In mathematical logic, a theory (also called a formal theory) is a set of sentences in a formal language. In most scenarios, a deductive system is first understood from context, after which an element of a deductively closed theory is then called a theorem of the theory. In many deductive systems there is usually a subset that is called ""the set of axioms"" of the theory , in which case the deductive system is also called an ""axiomatic system"". By definition, every axiom is automatically a theorem. A first-order theory is a set of first-order sentences (theorems) recursively obtained by the inference rules of the system applied to the set of axioms.",4968793900965712722,related_concept,Theorem,2024-06-23 21:17:09.728,[],['Peano arithmetic'],set(),set(),0,1.518032786885246,3.6537267416157226e-304
707,1440,Gödel's completeness theorem,http://dbpedia.org/resource/Gödel's_completeness_theorem,http://en.wikipedia.org/wiki/Gödel's_completeness_theorem,"Gödel's completeness theorem is a fundamental theorem in mathematical logic that establishes a correspondence between semantic truth and syntactic provability in first-order logic. The completeness theorem applies to any first-order theory: If T is such a theory, and φ is a sentence (in the same language) and every model of T is a model of φ, then there is a (first-order) proof of φ using the statements of T as axioms. One sometimes says this as ""anything universally true is provable"". This is in contrast, but not contradiction, to Gödel's incompleteness theorems, in which a formula true in only some models may not be provable. It makes a close link between model theory that deals with what is true in different models, and proof theory that studies what can be formally proven in particular formal systems. It was first proved by Kurt Gödel in 1929. It was then simplified when Leon Henkin observed in his Ph.D. thesis that the hard part of the proof can be presented as the Model Existence Theorem (published in 1949). Henkin's proof was simplified by Gisbert Hasenjaeger in 1953.",3509364482189744090,related_concept,Theorem,2024-06-23 21:17:09.728,['Theorem'],"[""Gödel's completeness theorem"", 'Theorem', 'Peano arithmetic', ""Gödel's incompleteness theorems""]",set(),set(),0,1.3773006134969326,0.2616643745847697
708,1441,Mathematical proof,http://dbpedia.org/resource/Mathematical_proof,http://en.wikipedia.org/wiki/Mathematical_proof,"A mathematical proof is an inferential argument for a mathematical statement, showing that the stated assumptions logically guarantee the conclusion. The argument may use other previously established statements, such as theorems; but every proof can, in principle, be constructed using only certain basic or original assumptions known as axioms, along with the accepted rules of inference. Proofs are examples of exhaustive deductive reasoning which establish logical certainty, to be distinguished from empirical arguments or non-exhaustive inductive reasoning which establish ""reasonable expectation"". Presenting many cases in which the statement holds is not enough for a proof, which must demonstrate that the statement is true in all possible cases. A proposition that has not been proved but is believed to be true is known as a conjecture, or a hypothesis if frequently used as an assumption for further mathematical work. Proofs employ logic expressed in mathematical symbols, along with natural language which usually admits some ambiguity. In most mathematical literature, proofs are written in terms of rigorous informal logic. Purely formal proofs, written fully in symbolic language without the involvement of natural language, are considered in proof theory. The distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics, oral traditions in the mainstream mathematical community or in other cultures. The philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language.",2636279785473886610,related_concept,Theorem,2024-06-23 21:17:09.728,[],"['Mathematical proof', 'Euclidean geometry', 'Axiom', ""Bayes' theorem""]",set(),set(),0,2.2310838445807772,3.690000868815235e-304
709,1442,Non-classical logic,http://dbpedia.org/resource/Non-classical_logic,http://en.wikipedia.org/wiki/Non-classical_logic,"Non-classical logics (and sometimes alternative logics) are formal systems that differ in a significant way from standard logical systems such as propositional and predicate logic. There are several ways in which this is done, including by way of extensions, deviations, and variations. The aim of these departures is to make it possible to construct different models of logical consequence and logical truth. Philosophical logic is understood to encompass and focus on non-classical logics, although the term has other meanings as well. In addition, some parts of theoretical computer science can be thought of as using non-classical reasoning, although this varies according to the subject area. For example, the basic boolean functions (e.g. AND, OR, NOT, etc) in computer science are very much classical in nature, as is clearly the case given that they can be fully described by classical truth tables. However, in contrast, some computerized proof methods may not use classical logic in the reasoning process.",5087504722351798472,related_concept,Theorem,2024-06-23 21:17:09.728,['Non-classical logic'],['Non-classical logic'],set(),set(),0,2.3642384105960264,1.4352617309933309
710,1443,Consistency,http://dbpedia.org/resource/Consistency,http://en.wikipedia.org/wiki/Consistency,"In classical deductive logic, a consistent theory is one that does not lead to a logical contradiction. The lack of contradiction can be defined in either semantic or syntactic terms. The semantic definition states that a theory is consistent if it has a model, i.e., there exists an interpretation under which all formulas in the theory are true. This is the sense used in traditional Aristotelian logic, although in contemporary mathematical logic the term satisfiable is used instead. The syntactic definition states a theory is consistent if there is no formula such that both and its negation are elements of the set of consequences of . Let be a set of closed sentences (informally ""axioms"") and the set of closed sentences provable from under some (specified, possibly implicitly) formal deductive system. The set of axioms is consistent when for no formula . If there exists a deductive system for which these semantic and syntactic definitions are equivalent for any theory formulated in a particular deductive logic, the logic is called complete. The completeness of the sentential calculus was proved by Paul Bernays in 1918 and Emil Post in 1921, while the completeness of predicate calculus was proved by Kurt Gödel in 1930, and consistency proofs for arithmetics restricted with respect to the induction axiom schema were proved by Ackermann (1924), von Neumann (1927) and Herbrand (1931). Stronger logics, such as second-order logic, are not complete. A consistency proof is a mathematical proof that a particular theory is consistent. The early development of mathematical proof theory was driven by the desire to provide finitary consistency proofs for all of mathematics as part of Hilbert's program. Hilbert's program was strongly impacted by the incompleteness theorems, which showed that sufficiently strong proof theories cannot prove their own consistency (provided that they are in fact consistent). Although consistency can be proved by means of model theory, it is often done in a purely syntactical way, without any need to reference some model of the logic. The cut-elimination (or equivalently the normalization of the underlying calculus if there is one) implies the consistency of the calculus: since there is no cut-free proof of falsity, there is no contradiction in general.",8088007890914171438,related_concept,Theorem,2024-06-23 21:17:09.728,[],"['Peano arithmetic', ""Gödel's incompleteness theorems""]",set(),set(),0,1.9023668639053255,3.63976727072859e-304
711,1444,Peano arithmetic,http://dbpedia.org/resource/Peano_arithmetic,http://en.wikipedia.org/wiki/Peano_arithmetic,,157830994557438841,related_concept,Theorem,2024-06-23 21:17:09.728,[],"['Peano arithmetic', 'Axiom', 'Mean', ""Gödel's incompleteness theorems"", ""Gödel's completeness theorem""]",set(),set(),0,1.245,0.5444176227854867
712,1445,Compactness theorem,http://dbpedia.org/resource/Compactness_theorem,http://en.wikipedia.org/wiki/Compactness_theorem,"In mathematical logic, the compactness theorem states that a set of first-order sentences has a model if and only if every finite subset of it has a model. This theorem is an important tool in model theory, as it provides a useful (but generally not effective) method for constructing models of any set of sentences that is finitely consistent. The compactness theorem for the propositional calculus is a consequence of Tychonoff's theorem (which says that the product of compact spaces is compact) applied to compact Stone spaces, hence the theorem's name. Likewise, it is analogous to the finite intersection property characterization of compactness in topological spaces: a collection of closed sets in a compact space has a non-empty intersection if every finite subcollection has a non-empty intersection. The compactness theorem is one of the two key properties, along with the downward Löwenheim–Skolem theorem, that is used in Lindström's theorem to characterize first-order logic. Although, there are some generalizations of the compactness theorem to non-first-order logics, the compactness theorem itself does not hold in them, except for a very limited number of examples.",801457508610272407,related_concept,Theorem,2024-06-23 21:17:09.728,[],"['Peano arithmetic', ""Gödel's completeness theorem""]",set(),set(),0,1.3271028037383177,3.635305368779134e-304
713,1446,Mathematical logic,http://dbpedia.org/resource/Mathematical_logic,http://en.wikipedia.org/wiki/Mathematical_logic,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics. Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.",3181521159217149253,related_concept,Theorem,2024-06-23 21:17:09.728,['Mathematical logic'],"['Mathematical logic', 'Non-classical logic', ""Gödel's completeness theorem"", ""Gödel's incompleteness theorems"", 'Peano arithmetic', 'Model theory', 'Proof theory', 'Computer science', 'Euclidean geometry', 'PDF']",set(),set(),0,2.642771804062127,0.635876101968554
714,1448,Logically,http://dbpedia.org/resource/Logically,http://en.wikipedia.org/wiki/Logically,,7538732993832655184,related_concept,Theorem,2024-06-23 21:17:09.728,[],"['Fuzzy logic', 'Bayesianism', ""Gödel's incompleteness theorems"", 'Axiom', 'Computation', 'Prolog', 'Analytics']",set(),set(),0,0.016025641025641024,0.4063662694495177
715,1449,Mathematics,http://dbpedia.org/resource/Mathematics,http://en.wikipedia.org/wiki/Mathematics,"Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline. Most mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or—in modern mathematics—entities that are stipulated with certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered as true starting points of the theory under consideration. Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. The fundamental truths of mathematics are independent from any scientific experimentation, although mathematics is extensively used for modeling phenomena. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other mathematical areas are developed independently from any application (and are therefore called pure mathematics), but practical applications are often discovered later. A fitting example is the problem of integer factorization, which goes back to Euclid, but which had no practical application before its use in the RSA cryptosystem (for the security of computer networks). Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas of the subject. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method. This gave rise to a dramatic increase in the number of mathematics areas and their fields of applications. This can be seen, for example, in the contemporary Mathematics Subject Classification, which lists more than 60 first-level areas of mathematics.",1230990331294589937,related_concept,Theorem,2024-06-23 21:17:09.728,"['Classification', 'Mathematics']","['Mathematics', 'Classification', 'Calculus', 'Theorem', 'Geometry', 'Euclidean plane', 'Euclidean geometry', 'Discrete mathematics', 'Algorithm', 'Peano arithmetic', ""Gödel's incompleteness theorems"", 'Statistical theory', 'Computation', 'Numerical analysis', 'Statistical hypothesis testing', 'Mathematical model']",{'Mathematics'},set(),0,69.44358974358974,1.4011541060713606
716,1450,Fundamental theorem,http://dbpedia.org/resource/Fundamental_theorem,http://en.wikipedia.org/wiki/Fundamental_theorem,,138327666378827650,related_concept,Theorem,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.25,0.5671854657484158
717,1451,Scientific law,http://dbpedia.org/resource/Scientific_law,http://en.wikipedia.org/wiki/Scientific_law,"Scientific laws or laws of science are statements, based on repeated experiments or observations, that describe or predict a range of natural phenomena. The term law has diverse usage in many cases (approximate, accurate, broad, or narrow) across all fields of natural science (physics, chemistry, astronomy, geoscience, biology). Laws are developed from data and can be further developed through mathematics; in all cases they are directly or indirectly based on empirical evidence. It is generally understood that they implicitly reflect, though they do not explicitly assert, causal relationships fundamental to reality, and are discovered rather than invented. Scientific laws summarize the results of experiments or observations, usually within a certain range of application. In general, the accuracy of a law does not change when a new theory of the relevant phenomenon is worked out, but rather the scope of the law's application, since the mathematics or statement representing the law does not change. As with other kinds of scientific knowledge, scientific laws do not express absolute certainty, as mathematical theorems or identities do. A scientific law may be contradicted, restricted, or extended by future observations. A law can often be formulated as one or several statements or equations, so that it can predict the outcome of an experiment. Laws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws, since they have not been verified to the same degree, although they may lead to the formulation of laws. Laws are narrower in scope than scientific theories, which may entail one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. The nature of scientific laws has been much discussed in philosophy, but in essence scientific laws are simply empirical conclusions reached by scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.",4836504585460797129,related_concept,Theorem,2024-06-23 21:17:09.728,['Scientific law'],"['Scientific law', 'Euclidean geometry']",set(),set(),0,1.2794411177644711,1.1282913288125564
718,1453,Gödel's incompleteness theorems,http://dbpedia.org/resource/Gödel's_incompleteness_theorems,http://en.wikipedia.org/wiki/Gödel's_incompleteness_theorems,"Gödel's incompleteness theorems are two theorems of mathematical logic that are concerned with the limits of provability in formal axiomatic theories. These results, published by Kurt Gödel in 1931, are important both in mathematical logic and in the philosophy of mathematics. The theorems are widely, but not universally, interpreted as showing that Hilbert's program to find a complete and consistent set of axioms for all mathematics is impossible. The first incompleteness theorem states that no consistent system of axioms whose theorems can be listed by an effective procedure (i.e., an algorithm) is capable of proving all truths about the arithmetic of natural numbers. For any such consistent formal system, there will always be statements about natural numbers that are true, but that are unprovable within the system. The second incompleteness theorem, an extension of the first, shows that the system cannot demonstrate its own consistency. Employing a diagonal argument, Gödel's incompleteness theorems were the first of several closely related theorems on the limitations of formal systems. They were followed by Tarski's undefinability theorem on the formal undefinability of truth, Church's proof that Hilbert's Entscheidungsproblem is unsolvable, and Turing's theorem that there is no algorithm to solve the halting problem.",7344389122801750323,related_concept,Theorem,2024-06-23 21:17:09.728,[],"[""Gödel's incompleteness theorems"", 'Peano arithmetic', 'Euclidean geometry', 'Theorem', ""Gödel's completeness theorem"", 'Kolmogorov complexity', 'Mathematics']",set(),set(),0,2.1079429735234214,3.6418674071248345e-304
719,1454,Axiom,http://dbpedia.org/resource/Axiom,http://en.wikipedia.org/wiki/Axiom,"An axiom, postulate, or assumption is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Ancient Greek word ἀξίωμα (axíōma), meaning 'that which is thought worthy or fit' or 'that which commends itself as evident'. The term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question. As used in modern logic, an axiom is a premise or starting point for reasoning. As used in mathematics, the term axiom is used in two related but distinguishable senses: and . Logical axioms are usually statements that are taken to be true within the system of logic they define and are often shown in symbolic form (e.g., (A and B) implies A), while non-logical axioms (e.g., a + b = b + a) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, ""axiom"", ""postulate"", and ""assumption"" may be used interchangeably. In most cases, a non-logical axiom is simply a formal logical expression used in deduction to build a mathematical theory, and might or might not be self-evident in nature (e.g., parallel postulate in Euclidean geometry). To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms), and there are typically many ways to axiomatize a given mathematical domain. Any axiom is a statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom to be ""true"" is a subject of debate in the philosophy of mathematics.",6950205188393554337,related_concept,Theorem,2024-06-23 21:17:09.728,['Euclidean geometry'],"['Euclidean geometry', 'Axiom', 'Peano arithmetic', 'Natural selection', ""Gödel's completeness theorem""]",set(),set(),0,3.787037037037037,3.658457172811246e-304
720,1455,Inference,http://dbpedia.org/resource/Inference,http://en.wikipedia.org/wiki/Inference,"Inferences are steps in reasoning, moving from premises to logical consequences; etymologically, the word infer means to ""carry forward"". Inference is theoretically traditionally divided into deduction and induction, a distinction that in Europe dates at least to Aristotle (300s BCE). Deduction is inference deriving logical conclusions from premises known or assumed to be true, with the laws of valid inference being studied in logic. Induction is inference from particular evidence to a universal conclusion. A third type of inference is sometimes distinguished, notably by Charles Sanders Peirce, contradistinguishing abduction from induction. Various fields study how inference is done in practice. Human inference (i.e. how humans draw conclusions) is traditionally studied within the fields of logic, argumentation studies, and cognitive psychology; artificial intelligence researchers develop automated inference systems to emulate human inference. Statistical inference uses mathematics to draw conclusions in the presence of uncertainty. This generalizes deterministic reasoning, with the absence of uncertainty as a special case. Statistical inference uses quantitative or qualitative (categorical) data which may be subject to random variations.",6934639550470046399,related_concept,Theorem,2024-06-23 21:17:09.728,"['Inference', 'Statistical inference']","['Inference', 'Statistical inference', 'AI', 'Prolog', 'Bayesian inference', ""Bayes' theorem"", 'Inductive inference']",{'Inference'},set(),0,2.4142538975501115,1.3859599465124275
721,1456,Proof theory,http://dbpedia.org/resource/Proof_theory,http://en.wikipedia.org/wiki/Proof_theory,"Proof theory is a major branch of mathematical logic that represents proofs as formal mathematical objects, facilitating their analysis by mathematical techniques. Proofs are typically presented as inductively-defined data structures such as lists, boxed lists, or trees, which are constructed according to the axioms and rules of inference of the logical system. Consequently, proof theory is syntactic in nature, in contrast to model theory, which is semantic in nature. Some of the major areas of proof theory include structural proof theory, ordinal analysis, provability logic, reverse mathematics, proof mining, automated theorem proving, and proof complexity. Much research also focuses on applications in computer science, linguistics, and philosophy.",3525002708559698065,related_concept,Theorem,2024-06-23 21:17:09.728,['Proof theory'],"['Proof theory', 'Mathematics', ""Gödel's incompleteness theorems"", 'Peano arithmetic']",set(),set(),0,1.8535911602209945,3.678973063767108e-304
722,1459,Marginal likelihood,http://dbpedia.org/resource/Marginal_likelihood,http://en.wikipedia.org/wiki/Marginal_likelihood,"A marginal likelihood is a likelihood function that has been integrated over the parameter space. In Bayesian statistics, it represents the probability of generating the observed sample from a prior and is therefore often referred to as model evidence or simply evidence.",7283811752377617558,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,['Bayesian statistics'],"['Bayesian statistics', 'Bayes factor']",set(),set(),0,1.0933333333333333,4.831755473625849e-304
723,1460,Iterative method,http://dbpedia.org/resource/Iterative_method,http://en.wikipedia.org/wiki/Iterative_method,"In computational mathematics, an iterative method is a mathematical procedure that uses an initial value to generate a sequence of improving approximate solutions for a class of problems, in which the n-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (for example, solving a linear system of equations by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving many variables (sometimes on the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.",6758108348268824935,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,"['Iterative', 'Iterative method']","[""Newton's method"", 'Iterative', 'Iterative method']",set(),set(),0,2.6758620689655173,1.3910362647122212
724,1461,Newton's method,http://dbpedia.org/resource/Newton's_method,http://en.wikipedia.org/wiki/Newton's_method,"In numerical analysis, Newton's method, also known as the Newton–Raphson method, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. The most basic version starts with a single-variable function f defined for a real variable x, the function's derivative f′, and an initial guess x0 for a root of f. If the function satisfies sufficient assumptions and the initial guess is close, then is a better approximation of the root than x0. Geometrically, (x1, 0) is the intersection of the x-axis and the tangent of the graph of f at (x0, f(x0)): that is, the improved guess is the unique root of the linear approximation at the initial point. The process is repeated as until a sufficiently precise value is reached. This algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations.",1540393471600560415,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,"[""Newton's method""]","[""Newton's method"", 'Normal distribution']",set(),set(),0,2.4035087719298245,1.4008034801839346
725,1462,Inside-outside algorithm,http://dbpedia.org/resource/Inside-outside_algorithm,http://en.wikipedia.org/wiki/Inside-outside_algorithm,,7910780664744903166,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.2727272727272727,0.6062678651985416
726,1466,Baum–Welch algorithm,http://dbpedia.org/resource/Baum–Welch_algorithm,http://en.wikipedia.org/wiki/Baum–Welch_algorithm,"In electrical engineering, statistical computing and bioinformatics, the Baum–Welch algorithm is a special case of the expectation–maximization algorithm used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm to compute the statistics for the expectation step.",536458798163895114,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,[],"['Baum–Welch algorithm', ""Bayes' theorem""]",set(),set(),0,1.7115384615384615,1.3431372068960663
727,1470,Viterbi algorithm,http://dbpedia.org/resource/Viterbi_algorithm,http://en.wikipedia.org/wiki/Viterbi_algorithm,"The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate of the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM). The algorithm has found universal application in decoding the convolutional codes used in both CDMA and GSM digital cellular, dial-up modems, satellite, deep-space communications, and 802.11 wireless LANs. It is now also commonly used in speech recognition, speech synthesis, diarization, keyword spotting, computational linguistics, and bioinformatics. For example, in speech-to-text (speech recognition), the acoustic signal is treated as the observed sequence of events, and a string of text is considered to be the ""hidden cause"" of the acoustic signal. The Viterbi algorithm finds the most likely string of text given the acoustic signal.",5573216199957690463,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,['Viterbi algorithm'],"['Viterbi algorithm', 'Bayesian network', 'Iterative']",set(),set(),0,2.75,1.4186647738901865
728,1472,Gradient descent,http://dbpedia.org/resource/Gradient_descent,http://en.wikipedia.org/wiki/Gradient_descent,"In mathematics, gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.",2595686056663304355,main_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,"['Gradient', 'Gradient descent']","['Gradient', 'Gradient descent', ""Newton's method"", ""Euler's method"", 'Euclidean distance']",set(),set(),0,1.4197247706422018,4.637285618655118e-304
729,1475,Kalman filter,http://dbpedia.org/resource/Kalman_filter,http://en.wikipedia.org/wiki/Kalman_filter,"For statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, including statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone, by estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. Kálmán, who was one of the primary developers of its theory. This digital filter is sometimes termed the Stratonovich–Kalman–Bucy filter because it is a special case of a more general, nonlinear filter developed somewhat earlier by the Soviet mathematician Ruslan Stratonovich. In fact, some of the special case linear filter's equations appeared in papers by Stratonovich that were published before summer 1960, when Kalman met with Stratonovich during a conference in Moscow. Kalman filtering has numerous technological applications. A common application is for guidance, navigation, and control of vehicles, particularly aircraft, spacecraft and ships positioned dynamically. Furthermore, Kalman filtering is a concept much applied in time series analysis used for topics such as signal processing and econometrics. Kalman filtering is also one of the main topics of robotic motion planning and control and can be used for trajectory optimization. Kalman filtering also works for modeling the central nervous system's control of movement. Due to the time delay between issuing motor commands and receiving sensory feedback, the use of Kalman filters provides a realistic model for making estimates of the current state of a motor system and issuing updated commands. The algorithm works by a two-phase process. For the prediction phase, the Kalman filter produces estimates of the current state variables, along with their uncertainties. Once the outcome of the next measurement (necessarily corrupted with some error, including random noise) is observed, these estimates are updated using a weighted average, with more weight being given to estimates with greater certainty. The algorithm is recursive. It can operate in real time, using only the present input measurements and the state calculated previously and its uncertainty matrix; no additional past information is required. Optimality of Kalman filtering assumes that errors have a normal (Gaussian) distribution. In the words of Rudolf E. Kálmán: ""In summary, the following assumptions are made about random processes: Physical random phenomena may be thought of as due to primary random sources exciting dynamic systems. The primary sources are assumed to be independent gaussian random processes with zero mean; the dynamic systems will be linear."" Though regardless of Gaussianity, if the process and measurement covariances are known, the Kalman filter is the best possible linear estimator in the minimum mean-square-error sense. Extensions and generalizations of the method have also been developed, such as the extended Kalman filter and the which work on nonlinear systems. The basis is a hidden Markov model such that the state space of the latent variables is continuous and all latent and observed variables have Gaussian distributions. Kalman filtering has been used successfully in multi-sensor fusion, and distributed sensor networks to develop distributed or consensus Kalman filtering.",7821116853953831132,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,['Kalman filter'],"['Kalman filter', 'Bayesian network', 'PDF', 'Smoothing', 'Expectation–maximization algorithm']",set(),set(),0,1.4059945504087195,1.3784072628868935
730,1476,Ordered subset expectation maximization,http://dbpedia.org/resource/Ordered_subset_expectation_maximization,http://en.wikipedia.org/wiki/Ordered_subset_expectation_maximization,"In mathematical optimization, the ordered subset expectation maximization (OSEM) method is an iterative method that is used in computed tomography. In applications in medical imaging, the OSEM method is used for positron emission tomography, for single photon emission computed tomography, and for X-ray computed tomography. The OSEM method is related to the expectation maximization (EM) method of statistics. The OSEM method is also related to methods of filtered back projection.",6968542077717377767,related_concept,Expectation–maximization algorithm,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.625,1.35585934832543
731,1478,Expectation–maximization algorithm,http://dbpedia.org/resource/Expectation–maximization_algorithm,http://en.wikipedia.org/wiki/Expectation–maximization_algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.",6604508325088377109,main_concept,,2024-06-23 21:17:09.728,[],"['Viterbi algorithm', 'Bayesian inference', 'Kullback–Leibler divergence', 'Kalman filter', ""Newton's method""]",set(),set(),0,3.4705882352941178,1.3825645664682265
732,1480,Estimator,http://dbpedia.org/resource/Estimator,http://en.wikipedia.org/wiki/Estimator,"In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. For example, the sample mean is a commonly used estimator of the population mean. There are point and interval estimators. The point estimators yield single-valued results. This is in contrast to an interval estimator, where the result would be a range of plausible values. ""Single value"" does not necessarily mean ""single number"", but includes vector valued or function valued estimators. Estimation theory is concerned with the properties of estimators; that is, with defining properties that can be used to compare different estimators (different rules for creating estimates) for the same quantity, based on the same data. Such properties can be used to determine the best rules to use under given circumstances. However, in robust statistics, statistical theory goes on to consider the balance between having good properties, if tightly defined assumptions hold, and having less good properties that hold under wider conditions.",1693217822404939766,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,['Estimation theory'],"['Estimation theory', 'Variance']",{'Estimator'},set(),0,3.36734693877551,1.3342030743003355
733,1482,Generalized method of moments,http://dbpedia.org/resource/Generalized_method_of_moments,http://en.wikipedia.org/wiki/Generalized_method_of_moments,"In econometrics and statistics, the generalized method of moments (GMM) is a generic method for estimating parameters in statistical models. Usually it is applied in the context of semiparametric models, where the parameter of interest is finite-dimensional, whereas the full shape of the data's distribution function may not be known, and therefore maximum likelihood estimation is not applicable. The method requires that a certain number of moment conditions be specified for the model. These moment conditions are functions of the model parameters and the data, such that their expectation is zero at the parameters' true values. The GMM method then minimizes a certain norm of the sample averages of the moment conditions, and can therefore be thought of as a special case of minimum-distance estimation. The GMM estimators are known to be consistent, asymptotically normal, and most efficient in the class of all estimators that do not use any extra information aside from that contained in the moment conditions. GMM were advocated by Lars Peter Hansen in 1982 as a generalization of the method of moments, introduced by Karl Pearson in 1894. However, these estimators are mathematically equivalent to those based on ""orthogonality conditions"" (Sargan, 1958, 1959) or ""unbiased estimating equations"" (Huber, 1967; Wang et al., 1997).",7304822422612967304,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],['Consistency'],set(),set(),0,1.1515151515151516,1.3474882123784335
734,1484,Maximum spacing estimation,http://dbpedia.org/resource/Maximum_spacing_estimation,http://en.wikipedia.org/wiki/Maximum_spacing_estimation,"In statistics, maximum spacing estimation (MSE or MSP), or maximum product of spacing estimation (MPS), is a method for estimating the parameters of a univariate statistical model. The method requires maximization of the geometric mean of spacings in the data, which are the differences between the values of the cumulative distribution function at neighbouring data points. The concept underlying the method is based on the probability integral transform, in that a set of independent random samples derived from any random variable should on average be uniformly distributed with respect to the cumulative distribution function of the random variable. The MPS method chooses the parameter values that make the observed data as uniform as possible, according to a specific quantitative measure of uniformity. One of the most common methods for estimating the parameters of a distribution from data, the method of maximum likelihood (MLE), can break down in various cases, such as involving certain mixtures of continuous distributions. In these cases the method of maximum spacing estimation may be successful. Apart from its use in pure mathematics and statistics, the trial applications of the method have been reported using data from fields such as hydrology, econometrics, magnetic resonance imaging, and others.",408184296918979806,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],['Kullback–Leibler divergence'],set(),set(),0,0.19607843137254902,1.3494813218590096
735,1486,Quasi-maximum likelihood,http://dbpedia.org/resource/Quasi-maximum_likelihood,http://en.wikipedia.org/wiki/Quasi-maximum_likelihood,,2481618596012025798,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.3125,0.4760447190995505
736,1487,Bayesian estimator,http://dbpedia.org/resource/Bayesian_estimator,http://en.wikipedia.org/wiki/Bayesian_estimator,,163730202913107831,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],"['Bayesian statistics', 'Conjugate prior', ""Bayes' theorem"", 'Fisher information', 'Beta distribution', 'Database', 'Data']",set(),set(),0,0.15104166666666666,0.5742331978868325
737,1488,Fisher information,http://dbpedia.org/resource/Fisher_information,http://en.wikipedia.org/wiki/Fisher_information,"In mathematical statistics, the Fisher information (sometimes simply called information) is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information. In Bayesian statistics, the asymptotic distribution of the posterior mode depends on the Fisher information and not on the prior (according to the Bernstein–von Mises theorem, which was anticipated by Laplace for exponential families). The role of the Fisher information in the asymptotic theory of maximum-likelihood estimation was emphasized by the statistician Ronald Fisher (following some initial results by Francis Ysidro Edgeworth). The Fisher information is also used in the calculation of the Jeffreys prior, which is used in Bayesian statistics. The Fisher information matrix is used to calculate the covariance matrices associated with maximum-likelihood estimates. It can also be used in the formulation of test statistics, such as the Wald test. Statistical systems of a scientific nature (physical, biological, etc.) whose likelihood functions obey shift invariance have been shown to obey maximum Fisher information. The level of the maximum depends upon the nature of the system constraints.",4952431747274471687,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,"['Fisher information', 'Bayesian statistics', 'Jeffreys prior']","['Fisher information', 'Bayesian statistics', 'Bernstein–von Mises theorem', 'Bayesian network', 'Minkowski inequality', 'Jeffreys prior', 'Kullback–Leibler divergence']",set(),set(),0,1.3591549295774648,1.1188756764431906
738,1489,Minimum-distance estimation,http://dbpedia.org/resource/Minimum-distance_estimation,http://en.wikipedia.org/wiki/Minimum-distance_estimation,"Minimum-distance estimation (MDE) is a conceptual method for fitting a statistical model to data, usually the empirical distribution. Often-used estimators such as ordinary least squares can be thought of as special cases of minimum-distance estimation. While consistent and asymptotically normal, minimum-distance estimators are generally not statistically efficient when compared to maximum likelihood estimators, because they omit the Jacobian usually present in the likelihood function. This, however, substantially reduces the computational complexity of the optimization problem.",3416680484268569708,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,['Minimum-distance estimation'],['Minimum-distance estimation'],set(),set(),0,0.02694610778443114,1.3460715686668026
739,1491,M-estimator,http://dbpedia.org/resource/M-estimator,http://en.wikipedia.org/wiki/M-estimator,"In statistics, M-estimators are a broad class of extremum estimators for which the objective function is a sample average. Both non-linear least squares and maximum likelihood estimation are special cases of M-estimators. The definition of M-estimators was motivated by robust statistics, which contributed new types of M-estimators. The statistical procedure of evaluating an M-estimator on a data set is called M-estimation. 48 samples of robust M-estimators can be found in a recent review study. More generally, an M-estimator may be defined to be a zero of an estimating function. This estimating function is often the derivative of another statistical function. For example, a maximum-likelihood estimate is the point where the derivative of the likelihood function with respect to the parameter is zero; thus, a maximum-likelihood estimator is a critical point of the score function. In many applications, such M-estimators can be thought of as estimating characteristics of the population.",3123850451259697195,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,['M-estimator'],['M-estimator'],{'Estimator'},set(),0,1.5754985754985755,1.3499223832058278
740,1492,Generalized linear models,http://dbpedia.org/resource/Generalized_linear_models,http://en.wikipedia.org/wiki/Generalized_linear_models,,4602618375548754766,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],"['Generalized linear models', 'Poisson regression', 'Logically', 'Poisson distribution', 'Bernoulli distribution', ""Newton's method"", 'Fisher information', 'Gibbs sampling']",set(),set(),0,0.06756756756756757,0.713701748071767
741,1494,Efficient estimator,http://dbpedia.org/resource/Efficient_estimator,http://en.wikipedia.org/wiki/Efficient_estimator,,5282977645767259394,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],"['Fisher information', 'Efficient estimator', 'Poisson distribution', 'Consistency (statistics)', 'Consistency', 'Robust statistics', 'M-estimator']",{'Estimator'},set(),0,0.18082191780821918,0.4815721907024904
742,1495,Stochastic equicontinuity,http://dbpedia.org/resource/Stochastic_equicontinuity,http://en.wikipedia.org/wiki/Stochastic_equicontinuity,"In estimation theory in statistics, stochastic equicontinuity is a property of estimators (estimation procedures) that is useful in dealing with their asymptotic behaviour as the amount of data increases. It is a version of equicontinuity used in the context of functions of random variables: that is, random functions. The property relates to the rate of convergence of sequences of random variables and requires that this rate is essentially the same within a region of the parameter space being considered. For instance, stochastic equicontinuity, along with other conditions, can be used to show uniform weak convergence, which can be used to prove the convergence of extremum estimators.",4098122055664551443,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5909090909090909,1.344816669252641
743,1496,Local asymptotic normality,http://dbpedia.org/resource/Local_asymptotic_normality,http://en.wikipedia.org/wiki/Local_asymptotic_normality,"In statistics, local asymptotic normality is a property of a sequence of statistical models, which allows this sequence to be asymptotically approximated by a normal location model, after a rescaling of the parameter. An important example when the local asymptotic normality holds is in the case of i.i.d sampling from a regular parametric model. The notion of local asymptotic normality was introduced by .",1993414168276759370,related_concept,Maximum likelihood estimation,2024-06-23 21:17:09.728,[],['Fisher information'],set(),set(),0,1.1818181818181819,1.3351991913319856
744,1500,Weka (machine learning),http://dbpedia.org/resource/Weka_(machine_learning),http://en.wikipedia.org/wiki/Weka_(machine_learning),"Waikato Environment for Knowledge Analysis (Weka), developed at the University of Waikato, New Zealand, is free software licensed under the GNU General Public License, and the companion software to the book ""Data Mining: Practical Machine Learning Tools and Techniques"".",282688269337075864,related_concept,DBSCAN,2024-06-23 21:17:09.728,['Data'],"['Data', 'SQL', 'Database']",set(),set(),0,0.9646017699115044,1.406368411038736
745,1503,R* tree,http://dbpedia.org/resource/R*_tree,http://en.wikipedia.org/wiki/R*_tree,,8053382453251079795,related_concept,DBSCAN,2024-06-23 21:17:09.728,[],['Hans-Peter Kriegel'],set(),set(),0,1.3692307692307693,0.4828336852628151
746,1504,Ball tree,http://dbpedia.org/resource/Ball_tree,http://en.wikipedia.org/wiki/Ball_tree,"In computer science, a ball tree, balltree or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of intersecting hyperspheres known as ""balls"". The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search.",8028870530290740093,related_concept,DBSCAN,2024-06-23 21:17:09.728,[],"['Algorithm', 'Euclidean distance']",set(),set(),0,5.55,1.3756688573304432
747,1506,Mlpack,http://dbpedia.org/resource/Mlpack,http://en.wikipedia.org/wiki/Mlpack,"mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library. mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users. Its intended target users are scientists and engineers. It is open-source software distributed under the BSD license, making it useful for developing both open source and proprietary software. Releases 1.0.11 and before were released under the LGPL license. The project is supported by the Georgia Institute of Technology and contributions from around the world.",4463022939307793818,related_concept,DBSCAN,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.35,4.773922502518821e-304
748,1507,Nearest neighbor graph,http://dbpedia.org/resource/Nearest_neighbor_graph,http://en.wikipedia.org/wiki/Nearest_neighbor_graph,"The nearest neighbor graph (NNG) is a directed graph defined for a set of points in a metric space, such as the Euclidean distance in the plane. The NNG has a vertex for each point, and a directed edge from p to q whenever q is a nearest neighbor of p, a point whose distance from p is minimum among all the given points other than p itself. In many uses of these graphs, the directions of the edges are ignored and the NNG is defined instead as an undirected graph. However, the nearest neighbor relation is not a symmetric one, i.e., p from the definition is not necessarily a nearest neighbor for q. In theoretical discussions of algorithms a kind of general position is often assumed, namely, the nearest (k-nearest) neighbor is unique for each object. In implementations of the algorithms it is necessary to bear in mind that this is not always the case. For situations in which it is necessary to make the nearest neighbor for each object unique, the set P may be indexed and in the case of a tie the object with, e.g., the largest index may be taken as the nearest neighbor. The k-nearest neighbor graph (k-NNG) is a graph in which two vertices p and q are connected by an edge, if the distance between p and q is among the k-th smallest distances from p to other objects from P. The NNG is a special case of the k-NNG, namely it is the 1-NNG. k-NNGs obey a separator theorem: they can be partitioned into two subgraphs of at most n(d + 1)/(d + 2) vertices each by the removal of O(k1/dn1 − 1/d) points. Another variation is the farthest neighbor graph (FNG), in which each point is connected by an edge to the farthest point from it, instead of the nearest point. NNGs for points in the plane as well as in multidimensional spaces find applications, e.g., in data compression, motion planning, and facilities location. In statistical analysis, the nearest-neighbor chain algorithm based on following paths in this graph can be used to find hierarchical clusterings quickly. Nearest neighbor graphs are also a subject of computational geometry. The method can be used to induce a graph on nodes with unknown connectivity.",3759226461385877544,related_concept,DBSCAN,2024-06-23 21:17:09.728,"['Nearest neighbor graph', 'Euclidean distance']","['Euclidean distance', 'Nearest neighbor graph']",set(),set(),0,0.86,1.3847564874764788
749,1508,Fixed-radius near neighbors,http://dbpedia.org/resource/Fixed-radius_near_neighbors,http://en.wikipedia.org/wiki/Fixed-radius_near_neighbors,"In computational geometry, the fixed-radius near neighbor problem is a variant of the nearest neighbor search problem. In the fixed-radius near neighbor problem, one is given as input a set of points in d-dimensional Euclidean space and a fixed distance Δ. One must design a data structure that, given a query point q, efficiently reports the points of the data structure that are within distance Δ of q. The problem has long been studied; cites a 1966 paper by Levinthal that uses this technique as part of a system for visualizing molecular structures, and it has many other applications.",2252454505940606402,related_concept,DBSCAN,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2666666666666666,1.3719775956405935
750,1509,Curse of dimensionality,http://dbpedia.org/resource/Curse_of_dimensionality,http://en.wikipedia.org/wiki/Curse_of_dimensionality,"The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic programming. Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.",7741243778574641365,related_concept,DBSCAN,2024-06-23 21:17:09.728,['Dimension'],"['Dimension', 'Euclidean distance', 'Machine learning']",{'Dimension'},set(),0,2.6705882352941175,1.2624573157438468
751,1512,Hans-Peter Kriegel,http://dbpedia.org/resource/Hans-Peter_Kriegel,http://en.wikipedia.org/wiki/Hans-Peter_Kriegel,"Hans-Peter Kriegel (1 October 1948, Germany) is a German computer scientist and professor at the Ludwig Maximilian University of Munich and leading the Database Systems Group in the Department of Computer Science. He was previously professor at the University of Würzburg and the University of Bremen after habilitation at the Technical University of Dortmund and doctorate from Karlsruhe Institute of Technology.",7884319966434649961,related_concept,DBSCAN,2024-06-23 21:17:09.728,"['Database', 'Data', 'Hans-Peter Kriegel']","['Database', 'Data', 'Hans-Peter Kriegel', 'SUBCLU', 'Outlier', 'DBSCAN', 'ELKI']",set(),set(),0,0.6666666666666666,1.4307015555355966
752,1513,OPTICS algorithm,http://dbpedia.org/resource/OPTICS_algorithm,http://en.wikipedia.org/wiki/OPTICS_algorithm,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.",7737346698742719126,main_concept,DBSCAN,2024-06-23 21:17:09.728,"['Hans-Peter Kriegel', 'DBSCAN']","['Hans-Peter Kriegel', 'DBSCAN', 'ELKI', 'Euclidean distance']",set(),set(),0,2.108108108108108,1.1081230265725797
753,1514,SUBCLU,http://dbpedia.org/resource/SUBCLU,http://en.wikipedia.org/wiki/SUBCLU,"SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger. It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.",5866014733000262659,related_concept,DBSCAN,2024-06-23 21:17:09.728,"['Hans-Peter Kriegel', 'SUBCLU', 'DBSCAN']","['Hans-Peter Kriegel', 'SUBCLU', 'DBSCAN', 'Apriori algorithm']",set(),set(),0,5.266666666666667,0.9271209810895722
754,1515,Spatial index,http://dbpedia.org/resource/Spatial_index,http://en.wikipedia.org/wiki/Spatial_index,,5924088204562190103,related_concept,DBSCAN,2024-06-23 21:17:09.728,[],"['SQL', 'Database', 'Data', 'PostGIS']",set(),set(),0,0.8920863309352518,0.5690695705971773
755,1518,PostGIS,http://dbpedia.org/resource/PostGIS,http://en.wikipedia.org/wiki/PostGIS,PostGIS (/ˈpoʊstdʒɪs/ POST-jis) is an open source software program that adds support for geographic objects to the PostgreSQL object-relational database. PostGIS follows the Simple Features for SQL specification from the Open Geospatial Consortium (OGC). Technically PostGIS was implemented as a PostgreSQL external extension.,3892370922253628300,related_concept,DBSCAN,2024-06-23 21:17:09.728,"['SQL', 'PostGIS']","['SQL', 'PostGIS']",set(),set(),0,1.7358490566037736,3.842996140132889e-304
756,1521,Birch (data clustering),http://dbpedia.org/resource/Birch_(data_clustering),http://en.wikipedia.org/wiki/Birch_(data_clustering),,773822811675605407,related_concept,Cluster analysis,2024-06-23 21:17:09.728,[],"['DBSCAN', 'Manhattan distance']",set(),set(),0,0.012903225806451613,0.5572412495937502
757,1522,Human genetic clustering,http://dbpedia.org/resource/Human_genetic_clustering,http://en.wikipedia.org/wiki/Human_genetic_clustering,"Human genetic clustering refers to patterns of relative genetic similarity among human individuals and populations, as well as the wide range of scientific and statistical methods used to study this aspect of human genetic variation. Clustering studies are thought to be valuable for characterizing the general structure of genetic variation among human populations, to contribute to the study of ancestral origins, evolutionary history, and precision medicine. Since the mapping of the human genome, and with the availability of increasingly powerful analytic tools, cluster analyses have revealed a range of ancestral and migratory trends among human populations and individuals. Human genetic clusters tend to be organized by geographic ancestry, with divisions between clusters aligning largely with geographic barriers such as oceans or mountain ranges. Clustering studies have been applied to global populations, as well as to population subsets like post-colonial North America. Notably, the practice of defining clusters among modern human populations is largely arbitrary and variable due to the continuous nature of human genotypes; although individual genetic markers can be used to produce smaller groups, there are no models that produce completely distinct subgroups when larger numbers of genetic markers are used. Many studies of human genetic clustering have been implicated in discussions of race, ethnicity, and scientific racism, as some have controversially suggested that genetically derived clusters may be understood as proof of genetically determined races. Although cluster analyses invariably organize humans (or groups of humans) into subgroups, debate is ongoing on how to interpret these genetic clusters with respect to race and its social and phenotypic features. And, because there is such a small fraction of genetic variation between human genotypes overall, genetic clustering approaches are highly dependent on the sampled data, genetic markers, and statistical methods applied to their construction.",3943765082126855815,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Human genetic clustering'],['Human genetic clustering'],set(),set(),0,0.49333333333333335,1.3475853142502479
758,1523,Anomaly detection,http://dbpedia.org/resource/Anomaly_detection,http://en.wikipedia.org/wiki/Anomaly_detection,"In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data. Anomaly detection finds application in many domains including cyber security, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers. Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.",2624939064024773217,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Anomaly detection'],"['Anomaly detection', 'Statistics']",set(),set(),0,1.8986013986013985,1.4024704634830643
759,1524,Consensus clustering,http://dbpedia.org/resource/Consensus_clustering,http://en.wikipedia.org/wiki/Consensus_clustering,"Consensus clustering is a method of aggregating (potentially conflicting) results from multiple clustering algorithms. Also called cluster ensembles or aggregation of clustering (or partitions), it refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Consensus clustering is thus the problem of reconciling clustering information about the same data set coming from different sources or from different runs of the same algorithm. When cast as an optimization problem, consensus clustering is known as median partition, and has been shown to be NP-complete, even when the number of input clusterings is three. Consensus clustering for unsupervised learning is analogous to ensemble learning in supervised learning.",152364422828870574,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Consensus clustering'],"['Consensus clustering', 'Iterative']",set(),set(),0,2.588235294117647,0.8707056414165827
760,1525,Determining the number of clusters in a data set,http://dbpedia.org/resource/Determining_the_number_of_clusters_in_a_data_set,http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set,"Determining the number of clusters in a data set, a quantity often labelled k as in the k-means algorithm, is a frequent problem in data clustering, and is a distinct issue from the process of actually solving the clustering problem. For a certain class of clustering algorithms (in particular k-means, k-medoids and expectation–maximization algorithm), there is a parameter commonly referred to as k that specifies the number of clusters to detect. Other algorithms such as DBSCAN and OPTICS algorithm do not require the specification of this parameter; hierarchical clustering avoids the problem altogether. The correct choice of k is often ambiguous, with interpretations depending on the shape and scale of the distribution of points in a data set and the desired clustering resolution of the user. In addition, increasing k without penalty will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when k equals the number of data points, n). Intuitively then, the optimal choice of k will strike a balance between maximum compression of the data using a single cluster, and maximum accuracy by assigning each data point to its own cluster. If an appropriate value of k is not apparent from prior knowledge of the properties of the data set, it must be chosen somehow. There are several categories of methods for making this decision.",5832936490700100683,related_concept,Cluster analysis,2024-06-23 21:17:09.728,"['OPTICS algorithm', 'Determining the number of clusters in a data set', 'DBSCAN']","['Determining the number of clusters in a data set', 'OPTICS algorithm', 'DBSCAN', 'F-test', 'Akaike information criterion', 'AI']",set(),set(),0,1.625,0.6223930501764644
761,1528,K-means++,http://dbpedia.org/resource/K-means++,http://en.wikipedia.org/wiki/K-means++,"In data mining, k-means++ is an algorithm for choosing the initial values (or ""seeds"") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)",5042180311254941731,related_concept,Cluster analysis,2024-06-23 21:17:09.728,[],"[""Lloyd's algorithm""]",set(),set(),0,3.6153846153846154,3.313386621701347e-304
762,1529,Automatic clustering algorithms,http://dbpedia.org/resource/Automatic_clustering_algorithms,http://en.wikipedia.org/wiki/Automatic_clustering_algorithms,"Automatic clustering algorithms are algorithms that can perform clustering without prior knowledge of data sets. In contrast with other cluster analysis techniques, automatic clustering algorithms can determine the optimal number of clusters even in the presence of noise and outlier points.",3119231970841322027,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Automatic clustering algorithms'],"['Automatic clustering algorithms', 'K-means clustering', 'DBSCAN', 'Algorithm']",set(),set(),0,1.1363636363636365,0.6908112336056229
763,1531,Conceptual clustering,http://dbpedia.org/resource/Conceptual_clustering,http://en.wikipedia.org/wiki/Conceptual_clustering,"Conceptual clustering is a machine learning paradigm for unsupervised classification that has been defined by Ryszard S. Michalski in 1980 (Fisher 1987, Michalski 1980) and developed mainly during the 1980s. It is distinguished from ordinary data clustering by generating a concept description for each generated class. Most conceptual clustering methods are capable of generating hierarchical category structures; see Categorization for more information on hierarchy. Conceptual clustering is closely related to formal concept analysis, decision tree learning, and mixture model learning.",787873636641637568,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Conceptual clustering'],['Conceptual clustering'],set(),set(),0,2.8260869565217392,1.340227106984606
764,1532,Fuzzy clustering,http://dbpedia.org/resource/Fuzzy_clustering,http://en.wikipedia.org/wiki/Fuzzy_clustering,"Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster. Clustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, while items belonging to different clusters are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application.",7039616381421709135,main_concept,Cluster analysis,2024-06-23 21:17:09.728,['Fuzzy clustering'],"['Fuzzy clustering', 'K-means clustering', 'HSL and HSV', 'Image segmentation']",set(),set(),0,2.1059602649006623,0.8333667123499955
765,1536,Cluster-weighted modeling,http://dbpedia.org/resource/Cluster-weighted_modeling,http://en.wikipedia.org/wiki/Cluster-weighted_modeling,"In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space. The overall approach works in jointly input-output space and an initial version was proposed by Neil Gershenfeld.",3191988068347074452,related_concept,Cluster analysis,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.315789473684211,1.40406964368504
766,1537,Canopy clustering algorithm,http://dbpedia.org/resource/Canopy_clustering_algorithm,http://en.wikipedia.org/wiki/Canopy_clustering_algorithm,"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.",2703937590785135060,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Hierarchical clustering'],['Hierarchical clustering'],set(),set(),0,10.625,3.305127651380727e-304
767,1538,Correlation clustering,http://dbpedia.org/resource/Correlation_clustering,http://en.wikipedia.org/wiki/Correlation_clustering,Clustering is the problem of partitioning data points into groups based on their similarity. Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.,4509107331319984201,related_concept,Cluster analysis,2024-06-23 21:17:09.728,['Correlation clustering'],"['Correlation clustering', 'Clustering high-dimensional data']",set(),set(),0,4.142857142857143,1.131817835980729
768,1539,Data stream clustering,http://dbpedia.org/resource/Data_stream_clustering,http://en.wikipedia.org/wiki/Data_stream_clustering,"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.",3614299288295443190,related_concept,Cluster analysis,2024-06-23 21:17:09.728,"['Data', 'Data stream clustering']","['Data', 'Data stream clustering', 'Median', 'Theorem', 'Algorithm']",set(),set(),0,2.7857142857142856,1.032805109802848
769,1541,Perceptron,http://dbpedia.org/resource/Perceptron,http://en.wikipedia.org/wiki/Perceptron,"In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.",7656535878041647267,related_concept,Multiclass classification,2024-06-23 21:17:09.728,[],"['Perceptron', 'AI', 'Boolean function', 'Theorem', 'Linear separability']",set(),set(),0,1.394736842105263,3.764360171011699e-304
770,1542,Multi-label classification,http://dbpedia.org/resource/Multi-label_classification,http://en.wikipedia.org/wiki/Multi-label_classification,"In machine learning, multi-label classification or multi-output classification is a variant of the classification problem where multiple nonexclusive labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of several (more than two) classes. In the multi-label problem the labels are nonexclusive and there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y; that is, it assigns a value of 0 or 1 for each element (label) in y.",8390530080949691240,related_concept,Multiclass classification,2024-06-23 21:17:09.728,['Multi-label classification'],"['Multi-label classification', 'Bayesian network', 'Data']",set(),set(),0,1.4651162790697674,1.3737972713007547
771,1543,Hierarchical classification,http://dbpedia.org/resource/Hierarchical_classification,http://en.wikipedia.org/wiki/Hierarchical_classification,"Hierarchical classification is a system of grouping things according to a hierarchy. In the field of machine learning, hierarchical classification is sometimes referred to as instance space decomposition, which splits a complete multi-class problem into a set of smaller classification problems.",4996806764668992706,related_concept,Multiclass classification,2024-06-23 21:17:09.728,['Hierarchical classification'],['Hierarchical classification'],set(),set(),0,1.2592592592592593,1.3703401039156
772,1544,Online machine learning,http://dbpedia.org/resource/Online_machine_learning,http://en.wikipedia.org/wiki/Online_machine_learning,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction.Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.",6371900109286512137,related_concept,Multiclass classification,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.9065934065934067,4.798853225461168e-304
773,1547,Softmax function,http://dbpedia.org/resource/Softmax_function,http://en.wikipedia.org/wiki/Softmax_function,"The softmax function, also known as softargmax or normalized exponential function, converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.",8362738829241833350,related_concept,Multiclass classification,2024-06-23 21:17:09.728,[],"['Softmax function', 'Bayes classifier', 'Computation']",set(),set(),0,0.8567073170731707,1.4335436067370775
774,1548,Multi-task learning,http://dbpedia.org/resource/Multi-task_learning,http://en.wikipedia.org/wiki/Multi-task_learning,"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called ""hints"". In a widely cited 1997 paper, Rich Caruana gave the following characterization: Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification. Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.",9214160858635795836,related_concept,Multiclass classification,2024-06-23 21:17:09.728,['Multi-task learning'],['Multi-task learning'],set(),set(),0,5.7,1.3345073874385787
775,1549,One-class classification,http://dbpedia.org/resource/One-class_classification,http://en.wikipedia.org/wiki/One-class_classification,"In machine learning, one-class classification (OCC), also known as unary classification or class-modelling, tries to identify objects of a specific class amongst all objects, by primarily learning from a training set containing only the objects of that class, although there exist variants of one-class classifiers where counter-examples are used to further refine the classification boundary. This is different from and more difficult than the traditional classification problem, which tries to distinguish between two or more classes with the training set containing objects from all the classes. Examples include the monitoring of helicopter gearboxes, motor failure prediction, or the operational status of a nuclear plant as 'normal': In this scenario, there are few, if any, examples of catastrophic system states; only the statistics of normal operation are known. While many of the above approaches focus on the case of removing a small number of outliers or anomalies, one can also learn the other extreme, where the single class covers a small coherent subset of the data, using an information bottleneck approach.",8324823946696796240,related_concept,Multiclass classification,2024-06-23 21:17:09.728,[],"['Data', 'Poisson distribution', 'Density estimation', 'Theorem', 'One-class classification']",set(),set(),0,2.206896551724138,1.3840961874503512
776,1550,Multinomial logistic regression,http://dbpedia.org/resource/Multinomial_logistic_regression,http://en.wikipedia.org/wiki/Multinomial_logistic_regression,"In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.). Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, softmax regression, multinomial logit (mlogit), the maximum entropy (MaxEnt) classifier, and the conditional maximum entropy model.",8307022181306044154,related_concept,Multiclass classification,2024-06-23 21:17:09.728,['Multinomial logistic regression'],"['Multinomial logistic regression', 'Bayes classifier']",set(),set(),0,1.2296296296296296,1.4331719846855286
777,1551,K-nearest neighbor,http://dbpedia.org/resource/K-nearest_neighbor,http://en.wikipedia.org/wiki/K-nearest_neighbor,,4701235180256385018,related_concept,Multiclass classification,2024-06-23 21:17:09.728,[],"['Euclidean distance', 'K-nearest neighbor', 'Feature extraction', 'Data reduction', 'Data']",set(),set(),0,0.10869565217391304,0.5527652798877463
778,1552,Conditional independence,http://dbpedia.org/resource/Conditional_independence,http://en.wikipedia.org/wiki/Conditional_independence,"In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If is the hypothesis, and and are observations, conditional independence can be stated as an equality: where is the probability of given both and . Since the probability of given is the same as the probability of given both and , this equality expresses that contributes nothing to the certainty of . In this case, and are said to be conditionally independent given , written symbolically as: . The concept of conditional independence is essential to graph-based theories of statistical inference, as it establishes a mathematical relation between a collection of conditional statements and a graphoid.",8221903369244492609,main_concept,Multiclass classification,2024-06-23 21:17:09.728,['Conditional independence'],"['Conditional independence', 'Graphoid', 'Axiom', 'Law of total probability']",set(),set(),0,2.183098591549296,1.3437516475804847
779,1556,Machine learning,http://dbpedia.org/resource/Machine_learning,http://en.wikipedia.org/wiki/Machine_learning,"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain. In its application across business problems, machine learning is also referred to as predictive analytics.",8262793725897823310,main_concept,Multiclass classification,2024-06-23 21:17:09.728,"['Data', 'Machine learning', 'Data mining']","['Machine learning', 'Data', 'Data mining', 'AI', 'Neural network', 'MATLAB', 'K-means clustering', 'Loss function', 'Supervised learning', 'Classification', 'Unsupervised learning', 'Cluster analysis', 'Semi-supervised learning', 'Dimension', 'Dimensionality reduction', 'Feature learning', 'Multilinear subspace learning', 'Deep learning', 'Association rule learning', 'Rule-based machine learning', 'Learning classifier system', 'Prolog', 'Inference', 'Artificial neural network', 'Artificial neurons', 'Decision tree learning', 'Decision tree', 'Support-vector machine', 'Regression analysis', 'Bayesian network', 'Bayesian optimization', 'Overfitting', 'Algorithm']",set(),set(),0,6.36075205640423,1.1010894369160507
780,1559,Heuristic,http://dbpedia.org/resource/Heuristic,http://en.wikipedia.org/wiki/Heuristic,"A heuristic (/hjʊˈrɪstɪk/; from Ancient Greek εὑρίσκω (heurískō) 'I find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess. Heuristics are the strategies derived from previous experiences with similar problems. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues. When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors. The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification. Here are a few commonly used heuristics from George Pólya's 1945 book, How to Solve It: 
* When experiencing a difficulty in understanding a problem, draw the architecture from all directions e.g. top-view, side-view, front-view. 
* If you can't find a solution, try assuming that you have a solution and seeing what you can derive from that (""working backward""). AKA ""what shape would it have"" aka system-requirements. 
* If the problem is abstract, try examining a concrete example. 
* Try solving a more general problem first (the ""inventor's paradox"": the more ambitious plan may have more chances of success). This is because only the general problem can provide to a specific problem—a context from which to draw meaning. In psychology, heuristics are simple, efficient rules, either learned or inculcated by evolutionary processes. These psychological heuristics have been proposed to explain how people make decisions, come to judgements, and solve problems. These rules typically come into play when people face complex problems or incomplete information. Researchers employ various methods to test whether people use these rules. The rules have been shown to work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.",4567450628990870451,related_concept,Multiclass classification,2024-06-23 21:17:09.728,['Heuristic'],"['Evolution', 'Machine learning', 'Evolutionary robotics', 'Genetic programming', 'Genetic algorithm', 'Partial differential equation', 'Ordinary differential equation', 'Heuristic', 'Bayesian inference']",set(),set(),0,2.358108108108108,1.024799592941572
781,1561,Multiclass classification,http://dbpedia.org/resource/Multiclass_classification,http://en.wikipedia.org/wiki/Multiclass_classification,"In machine learning and statistical classification, multiclass classification or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification). While many classification algorithms (notably multinomial logistic regression) naturally permit the use of more than two classes, some are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies. Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.",1500767647251245361,main_concept,,2024-06-23 21:17:09.728,['Multiclass classification'],"['Multiclass classification', 'Extreme learning machine', 'Decision tree learning', 'Decision tree', 'Multi expression programming', 'Hierarchical classification']",set(),set(),0,0.7019867549668874,1.3915988769688046
782,1563,Box plot,http://dbpedia.org/resource/Box_plot,http://en.wikipedia.org/wiki/Box_plot,"In descriptive statistics, a box plot or boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also termed as the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot.Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings in each subsection of the box-plot indicate the degree of dispersion (spread) and skewness of the data, which are usually described using the five-number summary. In addition, the box-plot allows one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Box plots can be drawn either horizontally or vertically.",3426670693928819580,related_concept,Interquartile range,2024-06-23 21:17:09.728,"['Box plot', 'Outlier']","['Box plot', 'Outlier', 'Data', 'Statistics', 'Median']",set(),set(),0,1.9230769230769231,1.044908706158791
783,1564,Standard deviation,http://dbpedia.org/resource/Standard_deviation,http://en.wikipedia.org/wiki/Standard_deviation,"In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter σ (sigma), for the population standard deviation, or the Latin letter s, for the sample standard deviation. The standard deviation of a random variable, sample, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. The standard deviation of a population or sample and the standard error of a statistic (e.g., of the sample mean) are quite different, but related. The sample mean's standard error is the standard deviation of the set of means that would be found by drawing an infinite number of repeated samples from the population and computing a mean for each sample. The mean's standard error turns out to equal the population standard deviation divided by the square root of the sample size, and is estimated by using the sample standard deviation divided by the square root of the sample size. For example, a poll's standard error (what is reported as the margin of error of the poll), is the expected standard deviation of the estimated mean if the same poll were to be conducted multiple times. Thus, the standard error estimates the standard deviation of an estimate, which itself measures how much the estimate depends on the particular sample that was taken from the population. In science, it is common to report both the standard deviation of the data (as a summary statistic) and the standard error of the estimate (as a measure of potential error in the findings). By convention, only effects more than two standard errors away from a null expectation are considered ""statistically significant"", a safeguard against spurious conclusion that is really due to random sampling error. When only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data, or to a modified quantity that is an unbiased estimate of the population standard deviation (the standard deviation of the entire population).",6325914790773521428,related_concept,Interquartile range,2024-06-23 21:17:09.728,['Standard deviation'],"['Standard deviation', '68–95–99.7 rule', 'Cauchy distribution', 'Statistical independence']",set(),set(),0,4.335249042145594,1.3003336131167464
784,1565,Midhinge,http://dbpedia.org/resource/Midhinge,http://en.wikipedia.org/wiki/Midhinge,"In statistics, the midhinge is the average of the first and third quartiles and is thus a measure of location.Equivalently, it is the 25% trimmed mid-range or 25% midsummary; it is an L-estimator. The midhinge is related to the interquartile range (IQR), the difference of the third and first quartiles (i.e. ), which is a measure of statistical dispersion. The two are complementary in sense that if one knows the midhinge and the IQR, one can find the first and third quartiles. The use of the term ""hinge"" for the lower or upper quartiles derives from John Tukey's work on exploratory data analysis in the late 1970s, and ""midhinge"" is a fairly modern term dating from around that time. The midhinge is slightly simpler to calculate than the trimean, which originated in the same context and equals the average of the median and the midhinge.",8616668645639145154,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.5625,4.166636461517389e-304
785,1566,Range (statistics),http://dbpedia.org/resource/Range_(statistics),http://en.wikipedia.org/wiki/Range_(statistics),"In statistics, the range of a set of data is the difference between the largest and smallest values,the result of subtracting the sample maximum and minimum. It is expressed in the same units as the data. In descriptive statistics, range is the size of the smallest interval which contains all the data and provides an indication of statistical dispersion. Since it only depends on two of the observations, it is most useful in representing the dispersion of small data sets.",9029023013560544501,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.8149253731343284,1.2770009664286075
786,1567,Interdecile range,http://dbpedia.org/resource/Interdecile_range,http://en.wikipedia.org/wiki/Interdecile_range,"In statistics, the interdecile range is the difference between the first and the ninth deciles (10% and 90%). The interdecile range is a measure of statistical dispersion of the values in a set of data, similar to the range and the interquartile range, and can be computed from the (non-parametric) seven-number summary. Despite its simplicity, the interdecile range of a sample drawn from a normal distribution can be divided by 2.56 to give a reasonably efficient estimator of the standard deviation of a normal distribution. This is derived from the fact that the lower (respectively upper) decile of a normal distribution with arbitrary variance is equal to the mean minus (respectively, plus) 1.28 times the standard deviation. A more efficient estimator is given by instead taking the 7% trimmed range (the difference between the 7th and 93rd percentiles) and dividing by 3 (corresponding to 86% of the data falling within ±1.5 standard deviations of the mean in a normal distribution); this yields an estimator having about 65% efficiency. Analogous measures of location are given by the median, midhinge, and trimean (or statistics based on nearby points).",5735155440504071102,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.7272727272727273,4.407634557224605e-304
787,1568,Median absolute deviation,http://dbpedia.org/resource/Median_absolute_deviation,http://en.wikipedia.org/wiki/Median_absolute_deviation,"In statistics, the median absolute deviation (MAD) is a robust measure of the variability of a univariate sample of quantitative data. It can also refer to the population parameter that is estimated by the MAD calculated from a sample. For a univariate data set X1, X2, ..., Xn, the MAD is defined as the median of the absolute deviations from the data's median : that is, starting with the residuals (deviations) from the data's median, the MAD is the median of their absolute values.",4164115807134959491,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],['Cauchy distribution'],set(),set(),0,0.2802056555269923,4.189883229428374e-304
788,1569,Robust statistics,http://dbpedia.org/resource/Robust_statistics,http://en.wikipedia.org/wiki/Robust_statistics,"Robust statistics are statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. Robust statistical methods have been developed for many common problems, such as estimating location, scale, and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. Another motivation is to provide methods with good performance when there are small departures from a parametric distribution. For example, robust methods work well for mixtures of two normal distributions with different standard deviations; under this model, non-robust methods like a t-test work poorly.",4766227595503824230,related_concept,Interquartile range,2024-06-23 21:17:09.728,['Robust statistics'],"['Robust statistics', 'M-estimator', 'Trimmed estimator', 'Data', 'Q–Q plot', 'Outlier', 'Statistics', ""Student's t-distribution"", 'Cauchy distribution', 'Simple linear regression', 'Kalman filter']",set(),set(),0,1.8778625954198473,1.3368306854682956
789,1570,Percentiles,http://dbpedia.org/resource/Percentiles,http://en.wikipedia.org/wiki/Percentiles,,7923432421110587286,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],"['Percentiles', '68–95–99.7 rule', 'Algorithm', 'Interpolation']",set(),set(),0,0.041666666666666664,0.4994166117728777
790,1571,Robust measures of scale,http://dbpedia.org/resource/Robust_measures_of_scale,http://en.wikipedia.org/wiki/Robust_measures_of_scale,"In statistics, robust measures of scale are methods that quantify the statistical dispersion in a sample of numerical data while resisting outliers. The most common such robust statistics are the interquartile range (IQR) and the median absolute deviation (MAD). These are contrasted with conventional or non-robust measures of scale, such as sample variance or standard deviation, which are greatly influenced by outliers. These robust statistics are particularly used as estimators of a scale parameter, and have the advantages of both robustness and superior efficiency on contaminated data, at the cost of inferior efficiency on clean data from distributions such as the normal distribution. To illustrate robustness, the standard deviation can be made arbitrarily large by increasing exactly one observation (it has a breakdown point of 0, as it can be contaminated by a single point), a defect that is not shared by robust statistics.",5041421963758155875,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],"['Median absolute deviation', 'Median', 'Robust measures of scale', 'Cauchy distribution']",set(),set(),0,0.8333333333333334,1.242029117775351
791,1572,Five-number summary,http://dbpedia.org/resource/Five-number_summary,http://en.wikipedia.org/wiki/Five-number_summary,"The five-number summary is a set of descriptive statistics that provides information about a dataset. It consists of the five most important sample percentiles: 1. 
* the sample minimum (smallest observation) 2. 
* the lower quartile or first quartile 3. 
* the median (the middle value) 4. 
* the upper quartile or third quartile 5. 
* the sample maximum (largest observation) In addition to the median of a single set of data there are two related statistics called the upper and lower quartiles. If data are placed in order, then the lower quartile is central to the lower half of the data and the upper quartile is central to the upper half of the data. These quartiles are used to calculate the interquartile range, which helps to describe the spread of the data, and determine whether or not any data points are outliers. In order for these statistics to exist the observations must be from a univariate variable that can be measured on an ordinal, interval or ratio scale.",839352313116406039,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.1388888888888888,0.5082190270290297
792,1573,Q–Q plot,http://dbpedia.org/resource/Q–Q_plot,http://en.wikipedia.org/wiki/Q–Q_plot,"In statistics, a Q–Q plot (quantile-quantile plot) is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). This defines a parametric curve where the parameter is the index of the quantile interval. If the two distributions being compared are similar, the points in the Q–Q plot will approximately lie on the identity line y = x. If the distributions are linearly related, the points in the Q–Q plot will approximately lie on a line, but not necessarily on the line y = x. Q–Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q–Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Q–Q plots can be used to compare collections of data, or theoretical distributions. The use of Q–Q plots to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions. A Q–Q plot is generally more diagnostic than comparing the samples' histograms, but is less widely known. Q–Q plots are commonly used to compare a data set to a theoretical model. This can provide an assessment of goodness of fit that is graphical, rather than reducing to a numerical summary statistic. Q–Q plots are also used to compare two theoretical distributions to each other. Since Q–Q plots compare distributions, there is no need for the values to be observed as pairs, as in a scatter plot, or even for the numbers of values in the two groups being compared to be equal. The term ""probability plot"" sometimes refers specifically to a Q–Q plot, sometimes to a more general class of plots, and sometimes to the less commonly used P–P plot. The probability plot correlation coefficient plot (PPCC plot) is a quantity derived from the idea of Q–Q plots, which measures the agreement of a fitted distribution with observed data and which is sometimes used as a means of fitting a distribution to data.",8120161940315974696,related_concept,Interquartile range,2024-06-23 21:17:09.728,[],['Q–Q plot'],set(),set(),0,1.5754189944134078,1.1559532100555208
793,1574,Median,http://dbpedia.org/resource/Median,http://en.wikipedia.org/wiki/Median,"In statistics and probability theory, the median is the value separating the higher half from the lower half of a data sample, a population, or a probability distribution. For a data set, it may be thought of as ""the middle"" value. The basic feature of the median in describing data compared to the mean (often simply described as the ""average"") is that it is not skewed by a small proportion of extremely large or small values, and therefore provides a better representation of a ""typical"" value. Median income, for example, may be a better way to suggest what a ""typical"" income is, because income distribution can be very skewed. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median is not an arbitrarily large or small result.",3451144256491131809,related_concept,Interquartile range,2024-06-23 21:17:09.728,['Median'],"['Median', 'Cauchy distribution', 'PDF']",set(),set(),0,5.263948497854077,4.095915436554105e-304
794,1575,Trimmed estimator,http://dbpedia.org/resource/Trimmed_estimator,http://en.wikipedia.org/wiki/Trimmed_estimator,"In statistics, a trimmed estimator is an estimator derived from another estimator by excluding some of the extreme values, a process called truncation. This is generally done to obtain a more robust statistic, and the extreme values are considered outliers. Trimmed estimators also often have higher efficiency for mixture distributions and heavy-tailed distributions than the corresponding untrimmed estimator, at the cost of lower efficiency for other distributions, such as the normal distribution. Given an estimator, the x% trimmed version is obtained by discarding the x% lowest or highest observations or on both end: it is a statistic on the middle of the data. For instance, the 5% trimmed mean is obtained by taking the mean of the 5% to 95% range. In some cases a trimmed estimator discards a fixed number of points (such as maximum and minimum) instead of a percentage.",8550387540462359410,related_concept,Interquartile range,2024-06-23 21:17:09.728,['Trimmed estimator'],"['Trimmed estimator', 'Quantile']",{'Estimator'},set(),0,0.6521739130434783,1.0826930488801827
795,1576,Descriptive statistics,http://dbpedia.org/resource/Descriptive_statistics,http://en.wikipedia.org/wiki/Descriptive_statistics,"A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features from a collection of information, while descriptive statistics (in the mass noun sense) is the process of using and analysing those statistics. Descriptive statistic is distinguished from inferential statistics (or inductive statistics) by its aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, is not developed on the basis of probability theory, and are frequently nonparametric statistics. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example, in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups (e.g., for each treatment or exposure group), and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, the proportion of subjects with related co-morbidities, etc. Some measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness.",7633395536364901334,main_concept,Interquartile range,2024-06-23 21:17:09.728,[],"['Descriptive statistics', 'Univariate analysis']",set(),set(),0,1.9975786924939467,1.3396236211080819
796,1577,Statistical dispersion,http://dbpedia.org/resource/Statistical_dispersion,http://en.wikipedia.org/wiki/Statistical_dispersion,"In statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range. For instance, when the variance of data in a set is large, the data is widely scattered. On the other hand, when the variance is small, the data in the set is clustered. Dispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.",6463812826676068782,main_concept,Interquartile range,2024-06-23 21:17:09.728,[],['Robust measures of scale'],set(),set(),0,2.0338983050847457,1.1815105014467262
797,1580,Outlier,http://dbpedia.org/resource/Outlier,http://en.wikipedia.org/wiki/Outlier,"In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses. Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model. In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition). Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations. Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175 °C, the median of the data will be between 20 and 25 °C but the mean temperature will be between 35.5 and 40 °C. In this case, the median better reflects the temperature of a randomly sampled object (but not the temperature in the room) than the mean; naively interpreting the mean as ""a typical sample"", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set. Estimators capable of coping with outliers are said to be robust: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally a more precise estimator.",4904107209897675461,main_concept,Interquartile range,2024-06-23 21:17:09.728,"['Estimator', 'Outlier']","['Outlier', 'Estimator', 'Poisson distribution', 'Box plot', 'Mean', 'Cauchy distribution']",set(),set(),0,6.87378640776699,1.3297171003536001
798,1581,Probable error,http://dbpedia.org/resource/Probable_error,http://en.wikipedia.org/wiki/Probable_error,"In statistics, probable error defines the half-range of an interval about a central point for the distribution, such that half of the values from the distribution will lie within the interval and half outside. Thus for a symmetric distribution it is equivalent to half the interquartile range, or the median absolute deviation. One such use of the term probable error in this sense is as the name for the scale parameter of the Cauchy distribution, which does not have a standard deviation. The probable error can also be expressed as a multiple of the standard deviation σ, which requires that at least the second statistical moment of the distribution should exist, whereas the other definition does not. For a normal distribution this is (see details)",598299222756400504,related_concept,Interquartile range,2024-06-23 21:17:09.728,['Cauchy distribution'],['Cauchy distribution'],set(),set(),0,2.2,1.3060564954255318
799,1582,Interquartile range,http://dbpedia.org/resource/Interquartile_range,http://en.wikipedia.org/wiki/Interquartile_range,"In descriptive statistics, the interquartile range (IQR) is a measure of statistical dispersion, which is the spread of the data. The IQR may also be called the midspread, middle 50%, fourth spread, or H‑spread. It is defined as the difference between the 75th and 25th percentiles of the data. To calculate the IQR, the data set is divided into quartiles, or four rank-ordered even parts via linear interpolation. These quartiles are denoted by Q1 (also called the lower quartile), Q2 (the median), and Q3 (also called the upper quartile). The lower quartile corresponds with the 25th percentile and the upper quartile corresponds with the 75th percentile, so IQR = Q3 − Q1. The IQR is an example of a trimmed estimator, defined as the 25% trimmed range, which enhances the accuracy of dataset statistics by dropping lower contribution, outlying points. It is also used as a robust measure of scale It can be clearly visualized by the box on a Box plot.",8996878844402317514,main_concept,,2024-06-23 21:17:09.728,['Box plot'],"['Five-number summary', 'PDF', 'Q–Q plot']",set(),set(),0,2.136094674556213,0.5552893905625045
800,1585,Decision tree learning,http://dbpedia.org/resource/Decision_tree_learning,http://en.wikipedia.org/wiki/Decision_tree_learning,"Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).",6751355774191080998,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Decision tree learning', 'Decision tree']","['Decision tree learning', 'Decision tree', 'Data', 'Algorithm', 'Confusion matrix', 'Evolution', 'Evolutionary algorithms']",set(),set(),0,2.2690582959641254,1.404292210521578
801,1586,Causal graph,http://dbpedia.org/resource/Causal_graph,http://en.wikipedia.org/wiki/Causal_graph,"In statistics, econometrics, epidemiology, genetics and related disciplines, causal graphs (also known as path diagrams, causal Bayesian networks or DAGs) are probabilistic graphical models used to encode assumptions about the data-generating process. Causal graphs can be used for communication and for inference. As communication devices, the graphs provide formal and transparent representation of the causal assumptions that researchers may wish to convey and defend. As inference tools, the graphs enable researchers to estimate effect sizes from non-experimental data, derive testable implications of the assumptions encoded, test for external validity, and manage missing data and selection bias. Causal graphs were first used by the geneticist Sewall Wright under the rubric ""path diagrams"". They were later adopted by social scientists and, to a lesser extent, by economists. These models were initially confined to linear equations with fixed parameters. Modern developments have extended graphical models to non-parametric analysis, and thus achieved a generality and flexibility that has transformed causal analysis in computer science, epidemiology, and social science.",8311452375439812248,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Causal graph', 'Bayesian network']","['Bayesian network', 'Causal graph']",set(),set(),0,1.65,1.3854099247135907
802,1587,Variable elimination,http://dbpedia.org/resource/Variable_elimination,http://en.wikipedia.org/wiki/Variable_elimination,"Variable elimination (VE) is a simple and general exact inference algorithm in probabilistic graphical models, such as Bayesian networks and Markov random fields. It can be used for inference of maximum a posteriori (MAP) state or estimation of conditional or marginal distributions over a subset of variables. The algorithm has exponential time complexity, but could be efficient in practice for low-treewidth graphs, if the proper elimination order is used.",6492999066594578822,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Variable elimination', 'Bayesian network']","['Variable elimination', 'Bayesian network', 'Algorithm']",set(),set(),0,1.75,1.379634872800461
803,1589,Variable-order Bayesian network,http://dbpedia.org/resource/Variable-order_Bayesian_network,http://en.wikipedia.org/wiki/Variable-order_Bayesian_network,"Variable-order Bayesian network (VOBN) models provide an important extension of both the Bayesian network models and the variable-order Markov models. VOBN models are used in machine learning in general and have shown great potential in bioinformatics applications.These models extend the widely used position weight matrix (PWM) models, Markov models, and Bayesian network (BN) models. In contrast to the BN models, where each random variable depends on a fixed subset of random variables, in VOBN models these subsets may vary based on the specific realization of observed variables. The observed realizations are often called the context and, hence, VOBN models are also known as context-specific Bayesian networks.The flexibility in the definition of conditioning subsets of variables turns out to be a real advantage in classification and analysis applications, as the statistical dependencies between random variables in a sequence of variables (not necessarily adjacent) may be taken into account efficiently, and in a position-specific and context-specific manner.",6254619534669181862,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Variable-order Bayesian network', 'Bayesian network']","['Variable-order Bayesian network', 'Bayesian network']",set(),set(),0,1.0526315789473684,1.3749994421491716
804,1590,Influence diagram,http://dbpedia.org/resource/Influence_diagram,http://en.wikipedia.org/wiki/Influence_diagram,"An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation. It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved. ID was first developed in the mid-1970s by decision analysts with an intuitive semantic that is easy to understand. It is now adopted widely and becoming an alternative to the decision tree which typically suffers from exponential growth in number of branches with each variable modeled. ID is directly applicable in , since it allows incomplete sharing of information among team members to be modeled and solved explicitly. Extensions of ID also find their use in game theory as an alternative representation of the game tree.",359010662267792925,related_concept,Bayesian network,2024-06-23 21:17:09.728,['Bayesian network'],"['Bayesian network', 'Influence diagram']",set(),set(),0,2.138888888888889,1.440007484082307
805,1592,Naive Bayes classifier,http://dbpedia.org/resource/Naive_Bayes_classifier,http://en.wikipedia.org/wiki/Naive_Bayes_classifier,"In statistics, naive Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.",9008802719306266047,main_concept,Bayesian network,2024-06-23 21:17:09.728,"['Bayes classifier', 'Naive Bayes classifier', 'Bayesian network', ""Bayes' theorem""]","['Bayes classifier', 'Bayesian network', 'Naive Bayes classifier', ""Bayes' theorem"", 'Bernoulli distribution', 'Theorem']",set(),set(),0,3.712121212121212,1.4155263211678977
806,1593,Graphical model,http://dbpedia.org/resource/Graphical_model,http://en.wikipedia.org/wiki/Graphical_model,"A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.",8998835209187968933,related_concept,Bayesian network,2024-06-23 21:17:09.728,['Bayesian statistics'],"['Bayesian statistics', 'Bayesian network', 'Bayes classifier', 'Naive Bayes classifier']",set(),set(),0,1.915983606557377,1.3797790632211202
807,1594,Dynamic Bayesian network,http://dbpedia.org/resource/Dynamic_Bayesian_network,http://en.wikipedia.org/wiki/Dynamic_Bayesian_network,"A Dynamic Bayesian Network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. This is often called a Two-Timeslice BN (2TBN) because it says that at any point in time T, the value of a variable can be calculated from the internal regressors and the immediate prior value (time T-1). DBNs were developed by in the early 1990s at Stanford University's Section on Medical Informatics. Dagum developed DBNs to unify and extend traditional linear state-space models such as Kalman filters, linear and normal forecasting models such as ARMA and simple dependency models such as hidden Markov models into a general probabilistic representation and inference mechanism for arbitrary nonlinear and non-normal time-dependent domains. Today, DBNs are common in robotics, and have shown potential for a wide range of data mining applications. For example, they have been used in speech recognition, digital forensics, protein sequencing, and bioinformatics. DBN is a generalization of hidden Markov models and Kalman filters. DBNs are conceptually related to Probabilistic Boolean Networks and can, similarly, be used to model dynamical systems at steady-state.",9138911776321112876,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Kalman filter', 'Bayesian network']","['Bayesian network', 'Kalman filter']",set(),set(),0,1.6511627906976745,1.3797230825237263
808,1595,Markov network,http://dbpedia.org/resource/Markov_network,http://en.wikipedia.org/wiki/Markov_network,,4276119926781806049,related_concept,Bayesian network,2024-06-23 21:17:09.728,[],"['Markov network', 'Bayesian network', 'Inference']",set(),set(),0,0.11589403973509933,0.44657677895953596
809,1596,Loopy belief propagation,http://dbpedia.org/resource/Loopy_belief_propagation,http://en.wikipedia.org/wiki/Loopy_belief_propagation,,4450948847161249482,related_concept,Bayesian network,2024-06-23 21:17:09.728,[],"['Bayesian network', 'Viterbi algorithm']",set(),set(),0,0.03614457831325301,0.6052016403848056
810,1597,PyMC3,http://dbpedia.org/resource/PyMC3,http://en.wikipedia.org/wiki/PyMC3,,1178201845210995698,related_concept,Bayesian network,2024-06-23 21:17:09.728,[],"['PyMC3', 'Probability']",set(),set(),0,0.9253731343283582,0.38278983157577423
811,1600,Bayesian programming,http://dbpedia.org/resource/Bayesian_programming,http://en.wikipedia.org/wiki/Bayesian_programming,"Bayesian programming is a formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available. Edwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science he developed this theory and proposed what he called “the robot,” which was nota physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this ""robot"". Bayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.",6289487710379719271,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Prolog', 'Probability', 'Kalman filter', 'Bayesian programming', 'Bayesian network']","['Bayesian programming', 'Prolog', 'Probability', 'Kalman filter', 'Bayesian network', ""Bayes' theorem"", 'Conditional independence', 'Bayesian inference', 'Bayesian spam filtering', 'Viterbi algorithm', 'Baum–Welch algorithm', 'AI']",set(),set(),0,0.3803680981595092,1.3664060410366123
812,1603,Bayesian hierarchical modeling,http://dbpedia.org/resource/Bayesian_hierarchical_modeling,http://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling,"Bayesian hierarchical modelling is a statistical model written in multiple levels (hierarchical form) that estimates the parameters of the posterior distribution using the Bayesian method. The sub-models combine to form the hierarchical model, and Bayes' theorem is used to integrate them with the observed data and account for all the uncertainty that is present. The result of this integration is the posterior distribution, also known as the updated probability estimate, as additional evidence on the prior distribution is acquired. Frequentist statistics may yield conclusions seemingly incompatible with those offered by Bayesian statistics due to the Bayesian treatment of the parameters as random variables and its use of subjective information in establishing assumptions on these parameters. As the approaches answer different questions the formal results aren't technically contradictory but the two approaches disagree over which answer is relevant to particular applications. Bayesians argue that relevant information regarding decision-making and updating beliefs cannot be ignored and that hierarchical modeling has the potential to overrule classical methods in applications where respondents give multiple observational data. Moreover, the model has proven to be robust, with the posterior distribution less sensitive to the more flexible hierarchical priors. Hierarchical modeling is used when information is available on several different levels of observational units. For example, in epidemiological modeling to describe infection trajectories for multiple countries, observational units are countries, and each country has its own temporal profile of daily infected cases. In decline curve analysis to describe oil or gas production decline curve for multiple wells, observational units are oil or gas wells in a reservoir region, and each well has each own temporal profile of oil or gas production rates (usually, barrels per month). Data structure for the hierarchical modeling retains nested data structure. The hierarchical form of analysis and organization helps in the understanding of multiparameter problems and also plays an important role in developing computational strategies.",3449091867837247990,related_concept,Bayesian network,2024-06-23 21:17:09.728,"['Data structure', 'Data', 'Bayesian statistics', ""Bayes' theorem""]","[""Bayes' theorem"", 'Bayesian statistics', 'Data structure', 'Data', 'Bayesian inference', 'Bayesian hierarchical modeling']",set(),set(),0,1.4142857142857144,4.6992944111338e-304
813,1604,Bayesian network,http://dbpedia.org/resource/Bayesian_network,http://en.wikipedia.org/wiki/Bayesian_network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.",1800544024023196732,main_concept,,2024-06-23 21:17:09.728,['Bayesian network'],"['Bayesian network', 'Markov network', ""Bayes' theorem"", 'Algorithm', 'Jeffreys prior', 'AI']",set(),set(),0,3.9791666666666665,1.3786094152511674
814,1605,Cumulative distribution function,http://dbpedia.org/resource/Cumulative_distribution_function,http://en.wikipedia.org/wiki/Cumulative_distribution_function,"In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable , or just distribution function of , evaluated at , is the probability that will take a value less than or equal to . Every probability distribution supported on the real numbers, discrete or ""mixed"" as well as continuous, is uniquely identified by an upwards continuous monotonic increasing cumulative distribution function satisfying and . In the case of a scalar continuous distribution, it gives the area under the probability density function from minus infinity to . Cumulative distribution functions are also used to specify the distribution of multivariate random variables.",8201061403766847949,related_concept,Probability density function,2024-06-23 21:17:09.728,['Cumulative distribution function'],"['Cumulative distribution function', 'Poisson distribution', 'Calculus', 'Theorem']",set(),set(),0,8.53,0.8323937386445814
815,1606,Law of the unconscious statistician,http://dbpedia.org/resource/Law_of_the_unconscious_statistician,http://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician,"In probability theory and statistics, the law of the unconscious statistician, or LOTUS, is a theorem used to calculate the expected value of a function g(X) of a random variable X when one knows the probability distribution of X but one does not know the distribution of g(X). The form of the law can depend on the form in which one states the probability distribution of the random variable X. If it is a discrete distribution and one knows its probability mass function ƒX (but not ƒg(X)), then the expected value of g(X) is where the sum is over all possible values x of X. If it is a continuous distribution and one knows its probability density function ƒX (but not ƒg(X)), then the expected value of g(X) is If one knows the cumulative probability distribution function FX (but not Fg(X)), then the expected value of g(X) is given by a Riemann–Stieltjes integral (again assuming X is real-valued).",1880701090384646413,related_concept,Probability density function,2024-06-23 21:17:09.728,[],['Probability'],set(),set(),0,0.6031746031746031,1.1239589464558375
816,1607,Density estimation,http://dbpedia.org/resource/Density_estimation,http://en.wikipedia.org/wiki/Density_estimation,"In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population. A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram.",7571625956421298441,related_concept,Probability density function,2024-06-23 21:17:09.728,[],['Density estimation'],set(),set(),0,2.1530054644808745,1.3488906471504476
817,1610,Probability amplitude,http://dbpedia.org/resource/Probability_amplitude,http://en.wikipedia.org/wiki/Probability_amplitude,"In quantum mechanics, a probability amplitude is a complex number used for describing the behaviour of systems. The modulus squared of this quantity represents a probability density. Probability amplitudes provide a relationship between the quantum state vector of a system and the results of observations of that system, a link was first proposed by Max Born, in 1926. Interpretation of values of a wave function as the probability amplitude is a pillar of the Copenhagen interpretation of quantum mechanics. In fact, the properties of the space of wave functions were being used to make physical predictions (such as emissions from atoms being at certain discrete energies) before any physical interpretation of a particular function was offered. Born was awarded half of the 1954 Nobel Prize in Physics for this understanding, and the probability thus calculated is sometimes called the ""Born probability"". These probabilistic concepts, namely the probability density and quantum measurements, were vigorously contested at the time by the original physicists working on the theory, such as Schrödinger and Einstein. It is the source of the mysterious consequences and philosophical difficulties in the interpretations of quantum mechanics—topics that continue to be debated even today.",5012964661340144411,related_concept,Probability density function,2024-06-23 21:17:09.728,"['Probability', 'Probability amplitude']","['Probability', 'Probability amplitude']",set(),set(),0,1.1715481171548117,0.8239378088196554
818,1612,Kernel (statistics),http://dbpedia.org/resource/Kernel_(statistics),http://en.wikipedia.org/wiki/Kernel_(statistics),"The term kernel is used in statistical analysis to refer to a window function. The term ""kernel"" has several distinct meanings in different branches of statistics.",8240224551089145995,related_concept,Probability density function,2024-06-23 21:17:09.728,[],['Bayesian statistics'],set(),set(),0,1.5873015873015872,1.4282629177695982
819,1613,List of probability distributions,http://dbpedia.org/resource/List_of_probability_distributions,http://en.wikipedia.org/wiki/List_of_probability_distributions,Many probability distributions that are important in theory or applications have been given specific names.,8394362749781681975,related_concept,Probability density function,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5886287625418061,0.6531123145478693
820,1618,Univariate distribution,http://dbpedia.org/resource/Univariate_distribution,http://en.wikipedia.org/wiki/Univariate_distribution,"In statistics, a univariate distribution is a probability distribution of only one random variable. This is in contrast to a multivariate distribution, the probability distribution of a random vector (consisting of multiple random variables).",7280832247440918268,related_concept,Probability density function,2024-06-23 21:17:09.728,[],['Poisson distribution'],set(),set(),0,1.8076923076923077,5.8811565903611985e-304
821,1620,Kernel density estimation,http://dbpedia.org/resource/Kernel_density_estimation,http://en.wikipedia.org/wiki/Kernel_density_estimation,"In statistics, kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights. KDE answers a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form. One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier, which can improve its prediction accuracy.",7524550004085722509,related_concept,Probability density function,2024-06-23 21:17:09.728,['Bayes classifier'],['Bayes classifier'],set(),set(),0,1.5188679245283019,1.4295146856706502
822,1622,Probability mass function,http://dbpedia.org/resource/Probability_mass_function,http://en.wikipedia.org/wiki/Probability_mass_function,"In probability and statistics, a probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete. A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability. The value of the random variable having the largest probability mass is called the mode.",381177547975549227,related_concept,Probability density function,2024-06-23 21:17:09.728,['PDF'],"['PDF', 'Probability', 'Probability mass function', 'Discretization', 'Bernoulli distribution']",set(),set(),0,6.245283018867925,4.692438595675257e-304
823,1626,Contrast set learning,http://dbpedia.org/resource/Contrast_set_learning,http://en.wikipedia.org/wiki/Contrast_set_learning,"Contrast set learning is a form of association rule learning that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the contrasting features between students seeking bachelor's degrees and those working toward PhD degrees.",2585456176130450415,related_concept,Association rule learning,2024-06-23 21:17:09.728,['Contrast set learning'],['Contrast set learning'],set(),set(),0,3.3333333333333335,1.4011213267998028
824,1627,Data mining,http://dbpedia.org/resource/Data_mining,http://en.wikipedia.org/wiki/Data_mining,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate. The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps. The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data. The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.",4933543684891846065,related_concept,Association rule learning,2024-06-23 21:17:09.728,"['Data', 'Data mining']","['Data', 'Data mining', 'Database', 'AI', ""Bayes' theorem"", 'Computer science']",set(),set(),0,3.629382303839733,1.4017131397395295
825,1628,Lift (data mining),http://dbpedia.org/resource/Lift_(data_mining),http://en.wikipedia.org/wiki/Lift_(data_mining),"In data mining and association rule learning, lift is a measure of the performance of a targeting model (association rule) at predicting or classifying cases as having an enhanced response (with respect to the population as a whole), measured against a random choice targeting model. A targeting model is doing a good job if the response within the target is much better than the baseline average for the population as a whole. Lift is simply the ratio of these values: target response divided by average response. Mathematically, For example, suppose a population has an average response rate of 5%, but a certain model (or rule) has identified a segment with a response rate of 20%. Then that segment would have a lift of 4.0 (20%/5%).",983369495655849614,related_concept,Association rule learning,2024-06-23 21:17:09.728,[],[],set(),set(),0,7.222222222222222,1.330938412507432
826,1629,Sequential pattern mining,http://dbpedia.org/resource/Sequential_pattern_mining,http://en.wikipedia.org/wiki/Sequential_pattern_mining,"Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence. It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity. Sequential pattern mining is a special case of structured data mining. There are several key traditional computational problems addressed within this field. These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members. In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms and itemset mining which is typically based on association rule learning. Local process models extend sequential pattern mining to more complex patterns that can include (exclusive) choices, loops, and concurrency constructs in addition to the sequential ordering construct.",877063161987654825,related_concept,Association rule learning,2024-06-23 21:17:09.728,['Sequential pattern mining'],['Sequential pattern mining'],{'Bioinformatics'},set(),0,1.2065217391304348,1.3991193334042393
827,1630,Intrusion detection,http://dbpedia.org/resource/Intrusion_detection,http://en.wikipedia.org/wiki/Intrusion_detection,,8685125060857439548,related_concept,Association rule learning,2024-06-23 21:17:09.728,[],"['Intrusion detection', 'Neural network', 'Analytics', 'IP address', 'Data', 'PDF']",set(),set(),0,0.3087557603686636,0.32983004713331965
828,1631,Production system (computer science),http://dbpedia.org/resource/Production_system_(computer_science),http://en.wikipedia.org/wiki/Production_system_(computer_science),"A ""production system "" (or ""production rule system"") is a computer program typically used to provide some form of artificial intelligence, which consists primarily of a set of rules about behavior but it also includes the mechanism necessary to follow those rules as the system responds to states of the world. Those rules, termed productions, are a basic representation found useful in automated planning, expert systems and action selection. Productions consist of two parts: a sensory precondition (or ""IF"" statement) and an action (or ""THEN""). If a production's precondition matches the current state of the world, then the production is said to be triggered. If a production's action is executed, it is said to have fired. A production system also contains a database, sometimes called working memory, which maintains data about current state or knowledge, and a rule interpreter. The rule interpreter must provide a mechanism for prioritizing productions when more than one is triggered.",8216923198147729134,related_concept,Association rule learning,2024-06-23 21:17:09.728,[],['Computation'],set(),set(),0,3.0377358490566038,1.4100461837793927
829,1632,Rule-based machine learning,http://dbpedia.org/resource/Rule-based_machine_learning,http://en.wikipedia.org/wiki/Rule-based_machine_learning,"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge. While rule-based machine learning is conceptually a type of rule-based system, it is distinct from traditional rule-based systems, which are often hand-crafted, and other rule-based decision makers. This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set.",4536738709850578781,related_concept,Association rule learning,2024-06-23 21:17:09.728,['Rule-based machine learning'],['Rule-based machine learning'],set(),set(),0,1.5855263157894737,1.4018140020238954
830,1633,Apriori algorithm,http://dbpedia.org/resource/Apriori_algorithm,http://en.wikipedia.org/wiki/Apriori_algorithm,Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.,8047262362252465924,main_concept,Association rule learning,2024-06-23 21:17:09.728,[],"['IP address', 'Winepi', 'Apriori algorithm']",set(),set(),0,4.882352941176471,1.4001784914163187
831,1634,K-optimal pattern discovery,http://dbpedia.org/resource/K-optimal_pattern_discovery,http://en.wikipedia.org/wiki/K-optimal_pattern_discovery,"K-optimal pattern discovery is a data mining technique that provides an alternative to the frequent pattern discovery approach that underlies most association rule learning techniques. Frequent pattern discovery techniques find all patterns for which there are sufficiently frequent examples in the sample data. In contrast, k-optimal pattern discovery techniques find the k patterns that optimize a user-specified measure of interest. The parameter k is also specified by the user. Examples of k-optimal pattern discovery techniques include: 
* k-optimal classification rule discovery. 
* k-optimal subgroup discovery. 
* finding k most interesting patterns using sequential sampling. 
* mining top.k frequent closed patterns without minimum support. 
* k-optimal rule discovery. In contrast to k-optimal rule discovery and frequent pattern mining techniques, subgroup discovery focuses on mining interesting patterns with respect to a specified target property of interest. This includes, for example, binary, nominal, or numeric attributes, but also more complex target concepts such as correlations between several variables. Background knowledge like constraints and ontological relations can often be successfully applied for focusing and improving the discovery results.",3423790692509715266,related_concept,Association rule learning,2024-06-23 21:17:09.728,['K-optimal pattern discovery'],['K-optimal pattern discovery'],set(),set(),0,6.333333333333333,1.3990312906582778
832,1636,Backtracking,http://dbpedia.org/resource/Backtracking,http://en.wikipedia.org/wiki/Backtracking,"Backtracking is a class of algorithms for finding solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons a candidate (""backtracks"") as soon as it determines that the candidate cannot possibly be completed to a valid solution. The classic textbook example of the use of backtracking is the eight queens puzzle, that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned. Backtracking can be applied only for problems which admit the concept of a ""partial candidate solution"" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute-force enumeration of all complete candidates, since it can eliminate many candidates with a single test. Backtracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient technique for parsing, for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog. Backtracking depends on user-given ""black box procedures"" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm – although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time. The term ""backtrack"" was coined by American mathematician D. H. Lehmer in the 1950s. The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility.",2320935092299825186,related_concept,Association rule learning,2024-06-23 21:17:09.728,"['Prolog', 'Backtracking']","['Backtracking', 'Prolog']",set(),set(),0,1.956896551724138,1.2808356048886151
833,1637,Market basket analysis,http://dbpedia.org/resource/Market_basket_analysis,http://en.wikipedia.org/wiki/Market_basket_analysis,,7791301934550302459,related_concept,Association rule learning,2024-06-23 21:17:09.728,[],['Market basket analysis'],set(),set(),0,0.4074074074074074,0.3191688005652554
834,1639,Learning classifier system,http://dbpedia.org/resource/Learning_classifier_system,http://en.wikipedia.org/wiki/Learning_classifier_system,"Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling, classification, data mining, regression, function approximation, or game strategy). This approach allows complex solution spaces to be broken up into smaller, simpler parts. The founding concepts behind learning classifier systems came from attempts to model complex adaptive systems, using rule-based agents to form an artificial cognitive system (i.e. artificial intelligence).",3728553412233187972,related_concept,Association rule learning,2024-06-23 21:17:09.728,['Learning classifier system'],"['Learning classifier system', 'Evolution', 'Evolutionary algorithms', 'Adaptation', 'Algorithm']",set(),set(),0,1.878787878787879,1.3989284036843426
835,1640,Bioinformatics,http://dbpedia.org/resource/Bioinformatics,http://en.wikipedia.org/wiki/Bioinformatics,"Bioinformatics (/ˌbaɪ.oʊˌɪnfərˈmætɪks/) is an interdisciplinary field that develops methods and software tools for understanding biological data, in particular when the data sets are large and complex. As an interdisciplinary field of science, bioinformatics combines biology, chemistry, physics, computer science, information engineering, mathematics and statistics to analyze and interpret the biological data. Bioinformatics has been used for in silico analyses of biological queries using computational and statistical techniques. Bioinformatics includes biological studies that use computer programming as part of their methodology, as well as specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim to better understand the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences, called proteomics. Image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in comparing, analyzing and interpreting genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.",7599270595357043145,related_concept,Association rule learning,2024-06-23 21:17:09.728,['Bioinformatics'],"['Bioinformatics', 'Computation', 'Algorithm', 'Database', 'Data', 'Evolution', 'Evolutionary biology', 'Prediction']",{'Bioinformatics'},set(),0,3.8411316648531013,1.2831654630253788
836,1641,Breadth-first search,http://dbpedia.org/resource/Breadth-first_search,http://en.wikipedia.org/wiki/Breadth-first_search,"Breadth-first search (BFS) is an algorithm for searching a tree data structure for a node that satisfies a given property. It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level. Extra memory, usually a queue, is needed to keep track of the child nodes that were encountered but not yet explored. For example, in a chess endgame a chess engine may build the game tree from the current position by applying all possible moves, and use breadth-first search to find a win position for white. Implicit trees (such as game trees or other problem-solving trees) may be of infinite size; breadth-first search is guaranteed to find a solution node if one exists. In contrast, (plain) depth-first search, which explores the node branch as far as possible before backtracking and expanding other nodes, may get lost in an infinite branch and never make it to the solution node. Iterative deepening depth-first search avoids the latter drawback at the price of exploring the tree's top parts over and over again. On the other hand, both depth-first algorithms get along without extra memory. Breadth-first search can be generalized to graphs, when the start node (sometimes referred to as a 'search key') is explicitly given, and precautions are taken against following a vertex twice. BFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, in his (rejected) Ph.D. thesis on the Plankalkül programming language, but this was not published until 1972. It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze, and later developed by C. Y. Lee into a wire routing algorithm (published 1961).",362636375042958237,related_concept,Association rule learning,2024-06-23 21:17:09.728,"['Iterative', 'Breadth-first search']","['Breadth-first search', 'Iterative']",set(),set(),0,2.9307692307692306,4.332077752263504e-304
837,1642,Clustering high-dimensional data,http://dbpedia.org/resource/Clustering_high-dimensional_data,http://en.wikipedia.org/wiki/Clustering_high-dimensional_data,"Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional spaces of data are often encountered in areas such as medicine, where DNA microarray technology can produce many measurements at once, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the vocabulary.",5382467943427380831,main_concept,Association rule learning,2024-06-23 21:17:09.728,"['DNA microarray', 'Clustering high-dimensional data']","['DNA microarray', 'Clustering high-dimensional data', 'Mean', 'SUBCLU', 'DBSCAN', 'Boot']",set(),set(),0,2.810810810810811,1.1669597961278224
838,1643,Depth-first search,http://dbpedia.org/resource/Depth-first_search,http://en.wikipedia.org/wiki/Depth-first_search,"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking. Extra memory, usually a stack, is needed to keep track of the nodes discovered so far along a specified branch which helps in backtracking of the graph. A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.",2916289742756642146,related_concept,Association rule learning,2024-06-23 21:17:09.728,['Depth-first search'],"['Depth-first search', 'Iterative', 'Algorithm']",set(),set(),0,3.185483870967742,0.8930930268474466
839,1644,Trie,http://dbpedia.org/resource/Trie,http://en.wikipedia.org/wiki/Trie,"In computer science, a trie, also called digital tree or prefix tree, is a type of k-ary search tree, a tree data structure used for locating specific keys from within a set. These keys are most often strings, with links between nodes defined not by the entire key, but by individual characters. In order to access a key (to recover its value, change it, or remove it), the trie is traversed depth-first, following the links between nodes, which represent each character in the key. Unlike a binary search tree, nodes in the trie do not store their associated key. Instead, a node's position in the trie defines the key with which it is associated. This distributes the value of each key across the data structure, and means that not every node necessarily has an associated value. All the children of a node have a common prefix of the string associated with that parent node, and the root is associated with the empty string. This task of storing data accessible by its prefix can be accomplished in a memory-optimized way by employing a radix tree. Though tries can be keyed by character strings, they need not be. The same algorithms can be adapted for ordered lists of any underlying type, e.g. permutations of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up a piece of fixed-length binary data, such as an integer or memory address. The key lookup complexity of a trie remains proportional to the key size. Specialized trie implementations such as compressed tries are used to deal with the enormous space requirement of a trie in naive implementations.",6422103715551748410,related_concept,Association rule learning,2024-06-23 21:17:09.728,[],"['Trie', 'Bioinformatics']",set(),set(),0,1.4098939929328622,1.3611460073057715
840,1645,Bitstring,http://dbpedia.org/resource/Bitstring,http://en.wikipedia.org/wiki/Bitstring,,5210846396696777710,related_concept,Association rule learning,2024-06-23 21:17:09.728,[],"['Data', 'SQL']",set(),set(),0,0.1,0.37144121418900256
841,1646,Association rule learning,http://dbpedia.org/resource/Association_rule_learning,http://en.wikipedia.org/wiki/Association_rule_learning,"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements. In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. The association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.",7088180602119684611,main_concept,,2024-06-23 21:17:09.728,['Association rule learning'],"['Association rule learning', 'Classification', 'Regression analysis', 'Data', 'Apriori algorithm', 'Database', 'Contrast set learning', 'K-optimal pattern discovery', 'Sequential pattern mining']",set(),set(),0,2.2947976878612715,1.3996098271451662
842,1647,Informedness,http://dbpedia.org/resource/Informedness,http://en.wikipedia.org/wiki/Informedness,,3026302935320215175,related_concept,Precision and recall,2024-06-23 21:17:09.728,[],"['Informedness', 'Classification', 'Markedness', 'F-score', 'Matthews correlation coefficient']",set(),set(),0,0.46875,0.4603102703398526
843,1648,Markedness,http://dbpedia.org/resource/Markedness,http://en.wikipedia.org/wiki/Markedness,"In linguistics and social sciences, markedness is the state of standing out as nontypical or divergent as opposed to regular or common. In a marked–unmarked relation, one term of an opposition is the broader, dominant one. The dominant default or minimum-effort form is known as unmarked; the other, secondary one is marked. In other words, markedness involves the characterization of a ""normal"" linguistic unit against one or more of its possible ""irregular"" forms. In linguistics, markedness can apply to, among others, phonological, grammatical, and semantic oppositions, defining them in terms of marked and unmarked oppositions, such as honest (unmarked) vs. dishonest (marked). Marking may be purely semantic, or may be realized as extra morphology. The term derives from the marking of a grammatical role with a suffix or another element, and has been extended to situations where there is no morphological distinction. In social sciences more broadly, markedness is, among other things, used to distinguish two meanings of the same term, where one is common usage (unmarked sense) and the other is specialized to a certain cultural context (marked sense). In psychology, the social science concept of markedness is quantified as a measure of how much one variable is marked as a predictor or possible cause of another, and is also known as Δp (deltaP) in simple two-choice cases. See confusion matrix for more details.",3760126577525951365,related_concept,Precision and recall,2024-06-23 21:17:09.728,[],"['Matthews correlation coefficient', 'Markedness', 'Informedness']",set(),set(),0,2.462068965517241,5.3639965151788594e-304
844,1653,F-score,http://dbpedia.org/resource/F-score,http://en.wikipedia.org/wiki/F-score,"In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification. The F1 score is the harmonic mean of the precision and recall. The more generic score applies additional weights, valuing one of precision or recall more than the other. The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero.",5440712733285348654,main_concept,Precision and recall,2024-06-23 21:17:09.728,['F-score'],"['F-score', 'Type I and type II errors', 'Matthews correlation coefficient', 'Informedness', 'Markedness', 'Fowlkes–Mallows index']",set(),set(),0,0.648,5.348053839294609e-304
845,1658,Uncertainty coefficient,http://dbpedia.org/resource/Uncertainty_coefficient,http://en.wikipedia.org/wiki/Uncertainty_coefficient,"In statistics, the uncertainty coefficient, also called proficiency, entropy coefficient or Theil's U, is a measure of nominal association. It was first introduced by Henri Theil and is based on the concept of information entropy.",6451944267479980014,related_concept,Precision and recall,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.0434782608695654,1.3364725047246921
846,1660,Confusion matrix,http://dbpedia.org/resource/Confusion_matrix,http://en.wikipedia.org/wiki/Confusion_matrix,"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa – both variants are found in the literature. The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another). It is a special kind of contingency table, with two dimensions (""actual"" and ""predicted""), and identical sets of ""classes"" in both dimensions (each combination of dimension and class is a variable in the contingency table).",1504539346957344523,main_concept,Precision and recall,2024-06-23 21:17:09.728,[],"['Accuracy', 'Matthews correlation coefficient', 'Confusion matrix']",set(),set(),0,2.4065420560747666,1.4079475652344497
847,1662,Object detection,http://dbpedia.org/resource/Object_detection,http://en.wikipedia.org/wiki/Object_detection,"Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.",288209915966494026,related_concept,Precision and recall,2024-06-23 21:17:09.728,['Object detection'],['Object detection'],set(),set(),0,3.375,1.4235314188381796
848,1665,Contingency table,http://dbpedia.org/resource/Contingency_table,http://en.wikipedia.org/wiki/Contingency_table,"In statistics, a contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables. They are heavily used in survey research, business intelligence, engineering, and scientific research. They provide a basic picture of the interrelation between two variables and can help find interactions between them. The term contingency table was first used by Karl Pearson in ""On the Theory of Contingency and Its Relation to Association and Normal Correlation"", part of the Drapers' Company Research Memoirs Biometric Series I published in 1904. A crucial problem of multivariate statistics is finding the (direct-)dependence structure underlying the variables contained in high-dimensional contingency tables. If some of the conditional independences are revealed, then even the storage of the data can be done in a smarter way (see Lauritzen (2002)). In order to do this one can use information theory concepts, which gain the information only from the distribution of probability, which can be expressed easily from the contingency table by the relative frequencies. A pivot table is a way to create contingency tables using spreadsheet software.",5378234704463018449,related_concept,Precision and recall,2024-06-23 21:17:09.728,[],"[""Pearson's chi-squared test"", 'Pearson correlation coefficient']",set(),set(),0,1.9269662921348314,1.3542599868443719
849,1666,Pattern recognition,http://dbpedia.org/resource/Pattern_recognition,http://en.wikipedia.org/wiki/Pattern_recognition,"Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. These activities can be viewed as two facets of the same field of application, and they have undergone substantial development over the past few decades. Pattern recognition systems are commonly trained from labeled ""training"" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam""). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors.",8684826263306912388,related_concept,Precision and recall,2024-06-23 21:17:09.728,['Pattern recognition'],"['Pattern recognition', 'Unsupervised learning', 'Supervised learning', 'Feature selection', 'Feature extraction', 'Bayesian statistics', 'Algorithm']",set(),set(),0,2.5531400966183573,1.3914918676811834
850,1668,Case-control study,http://dbpedia.org/resource/Case-control_study,http://en.wikipedia.org/wiki/Case-control_study,,7654026198265124916,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.40074906367041196,0.5573530908862083
851,1669,Information retrieval,http://dbpedia.org/resource/Information_retrieval,http://en.wikipedia.org/wiki/Information_retrieval,"Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.",2415850650969602583,main_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,['Information retrieval'],"['Information retrieval', 'SQL']",set(),set(),0,5.206030150753769,3.357897431600677e-304
852,1671,Gold standard (test),http://dbpedia.org/resource/Gold_standard_(test),http://en.wikipedia.org/wiki/Gold_standard_(test),"In medicine and statistics, a gold standard test is usually the diagnostic test or benchmark that is the best available under reasonable conditions. In other words, a gold standard is the most accurate test possible without restrictions. Both meanings are different because for example, in medicine, dealing with conditions that would require an autopsy to have a perfect diagnosis, the gold standard test would be the best one that keeps the patient alive instead of the autopsy.",7237328960823534202,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5408653846153846,1.2548841780380007
853,1672,Precision and recall,http://dbpedia.org/resource/Precision_and_recall,http://en.wikipedia.org/wiki/Precision_and_recall,"In pattern recognition, information retrieval, object detection and classification (machine learning), precision and recall are performance metrics that apply to data retrieved from a collection, corpus or sample space. Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance. Consider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements). When a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are. Adopting a hypothesis-testing approach from statistics, in which, in this case, the null hypothesis is that a given item is irrelevant, i.e., not a dog, absence of type I and type II errors (i.e. perfect specificity and sensitivity of 100% each) corresponds respectively to perfect precision (no false positive) and perfect recall (no false negative). More generally, recall is simply the complement of the type II error rate, i.e. one minus the type II error rate. Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the prior distribution of seeing a relevant vs an irrelevant item. The above cat and dog example contained 8 − 5 = 3 type I errors (false positives) out of 10 total cats (true negatives), for a type I error rate of 3/10, and 12 − 5 = 7 type II errors, for a type II error rate of 7/12. Precision can be seen as a measure of quality, and recall as a measure of quantity. Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned).",1819816592603909771,main_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],"['Precision and recall', 'Matthews correlation coefficient', 'Informedness', 'Markedness', 'Prevalence', 'Accuracy', ""Bayes' theorem"", 'F-score']",{'Bioinformatics'},set(),0,2.147286821705426,1.2456613939981123
854,1673,Prevalence,http://dbpedia.org/resource/Prevalence,http://en.wikipedia.org/wiki/Prevalence,"In epidemiology, prevalence is the proportion of a particular population found to be affected by a medical condition (typically a disease or a risk factor such as smoking or seatbelt use) at a specific time. It is derived by comparing the number of people found to have the condition with the total number of people studied and is usually expressed as a fraction, a percentage, or the number of cases per 10,000 or 100,000 people. Prevalence is most often used in questionnaire studies.",198281385290860154,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,['Prevalence'],['Prevalence'],set(),set(),0,2.5348837209302326,1.3621348759727157
855,1674,Diagnostic test,http://dbpedia.org/resource/Diagnostic_test,http://en.wikipedia.org/wiki/Diagnostic_test,,7415082281974349449,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.17207334273624825,0.47619389334806356
856,1675,Sensitivity index,http://dbpedia.org/resource/Sensitivity_index,http://en.wikipedia.org/wiki/Sensitivity_index,The sensitivity index or discriminability index or detectability index is a dimensionless statistic used in signal detection theory. A higher index indicates that the signal can be more readily detected.,3494094818569348464,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2105263157894737,0.9462982350847018
857,1676,Fecal occult blood,http://dbpedia.org/resource/Fecal_occult_blood,http://en.wikipedia.org/wiki/Fecal_occult_blood,"Fecal occult blood (FOB) refers to blood in the feces that is not visibly apparent (unlike other types of blood in stool such as melena or hematochezia). A fecal occult blood test (FOBT) checks for hidden (occult) blood in the stool (feces). The American College of Gastroenterology has recommended the abandoning of gFOBT testing as a colorectal cancer screening tool, in favor of the fecal immunochemical test (FIT). The newer and recommended tests look for globin, DNA, or other blood factors including transferrin, while conventional stool guaiac tests look for heme.",7282230622415389048,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,['Fecal occult blood'],['Fecal occult blood'],set(),set(),0,0.5579937304075235,5.615940717254662e-304
858,1677,Likelihood ratios in diagnostic testing,http://dbpedia.org/resource/Likelihood_ratios_in_diagnostic_testing,http://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing,"In evidence-based medicine, likelihood ratios are used for assessing the value of performing a diagnostic test. They use the sensitivity and specificity of the test to determine whether a test result usefully changes the probability that a condition (such as a disease state) exists. The first description of the use of likelihood ratios for decision rules was made at a symposium on information theory in 1954. In medicine, likelihood ratios were introduced between 1975 and 1980.",7793984218291056818,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],"[""Bayes' theorem"", 'Probability', 'Confidence interval']",set(),set(),0,1.121212121212121,5.980997995677046e-304
859,1678,Cross-sectional study,http://dbpedia.org/resource/Cross-sectional_study,http://en.wikipedia.org/wiki/Cross-sectional_study,"In medical research, social science, and biology, a cross-sectional study (also known as a cross-sectional analysis, transverse study, prevalence study) is a type of observational study that analyzes data from a population, or a representative subset, at a specific point in time—that is, cross-sectional data. In economics, cross-sectional studies typically involve the use of cross-sectional regression, in order to sort out the existence and magnitude of causal effects of one independent variable upon a dependent variable of interest at a given point in time. They differ from time series analysis, in which the behavior of one or more economic aggregates is traced through time. In medical research, cross-sectional studies differ from case-control studies in that they aim to provide data on the entire population under study, whereas case-control studies typically include only individuals who have developed a specific condition and compare them with a matched sample, often a tiny minority, of the rest of the population. Cross-sectional studies are descriptive studies (neither longitudinal nor experimental). Unlike case-control studies, they can be used to describe, not only the odds ratio, but also absolute risks and relative risks from prevalences (sometimes called prevalence risk ratio, or PRR). They may be used to describe some feature of the population, such as prevalence of an illness, but cannot prove cause and effect. Longitudinal studies differ from both in making a series of observations more than once on members of the study population over a period of time.",7662449946507902296,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],['Inference'],set(),set(),0,1.8028503562945368,1.3910341698083688
860,1679,False discovery rate,http://dbpedia.org/resource/False_discovery_rate,http://en.wikipedia.org/wiki/False_discovery_rate,"In statistics, the false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons. FDR-controlling procedures are designed to control the FDR, which is the expected proportion of ""discoveries"" (rejected null hypotheses) that are false (incorrect rejections of the null). Equivalently, the FDR is the expected ratio of the number of false positive classifications (false discoveries) to the total number of positive classifications (rejections of the null). The total number of rejections of the null include both the number of false positives (FP) and true positives (TP). Simply put, FDR = FP / (FP + TP). FDR-controlling procedures provide less stringent control of Type I errors compared to family-wise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of at least one Type I error. Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors.",1369106334810109836,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],['Mean'],set(),set(),0,0.3983516483516483,5.747152803955019e-304
861,1680,Relevance (information retrieval),http://dbpedia.org/resource/Relevance_(information_retrieval),http://en.wikipedia.org/wiki/Relevance_(information_retrieval),"In information science and information retrieval, relevance denotes how well a retrieved document or set of documents meets the information need of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.",2783745660011372609,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.5,0.7067313798037417
862,1681,Pre- and post-test probability,http://dbpedia.org/resource/Pre-_and_post-test_probability,http://en.wikipedia.org/wiki/Pre-_and_post-test_probability,"Pre-test probability and post-test probability (alternatively spelled pretest and posttest probability) are the probabilities of the presence of a condition (such as a disease) before and after a diagnostic test, respectively. Post-test probability, in turn, can be positive or negative, depending on whether the test falls out as a positive test or a negative test, respectively. In some cases, it is used for the probability of developing the condition of interest in the future. Test, in this sense, can refer to any medical test (but usually in the sense of diagnostic tests), and in a broad sense also including questions and even assumptions (such as assuming that the target individual is a female or male). The ability to make a difference between pre- and post-test probabilities of various conditions is a major factor in the indication of medical tests.",6895832833335201848,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],['False negative'],set(),set(),0,0.9097222222222222,5.779150021046447e-304
863,1682,Receiver operating characteristic,http://dbpedia.org/resource/Receiver_operating_characteristic,http://en.wikipedia.org/wiki/Receiver_operating_characteristic,"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The method was originally developed for operators of military radar receivers starting in 1941, which led to its name. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). The ROC can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis. ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making. The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research. The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes.",7044321343338764239,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],"['Brier score', 'Matthews correlation coefficient', 'Informedness', 'Markedness', 'Accuracy']",set(),set(),0,0.790625,5.3281980937535755e-304
864,1683,False negative,http://dbpedia.org/resource/False_negative,http://en.wikipedia.org/wiki/False_negative,,9158885945706634384,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],[],set(),set(),0,4.733333333333333,0.5144637591259066
865,1684,Binary classification,http://dbpedia.org/resource/Binary_classification,http://en.wikipedia.org/wiki/Binary_classification,"Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include: 
* Medical testing to determine if a patient has certain disease or not; 
* Quality control in industry, deciding whether a specification has been met; 
* In information retrieval, deciding whether a page should be in the result set of a search or not. Binary classification is dichotomization applied to a practical situation. In many practical binary classification problems, the two groups are not symmetric, and rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative).",8477029876423629062,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,['Binary classification'],"['Binary classification', 'Statistical classification', 'Matthews correlation coefficient', 'F-score']",set(),set(),0,0.5025510204081632,1.410298465652765
866,1685,Diagnostic odds ratio,http://dbpedia.org/resource/Diagnostic_odds_ratio,http://en.wikipedia.org/wiki/Diagnostic_odds_ratio,"In medical testing with binary classification, the diagnostic odds ratio (DOR) is a measure of the effectiveness of a diagnostic test. It is defined as the ratio of the odds of the test being positive if the subject has a disease relative to the odds of the test being positive if the subject does not have the disease. The rationale for the diagnostic odds ratio is that it is a single indicator of test performance (like accuracy and Youden's J statistic) but which is independent of prevalence (unlike accuracy) and is presented as an odds ratio, which is familiar to medical practitioners.",8195780429691918043,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,[],['Diagnostic odds ratio'],set(),set(),0,1.2941176470588236,5.606194161831037e-304
867,1686,Predictive value of tests,http://dbpedia.org/resource/Predictive_value_of_tests,http://en.wikipedia.org/wiki/Predictive_value_of_tests,"Predictive value of tests is the probability of a target condition given by the result of a test, often in regard to medical tests. 
* In cases where binary classification can be applied to the test results, such yes versus no, test target (such as a substance, symptom or sign) being present versus absent, or either a positive or negative test), then each of the two outcomes has a separate predictive value. For example, for positive or negative test, the predictive values are termed positive predictive value or negative predictive value, respectively. 
* In cases where the test result is of a continuous value, the predictive value generally changes continuously along with the value. For example, for a pregnancy test that displays the urine concentration of hCG, the predictive value increases with increasing hCG value. A conversion of continuous values into binary values can be performed, such as designating a pregnancy test as ""positive"" above a certain cutoff value, but this confers a loss of information and generally results in less accurate predictive values.(For more information on conversion and its disadvantages, see Artificial binary classification.)
",4820866210341018372,related_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,['Predictive value of tests'],['Predictive value of tests'],set(),set(),0,1.2307692307692308,5.561808064186237e-304
868,1687,Sensitivity and specificity,http://dbpedia.org/resource/Sensitivity_and_specificity,http://en.wikipedia.org/wiki/Sensitivity_and_specificity,"Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered ""positive"" and those for which it is not are considered ""negative"". 
* Sensitivity (true positive rate) refers to the probability of a positive test, conditioned on truly being positive. 
* Specificity (true negative rate) refers to the probability of a negative test, conditioned on truly being negative. If the true condition can not be known, a ""gold standard test"" is assumed to be correct. In a diagnostic test, sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives. For all testing, both diagnostic and screening, there is usually a trade-off between sensitivity and specificity, such that higher sensitivities will mean lower specificities and vice versa. If the goal is to return the ratio at which the test identifies the percentage of people highly likely to be identified as having the condition, the number of true positives should be high and the number of false negatives should be very low, which results in high sensitivity. This is especially important when the consequence of failing to treat the condition is serious and/or the treatment is very effective and has minimal side effects. If the goal is to return the ratio at which the test identifies the percentage of people highly likely to be identified as not having the condition, the number of true negatives should be high and the number of false positives should be very low, which results in high specificity. That is, people highly likely to be excluded by the test. This is especially important when people who are identified as having a condition may be subjected to more testing, expense, stigma, anxiety, etc. The terms ""sensitivity"" and ""specificity"" were introduced by American biostatistician Jacob Yerushalmy in 1947.",2850115748745429462,main_concept,Positive and negative predictive values,2024-06-23 21:17:09.728,['Sensitivity and specificity'],"['Sensitivity and specificity', 'Positive and negative predictive values', 'Confidence interval', 'F-score']",{'Bioinformatics'},set(),0,3.596412556053812,1.169262969091794
869,1688,Positive and negative predictive values,http://dbpedia.org/resource/Positive_and_negative_predictive_values,http://en.wikipedia.org/wiki/Positive_and_negative_predictive_values,"The positive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively. The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test (as true positive rate and true negative rate are); they depend also on the prevalence. Both PPV and NPV can be derived using Bayes' theorem. Although sometimes used synonymously, a positive predictive value generally refers to what is established by control groups, while a post-test probability refers to a probability for an individual. Still, if the individual's pre-test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal. In information retrieval, the PPV statistic is often called the precision.",3061161872540530959,main_concept,,2024-06-23 21:17:09.728,"[""Bayes' theorem""]","[""Bayes' theorem""]",set(),set(),0,1.3108108108108107,5.706198792776224e-304
870,1695,Data dredging,http://dbpedia.org/resource/Data_dredging,http://en.wikipedia.org/wiki/Data_dredging,"Data dredging (also known as data snooping or p-hacking) is the misuse of data analysis to find patterns in data that can be presented as statistically significant, thus dramatically increasing and understating the risk of false positives. This is done by performing many statistical tests on the data and only reporting those that come back with significant results. The process of data dredging involves testing multiple hypotheses using a single data set by exhaustively searching—perhaps for combinations of variables that might show a correlation, and perhaps for groups of cases or observations that show differences in their mean or in their breakdown by some other variable. Conventional tests of statistical significance are based on the probability that a particular result would arise if chance alone were at work, and necessarily accept some risk of mistaken conclusions of a certain type (mistaken rejections of the null hypothesis). This level of risk is called the significance. When large numbers of tests are performed, some produce false results of this type; hence 5% of randomly chosen hypotheses might be (erroneously) reported to be statistically significant at the 5% significance level, 1% might be (erroneously) reported to be statistically significant at the 1% significance level, and so on, by chance alone. When enough hypotheses are tested, it is virtually certain that some will be reported to be statistically significant (even though this is misleading), since almost every data set with any degree of randomness is likely to contain (for example) some spurious correlations. If they are not cautious, researchers using data mining techniques can be easily misled by these results. Data dredging is an example of disregarding the multiple comparisons problem. One form is when subgroups are compared without alerting the reader to the total number of subgroup comparisons examined.",3770498338853891594,related_concept,Statistical significance,2024-06-23 21:17:09.728,"['Data', 'Data dredging']","['Data', 'Data dredging', 'Statistical inference']",set(),set(),0,2.3763440860215055,5.250061146139012e-304
871,1697,Multiple comparisons problem,http://dbpedia.org/resource/Multiple_comparisons_problem,http://en.wikipedia.org/wiki/Multiple_comparisons_problem,"In statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values. The more inferences are made, the more likely erroneous inferences become. Several statistical techniques have been developed to address that problem, typically by requiring a stricter significance threshold for individual comparisons, so as to compensate for the number of inferences being made.",2163846625548251090,related_concept,Statistical significance,2024-06-23 21:17:09.728,[],['Poisson distribution'],set(),set(),0,0.28337236533957844,0.9613306788225232
872,1698,Clinical significance,http://dbpedia.org/resource/Clinical_significance,http://en.wikipedia.org/wiki/Clinical_significance,"In medicine and psychology, clinical significance is the practical importance of a treatment effect—whether it has a real genuine, palpable, noticeable effect on daily life.",6372316141940932463,related_concept,Statistical significance,2024-06-23 21:17:09.728,[],"['Statistical significance', 'Effect size', 'Clinical significance', 'Confidence interval']",set(),set(),0,3.28,0.725080164547809
873,1700,Effect size,http://dbpedia.org/resource/Effect_size,http://en.wikipedia.org/wiki/Effect_size,"In statistics, an effect size is a value measuring the strength of the relationship between two variables in a population, or a sample-based estimate of that quantity. It can refer to the value of a statistic calculated from a sample of data, the value of a parameter for a hypothetical population, or to the equation that operationalizes how statistics or parameters lead to the effect size value. Examples of effect sizes include the correlation between two variables, the regression coefficient in a regression, the mean difference, or the risk of a particular event (such as a heart attack) happening. Effect sizes complement statistical hypothesis testing, and play an important role in power analyses, sample size planning, and in meta-analyses. The cluster of data-analysis methods concerning effect sizes is referred to as estimation statistics. Effect size is an essential component when evaluating the strength of a statistical claim, and it is the first item (magnitude) in the MAGIC criteria. The standard deviation of the effect size is of critical importance, since it indicates how much uncertainty is included in the measurement. A standard deviation that is too large will make the measurement nearly meaningless. In meta-analysis, where the purpose is to combine multiple effect sizes, the uncertainty in the effect size is used to weigh effect sizes, so that large studies are considered more important than small studies. The uncertainty in the effect size is calculated differently for each type of effect size, but generally only requires knowing the study's sample size (N), or the number of observations (n) in each group. Reporting effect sizes or estimates thereof (effect estimate [EE], estimate of effect) is considered good practice when presenting empirical research findings in many fields. The reporting of effect sizes facilitates the interpretation of the importance of a research result, in contrast to its statistical significance. Effect sizes are particularly prominent in social science and in medical research (where size of treatment effect is important). Effect sizes may be measured in relative or absolute terms. In relative effect sizes, two groups are directly compared with each other, as in odds ratios and relative risks. For absolute effect sizes, a larger absolute value always indicates a stronger effect. Many types of measurements can be expressed as either absolute or relative, and these can be used together because they convey different information. A prominent task force in the psychology research community made the following recommendation: Always present effect sizes for primary outcomes...If the units of measurement are meaningful on a practical level (e.g., number of cigarettes smoked per day), then we usually prefer an unstandardized measure (regression coefficient or mean difference) to a standardized measure (r or d).",5523206986577774854,related_concept,Statistical significance,2024-06-23 21:17:09.728,['Effect size'],"['Effect size', 'Pearson correlation coefficient', 'ANOVA', 'F-test', 'Confidence interval']",set(),set(),0,1.865909090909091,6.606171875298418e-304
874,1706,Statistical Methods for Research Workers,http://dbpedia.org/resource/Statistical_Methods_for_Research_Workers,http://en.wikipedia.org/wiki/Statistical_Methods_for_Research_Workers,"Statistical Methods for Research Workers is a classic book on statistics, written by the statistician R. A. Fisher. It is considered by some to be one of the 20th century's most influential books on statistical methods, together with his The Design of Experiments (1935). It was originally published in 1925, by Oliver & Boyd (Edinburgh); the final and posthumous 14th edition was published in 1970.",661947448346295109,related_concept,Statistical significance,2024-06-23 21:17:09.728,"['Statistical Methods for Research Workers', 'The Design of Experiments']","['Statistical Methods for Research Workers', 'The Design of Experiments']",set(),set(),0,1.4848484848484849,1.3616717457445902
875,1707,Type I and type II errors,http://dbpedia.org/resource/Type_I_and_type_II_errors,http://en.wikipedia.org/wiki/Type_I_and_type_II_errors,"In statistical hypothesis testing, a type I error is the mistaken rejection of an actually true null hypothesis (also known as a ""false positive"" finding or conclusion; example: ""an innocent person is convicted""), while a type II error is the failure to reject a null hypothesis that is actually false (also known as a ""false negative"" finding or conclusion; example: ""a guilty person is not convicted""). Much of statistical theory revolves around the minimization of one or both of these errors, though the complete elimination of either is a statistical impossibility if the outcome is not determined by a known, observable causal process.By selecting a low threshold (cut-off) value and modifying the alpha (α) level, the quality of the hypothesis test can be increased. The knowledge of type I errors and type II errors is widely used in medical science, biometrics and computer science. Intuitively, type I errors can be thought of as errors of commission, i.e. the researcher unluckily concludes that something is the fact. For instance, consider a study where researchers compare a drug with a placebo. If the patients who are given the drug get better than the patients given the placebo by chance, it may appear that the drug is effective, but in fact the conclusion is incorrect.In reverse, type II errors are errors of omission. In the example above, if the patients who got the drug did not get better at a higher rate than the ones who got the placebo, but this was a random fluke, that would be a type II error. The consequence of a type II error depends on the size and direction of the missed determination and the circumstances. An expensive cure for one in a million patients may be inconsequential even if it truly is a cure.",2555200535963874733,main_concept,Statistical significance,2024-06-23 21:17:09.728,[],"['False negative', ""Bayes' theorem""]",set(),set(),0,1.2556962025316456,5.147492770460186e-304
876,1712,Chi squared test,http://dbpedia.org/resource/Chi_squared_test,http://en.wikipedia.org/wiki/Chi_squared_test,,1687787049440562519,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,[],"[""Pearson's chi-squared test"", 'Test statistic', 'Pearson distribution']",set(),set(),0,0.0111731843575419,0.4730088015280666
877,1713,Nonparametric statistics,http://dbpedia.org/resource/Nonparametric_statistics,http://en.wikipedia.org/wiki/Nonparametric_statistics,Nonparametric statistics is the branch of statistics that is not based solely on parametrized families of probability distributions (common examples of parameters are the mean and variance). Nonparametric statistics is based on either being distribution-free or having a specified distribution but with the distribution's parameters unspecified. Nonparametric statistics includes both descriptive statistics and statistical inference. Nonparametric tests are often used when the assumptions of parametric tests are violated.,4444933977614556658,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,['Nonparametric statistics'],"['Nonparametric statistics', 'Order statistic', 'Statistics', 'Hypothesis']",set(),set(),0,9.246575342465754,1.3336643616691055
878,1719,Fisher's method,http://dbpedia.org/resource/Fisher's_method,http://en.wikipedia.org/wiki/Fisher's_method,"In statistics, Fisher's method, also known as Fisher's combined probability test, is a technique for data fusion or ""meta-analysis"" (analysis of analyses). It was developed by and named for Ronald Fisher. In its basic form, it is used to combine the results from several independence tests bearing upon the same overall hypothesis (H0).",288834999146040189,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,"[""Fisher's method""]","[""Fisher's method""]",set(),set(),0,1.28125,1.2841523994357051
879,1721,Exact test,http://dbpedia.org/resource/Exact_test,http://en.wikipedia.org/wiki/Exact_test,"In statistics, an exact (significance) test is a test such that if the null hypothesis is true, then all assumptions made during the derivation of the distribution of the test statistic are met. Using an exact test provides a significance test that maintains the type I error rate of the test at the desired significance level of the test. For example, an exact test at a significance level of , when repeated over many samples where the null hypothesis is true, will reject at most of the time. This is in contrast to an approximate test in which the desired type I error rate is only approximately maintained (i.e.: the test might reject > 5% of the time), while this approximation may be made as close to as desired by making the sample size sufficiently large. Exact tests that are based on discrete test statistics may be conservative, indicating that the actual rejection rate lies below the nominal significance level . As an example, this is the case for Fisher's exact test and its more powerful alternative, Boschloo's test. If the test statistic is continuous, it will reach the significance level exactly. Parametric tests, such as those used in exact statistics, are exact tests when the parametric assumptions are fully met, but in practice, the use of the term exact (significance) test is reserved for non-parametric tests, i.e., tests that do not rest on parametric assumptions. However, in practice, most implementations of non-parametric test software use asymptotical algorithms to obtain the significance value, which renders the test non-exact. Hence, when a result of statistical analysis is termed an “exact test” or specifies an “exact p-value”, this implies that the test is defined without parametric assumptions and is evaluated without making use of approximate algorithms. In principle, however, this could also signify that a parametric test has been employed in a situation where all parametric assumptions are fully met, but it is in most cases impossible to prove this completely in a real-world situation. Exceptions in which it is certain that parametric tests are exact include tests based on the binomial or Poisson distributions. The term permutation test is sometimes used as a synonym for exact test, but it should be kept in mind that all permutation tests are exact tests, but not all exact tests are permutation tests.",9208143158482555021,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,"['Poisson distribution', 'Exact test']","['Exact test', 'Poisson distribution', ""Pearson's chi-squared test""]",set(),set(),0,0.1466275659824047,5.428901364434613e-304
880,1722,Almost sure hypothesis testing,http://dbpedia.org/resource/Almost_sure_hypothesis_testing,http://en.wikipedia.org/wiki/Almost_sure_hypothesis_testing,"In statistics, almost sure hypothesis testing or a.s. hypothesis testing utilizes almost sure convergence in order to determine the validity of a statistical hypothesis with probability one. This is to say that whenever the null hypothesis is true, then an a.s. hypothesis test will fail to reject the null hypothesis w.p. 1 for all sufficiently large samples. Similarly, whenever the alternative hypothesis is true, then an a.s. hypothesis test will reject the null hypothesis with probability one, for all sufficiently large samples. Along similar lines, an a.s. confidence interval eventually contains the parameter of interest with probability 1. Dembo and Peres (1994) proved the existence of almost sure hypothesis tests.",8205770323006168724,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.6363636363636364,0.970949674107668
881,1723,Fiducial inference,http://dbpedia.org/resource/Fiducial_inference,http://en.wikipedia.org/wiki/Fiducial_inference,"Fiducial inference is one of a number of different types of statistical inference. These are rules, intended for general application, by which conclusions can be drawn from samples of data. In modern statistical practice, attempts to work with fiducial inference have fallen out of fashion in favour of frequentist inference, Bayesian inference and decision theory. However, fiducial inference is important in the history of statistics since its development led to the parallel development of concepts and tools in theoretical statistics that are widely used. Some current research in statistical methodology is either explicitly linked to fiducial inference or is closely connected to it.",730620562131184509,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,"['Fiducial inference', 'Bayesian inference']","['Fiducial inference', 'Bayesian inference', 'Probability']",set(),set(),0,0.9803921568627451,1.356010174243599
882,1725,Pearson's chi-squared test,http://dbpedia.org/resource/Pearson's_chi-squared_test,http://en.wikipedia.org/wiki/Pearson's_chi-squared_test,"Pearson's chi-squared test is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It is the most widely used of many chi-squared tests (e.g., Yates, likelihood ratio, portmanteau test in time series, etc.) – statistical procedures whose results are evaluated by reference to the chi-squared distribution. Its properties were first investigated by Karl Pearson in 1900. In contexts where it is important to improve a distinction between the test statistic and its distribution, names similar to Pearson χ-squared test or statistic are used. It tests a null hypothesis stating that the frequency distribution of certain events observed in a sample is consistent with a particular theoretical distribution. The events considered must be mutually exclusive and have total probability 1. A common case for this is where the events each cover an outcome of a categorical variable. A simple example is the hypothesis that an ordinary six-sided die is ""fair"" (i. e., all six outcomes are equally likely to occur.)",7293923624752427748,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,"[""Pearson's chi-squared test""]","[""Pearson's chi-squared test"", 'Poisson distribution', 'Bayesian statistics', 'P-value', 'Z-test']",set(),set(),0,0.3481675392670157,1.160541269603922
883,1728,Statistical power,http://dbpedia.org/resource/Statistical_power,http://en.wikipedia.org/wiki/Statistical_power,,5621355552142311460,related_concept,Statistical hypothesis testing,2024-06-23 21:17:09.728,[],"['Statistical power', 'Bayesian statistics', 'Probit']",set(),set(),0,2.1041095890410957,0.5654752844072077
884,1731,Recursion (computer science),http://dbpedia.org/resource/Recursion_(computer_science),http://en.wikipedia.org/wiki/Recursion_(computer_science),"In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem. Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science. The power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions. — Niklaus Wirth, Algorithms + Data Structures = Programs, 1976 Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure) do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as while and for. Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for large problems, it is fundamental to use optimization techniques such as tail call optimization.",7072908302186867863,related_concept,Gene expression programming,2024-06-23 21:17:09.728,"['Data', 'Algorithm']","[""Newton's method"", 'Prolog']",set(),set(),0,2.4261363636363638,1.4196348814550255
885,1732,Linear genetic programming,http://dbpedia.org/resource/Linear_genetic_programming,http://en.wikipedia.org/wiki/Linear_genetic_programming,Linear genetic programming (LGP) is a particular subset of genetic programming wherein computer programs in a population are represented as a sequence of instructions from imperative programming language or machine language. The graph-based data flow that results from a multiple usage of register contents and the existence of structurally noneffective code (introns) are two main differences of this genetic representation from the more common tree-based genetic programming (TGP) variant. In genetic programming (GP) a linear tree is a program composed of a variable number of unary functions and a single terminal. Note that linear tree GP differs from bit string genetic algorithms since a population may contain programs of different lengths and there may be more than two types of functions or more than two types of terminals.,5276464093292659362,related_concept,Gene expression programming,2024-06-23 21:17:09.728,['Linear genetic programming'],"['Linear genetic programming', 'Boolean function']",set(),set(),0,0.5833333333333334,3.3017716005414355e-304
886,1733,Expression tree,http://dbpedia.org/resource/Expression_tree,http://en.wikipedia.org/wiki/Expression_tree,,447352045012963144,related_concept,Gene expression programming,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5833333333333334,0.6330561137155577
887,1735,Genetic operator,http://dbpedia.org/resource/Genetic_operator,http://en.wikipedia.org/wiki/Genetic_operator,"A genetic operator is an operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful. Genetic operators are used to create and maintain genetic diversity (mutation operator), combine existing solutions (also known as chromosomes) into new solutions (crossover) and select between solutions (selection). In his book discussing the use of genetic programming for the optimization of complex problems, computer scientist John Koza has also identified an 'inversion' or 'permutation' operator; however, the effectiveness of this operator has never been conclusively demonstrated and this operator is rarely discussed. Mutation (or mutation-like) operators are said to be unary operators, as they only operate on one chromosome at a time. In contrast, crossover operators are said to be binary operators, as they operate on two chromosomes at a time, combining two existing chromosomes into one new chromosome.",7410077255160757560,related_concept,Gene expression programming,2024-06-23 21:17:09.728,['Genetic operator'],['Genetic operator'],set(),set(),0,1.6428571428571428,3.245172073726194e-304
888,1737,Multi expression programming,http://dbpedia.org/resource/Multi_expression_programming,http://en.wikipedia.org/wiki/Multi_expression_programming,"Multi Expression Programming (MEP) is an evolutionary algorithm for generating mathematical functions describing a given set of data. MEP is a Genetic Programming variant encoding multiple solutions in the same chromosome. MEP representation is not specific (multiple representations have been tested). In the simplest variant, MEP chromosomes are linear strings of instructions. This representation was inspired by Three-address code. MEP strength consists in the ability to encode multiple solutions, of a problem, in the same chromosome. In this way, one can explore larger zones of the search space. For most of the problems this advantage comes with no running-time penalty compared with genetic programming variants encoding a single solution in a chromosome.",5933470415599468892,related_concept,Gene expression programming,2024-06-23 21:17:09.728,[],['Linear genetic programming'],set(),set(),0,0.6631578947368421,0.5151455931766907
889,1738,Parse tree,http://dbpedia.org/resource/Parse_tree,http://en.wikipedia.org/wiki/Parse_tree,"A parse tree or parsing tree or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar. The term parse tree itself is used primarily in computational linguistics; in theoretical syntax, the term syntax tree is more common. Concrete syntax trees reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents. Parse trees are usually constructed based on either the constituency relation of constituency grammars (phrase structure grammars) or the dependency relation of dependency grammars. Parse trees may be generated for sentences in natural languages (see natural language processing), as well as during processing of computer languages, such as programming languages. A related concept is that of phrase marker or P-marker, as used in transformational generative grammar. A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules, and themselves are subject to further transformational rules. A set of possible parse trees for a syntactically ambiguous sentence is called a ""parse forest.""",5470082451072370725,related_concept,Gene expression programming,2024-06-23 21:17:09.728,['Parse tree'],['Parse tree'],set(),set(),0,3.727272727272727,1.3853072581306058
890,1739,Decision tree,http://dbpedia.org/resource/Decision_tree,http://en.wikipedia.org/wiki/Decision_tree,"A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.",7238283472157804014,main_concept,Gene expression programming,2024-06-23 21:17:09.728,['Decision tree'],"['Decision tree', 'Decision rule', 'Accuracy', 'False discovery rate']",set(),set(),0,3.7586206896551726,1.4314701479567364
891,1740,Evolutionary algorithms,http://dbpedia.org/resource/Evolutionary_algorithms,http://en.wikipedia.org/wiki/Evolutionary_algorithms,,2226507712144651207,related_concept,Gene expression programming,2024-06-23 21:17:09.728,[],"['Evolution', 'Evolutionary algorithms', 'Fitness approximation']",set(),set(),0,0.28708133971291866,0.4815847643978337
892,1741,Evolution strategies,http://dbpedia.org/resource/Evolution_strategies,http://en.wikipedia.org/wiki/Evolution_strategies,,8473607227417443928,related_concept,Gene expression programming,2024-06-23 21:17:09.728,[],"['Evolution', 'Evolution strategies']",set(),set(),0,0.27472527472527475,0.5070998241878124
893,1742,Combinatorial optimization,http://dbpedia.org/resource/Combinatorial_optimization,http://en.wikipedia.org/wiki/Combinatorial_optimization,"Combinatorial optimization is a subfield of mathematical optimization that consists of finding an optimal object from a finite set of objects, where the set of feasible solutions is discrete or can be reduced to a discrete set. Typical combinatorial optimization problems are the travelling salesman problem (""TSP""), the minimum spanning tree problem (""MST""), and the knapsack problem. In many such problems, such as the ones previously mentioned, exhaustive search is not tractable, and so specialized algorithms that quickly rule out large parts of the search space or approximation algorithms must be resorted to instead. Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, software engineering, applied mathematics and theoretical computer science. Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures), although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.",5798653956209574591,related_concept,Gene expression programming,2024-06-23 21:17:09.728,['Combinatorial optimization'],['Combinatorial optimization'],set(),set(),0,2.1654411764705883,4.321904826603011e-304
894,1743,Statistical classification,http://dbpedia.org/resource/Statistical_classification,http://en.wikipedia.org/wiki/Statistical_classification,"In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to. Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis.",6546661504552202031,main_concept,Gene expression programming,2024-06-23 21:17:09.728,[],"['Classification', 'Algorithm']",set(),set(),0,2.791866028708134,1.4015012548669386
895,1746,Symbolic Regression,http://dbpedia.org/resource/Symbolic_Regression,http://en.wikipedia.org/wiki/Symbolic_Regression,,1580555491290722526,related_concept,Gene expression programming,2024-06-23 21:17:09.728,[],"['Accuracy', 'Data', 'Symbolic Regression', 'AI']",set(),set(),0,0.05,0.5844121669432492
896,1747,Grammatical evolution,http://dbpedia.org/resource/Grammatical_evolution,http://en.wikipedia.org/wiki/Grammatical_evolution,"Grammatical evolution (GE) is an evolutionary computation and, more specifically, a genetic programming (GP) technique (or approach) pioneered by Conor Ryan, JJ Collins and Michael O'Neill in 1998 at the BDS Group in the University of Limerick. As in any other GP approach, the objective is to find an executable program, program fragment, or function, which will achieve a good fitness value for a given objective function. In most published work on GP, a LISP-style tree-structured expression is directly manipulated, whereas GE applies genetic operators to an integer string, subsequently mapped to a program (or similar) through the use of a grammar, which is typically expressed in Backus–Naur form. One of the benefits of GE is that this mapping simplifies the application of search to different programming languages and other structures.",3407838931047910817,related_concept,Gene expression programming,2024-06-23 21:17:09.728,['Grammatical evolution'],"['Grammatical evolution', 'Evolution', 'Algorithm']",set(),set(),0,0.5416666666666666,3.3474241853896468e-304
897,1748,Mutation (genetic algorithm),http://dbpedia.org/resource/Mutation_(genetic_algorithm),http://en.wikipedia.org/wiki/Mutation_(genetic_algorithm),"Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. The classic example of a mutation operator involves a probability that an arbitrary bit in a genetic sequence will be flipped from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be flipped. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types are inversion and floating point mutation. When the gene encoding is restrictive as in permutation problems, mutations are swaps, inversions, and scrambles. The purpose of mutation in GAs is to introduce diversity into the sampled population. Mutation operators are used in an attempt to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping convergence to the global optimum. This reasoning also leads most GA systems to avoid only taking the fittest of the population in generating the next generation, but rather selecting a random (or semi-random) set with a weighting toward those that are fitter. For different genome types, different mutation types are suitable: 
* Bit string mutationThe mutation of bit strings ensue through bit flips at random positions.Example: The probability of a mutation of a bit is , where is the length of the binary vector. Thus, a mutation rate of per mutation and individual selected for mutation is reached. 
* ShrinkThis operator adds a random number taken from a Gaussian distribution with mean equal to the original value of each decision variable characterizing the entry parent vector.",8505223330359724104,related_concept,Gene expression programming,2024-06-23 21:17:09.728,[],"['Evolution', 'Algorithm']",set(),set(),0,0.9574468085106383,0.5786229214430275
898,1750,Gene expression programming,http://dbpedia.org/resource/Gene_expression_programming,http://en.wikipedia.org/wiki/Gene_expression_programming,"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype–phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.",3829775147418457413,main_concept,,2024-06-23 21:17:09.728,[],"['Evolution', 'Algorithm', 'Evolutionary algorithms', 'Gene expression programming', 'Fitness function', 'Classification', 'Matthews correlation coefficient', 'Boolean function', 'Decision tree']",set(),set(),0,1.1150442477876106,3.3127769507720405e-304
899,1751,Confidence band,http://dbpedia.org/resource/Confidence_band,http://en.wikipedia.org/wiki/Confidence_band,,8592070700264360620,related_concept,Confidence interval,2024-06-23 21:17:09.728,[],"['Confidence band', 'Family-wise error rate', 'Prediction']",set(),set(),0,0.07058823529411765,0.48286367207497355
900,1755,Pivotal quantity,http://dbpedia.org/resource/Pivotal_quantity,http://en.wikipedia.org/wiki/Pivotal_quantity,"In statistics, a pivotal quantity or pivot is a function of observations and unobservable parameters such that the function's probability distribution does not depend on the unknown parameters (including nuisance parameters). A pivot quantity need not be a statistic—the function and its value can depend on the parameters of the model, but its distribution must not. If it is a statistic, then it is known as an ancillary statistic. More formally, let be a random sample from a distribution that depends on a parameter (or vector of parameters) . Let be a random variable whose distribution is the same for all . Then is called a pivotal quantity (or simply a pivot). Pivotal quantities are commonly used for normalization to allow data from different data sets to be compared. It is relatively easy to construct pivots for location and scale parameters: for the former we form differences so that location cancels, for the latter ratios so that scale cancels. Pivotal quantities are fundamental to the construction of test statistics, as they allow the statistic to not depend on parameters – for example, Student's t-statistic is for a normal distribution with unknown variance (and mean). They also provide one method of constructing confidence intervals, and the use of pivotal quantities improves performance of the bootstrap. In the form of ancillary statistics, they can be used to construct frequentist prediction intervals (predictive confidence intervals).",8013490037760208366,related_concept,Confidence interval,2024-06-23 21:17:09.728,"[""Student's t-statistic""]","[""Student's t-statistic"", ""Student's t-distribution"", 'Prediction interval', 'Prediction', 'Normal distribution']",set(),set(),0,1.5833333333333333,1.3378110203629694
901,1757,Margin of error,http://dbpedia.org/resource/Margin_of_error,http://en.wikipedia.org/wiki/Margin_of_error,"The margin of error is a statistic expressing the amount of random sampling error in the results of a survey. The larger the margin of error, the less confidence one should have that a poll result would reflect the result of a census of the entire population. The margin of error will be positive whenever a population is incompletely sampled and the outcome measure has positive variance, which is to say, the measure varies. The term margin of error is often used in non-survey contexts to indicate observational error in reporting measured quantities.",3039613627729578493,related_concept,Confidence interval,2024-06-23 21:17:09.728,[],['Bernoulli distribution'],set(),set(),0,16.02777777777778,6.0448058164932994e-304
902,1761,Variance,http://dbpedia.org/resource/Variance,http://en.wikipedia.org/wiki/Variance,"In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its population mean or sample mean. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by , , , , or . An advantage of variance as a measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation; for example, the variance of a sum of uncorrelated random variables is equal to the sum of their variances. A disadvantage of the variance for practical applications is that, unlike the standard deviation, its units differ from the random variable, which is why the standard deviation is more commonly reported as a measure of dispersion once the calculation is finished. There are two distinct concepts that are both called ""variance"". One, as discussed above, is part of a theoretical probability distribution and is defined by an equation. The other variance is a characteristic of a set of observations. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below. The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance.",1702786571908223305,main_concept,Confidence interval,2024-06-23 21:17:09.728,['Variance'],"['Variance', 'Algorithm', 'Cauchy distribution', 'Mean', 'Covariance', 'Poisson distribution', 'U-statistic', 'F-test']",set(),set(),0,4.0,1.3414886056386441
903,1762,Maximum likelihood estimation,http://dbpedia.org/resource/Maximum_likelihood_estimation,http://en.wikipedia.org/wiki/Maximum_likelihood_estimation,"In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when all observed outcomes are assumed to have Normal distributions with the same variance. From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with uniform prior distributions (or a normal prior distribution with a standard deviation of infinity). In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.",8433473707225537469,main_concept,Confidence interval,2024-06-23 21:17:09.728,"['Bayesian inference', 'Normal distribution']","['Bayesian inference', 'Maximum likelihood estimation', 'Fisher information', 'Bayesian estimator', ""Bayes' theorem"", 'Decision theory', 'Decision rule', 'Kullback–Leibler divergence', 'Gradient', 'Gradient descent']",set(),set(),0,0.6784232365145229,1.3270483956520918
904,1764,68–95–99.7 rule,http://dbpedia.org/resource/68–95–99.7_rule,http://en.wikipedia.org/wiki/68–95–99.7_rule,"In statistics, the 68–95–99.7 rule, also known as the empirical rule, is a shorthand used to remember the percentage of values that lie withinan interval estimate in a normal distribution: 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively. In mathematical notation, these facts can be expressed as follows, where Pr is the probability function, Χ is an observation from a normally distributed random variable, μ (mu) is the mean of the distribution, and σ (sigma) is its standard deviation: The usefulness of this heuristic especially depends on the question under consideration. In the empirical sciences, the so-called three-sigma rule of thumb (or 3σ rule) expresses a conventional heuristic that nearly all values are taken to lie within three standard deviations of the mean, and thus it is empirically useful to treat 99.7% probability as near certainty. In the social sciences, a result may be considered ""significant"" if its confidence level is of the order of a two-sigma effect (95%), while in particle physics, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a discovery. A weaker three-sigma rule can be derived from Chebyshev's inequality, stating that even for non-normally distributed variables, at least 88.8% of cases should fall within properly calculated three-sigma intervals. For unimodal distributions, the probability of being within the interval is at least 95% by the Vysochanskij–Petunin inequality. There may be certain assumptions for a distribution that force this probability to be at least 98%.",6295841815334585918,related_concept,Confidence interval,2024-06-23 21:17:09.728,[],"['68–95–99.7 rule', 'Poisson distribution']",set(),set(),0,0.32222222222222224,1.2404852435141522
905,1765,Binomial proportion confidence interval,http://dbpedia.org/resource/Binomial_proportion_confidence_interval,http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval,"In statistics, a binomial proportion confidence interval is a confidence interval for the probability of success calculated from the outcome of a series of success–failure experiments (Bernoulli trials). In other words, a binomial proportion confidence interval is an interval estimate of a success probability p when only the number of experiments n and the number of successes nS are known. There are several formulas for a binomial confidence interval, but all of them rely on the assumption of a binomial distribution. In general, a binomial distribution applies when an experiment is repeated a fixed number of times, each trial of the experiment has two possible outcomes (success and failure), the probability of success is the same for each trial, and the trials are statistically independent. Because the binomial distribution is a discrete probability distribution (i.e., not continuous) and difficult to calculate for large numbers of trials, a variety of approximations are used to calculate this confidence interval, all with their own tradeoffs in accuracy and computational intensity. A simple example of a binomial distribution is the set of various possible outcomes, and their probabilities, for the number of heads observed when a coin is flipped ten times. The observed binomial proportion is the fraction of the flips that turn out to be heads. Given this observed proportion, the confidence interval for the true probability of the coin landing on heads is a range of possible proportions, which may or may not contain the true proportion. A 95% confidence interval for the proportion, for instance, will contain the true proportion 95% of the times that the procedure for constructing the confidence interval is employed.",5821787033325012111,related_concept,Confidence interval,2024-06-23 21:17:09.728,[],"['Bernoulli distribution', ""Pearson's chi-squared test"", 'PDF', 'Beta distribution', 'Jeffreys prior', 'Binomial distribution']",set(),set(),0,1.0348837209302326,1.3066444659173826
906,1767,Neyman construction,http://dbpedia.org/resource/Neyman_construction,http://en.wikipedia.org/wiki/Neyman_construction,"Neyman construction, named after Jerzy Neyman, is a frequentist method to construct an interval at a confidence level such that if we repeat the experiment many times the interval will contain the true value of some parameter a fraction of the time.",4251562294659813510,related_concept,Confidence interval,2024-06-23 21:17:09.728,['Neyman construction'],['Neyman construction'],set(),set(),0,0.9090909090909091,1.3423767431736855
907,1768,Sample size determination,http://dbpedia.org/resource/Sample_size_determination,http://en.wikipedia.org/wiki/Sample_size_determination,"Sample size determination is the act of choosing the number of observations or replicates to include in a statistical sample. The sample size is an important feature of any empirical study in which the goal is to make inferences about a population from a sample. In practice, the sample size used in a study is usually determined based on the cost, time, or convenience of collecting the data, and the need for it to offer sufficient statistical power. In complicated studies there may be several different sample sizes: for example, in a stratified survey there would be different sizes for each stratum. In a census, data is sought for an entire population, hence the intended sample size is equal to the population. In experimental design, where a study may be divided into different treatment groups, there may be different sample sizes for each group. Sample sizes may be chosen in several ways: 
* using experience – small samples, though sometimes unavoidable, can result in wide confidence intervals and risk of errors in statistical hypothesis testing. 
* using a target variance for an estimate to be derived from the sample eventually obtained, i.e., if a high precision is required (narrow confidence interval) this translates to a low target variance of the estimator. 
* using a target for the power of a statistical test to be applied once the sample is collected. 
* using a confidence level, i.e. the larger the required confidence level, the larger the sample size (given a constant precision requirement).",4533328767736550874,main_concept,Confidence interval,2024-06-23 21:17:09.728,['Sample size determination'],"['Sample size determination', 'Bernoulli distribution', 'Normal distribution', 'Statistical power']",set(),set(),0,1.910614525139665,5.985263182549491e-304
908,1769,Method of moments (statistics),http://dbpedia.org/resource/Method_of_moments_(statistics),http://en.wikipedia.org/wiki/Method_of_moments_(statistics),"In statistics, the method of moments is a method of estimation of population parameters. The same principle is used to derive higher moments like skewness and kurtosis. It starts by expressing the population moments (i.e., the expected values of powers of the random variable under consideration) as functions of the parameters of interest. Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters. The method of moments was introduced by Pafnuty Chebyshev in 1887 in the proof of the central limit theorem. The idea of matching empirical moments of a distribution to the population moments dates back at least to Pearson.",4171799722010675625,related_concept,Confidence interval,2024-06-23 21:17:09.728,[],['Econometric'],set(),set(),0,18.71875,1.3332855581647045
909,1770,Coverage probability,http://dbpedia.org/resource/Coverage_probability,http://en.wikipedia.org/wiki/Coverage_probability,"In statistics, the coverage probability is a technique for calculating a confidence interval which is the proportion of the time that the interval contains the true value of interest. For example, suppose our interest is in the mean number of months that people with a particular type of cancer remain in remission following successful treatment with chemotherapy. The confidence interval aims to contain the unknown mean remission duration with a given probability. This is the ""confidence level"" or ""confidence coefficient"" of the constructed interval which is effectively the ""nominal coverage probability"" of the procedure for constructing confidence intervals. The ""nominal coverage probability"" is often set at 0.95. The coverage probability is the actual probability that the interval contains the true mean remission duration in this example. If all assumptions used in deriving a confidence interval are met, the nominal coverage probability will equal the coverage probability (termed ""true"" or ""actual"" coverage probability for emphasis). If any assumptions are not met, the actual coverage probability could either be less than or greater than the nominal coverage probability. When the actual coverage probability is greater than the nominal coverage probability, the interval is termed a conservative (confidence) interval, if it is less than the nominal coverage probability, the interval is termed ""anti-conservative"", or ""permissive."" A discrepancy between the coverage probability and the nominal coverage probability frequently occurs when approximating a discrete distribution with a continuous one. The construction of binomial confidence intervals is a classic example where coverage probabilities rarely equal nominal levels. For the binomial case, several techniques for constructing intervals have been created. The Wilson or Score confidence interval is one well known construction based on the normal distribution. Other constructions include the Wald, exact, Agresti-Coull, and likelihood intervals. While the Wilson interval may not be the most conservative estimate, it produces average coverage probabilities that are equal to nominal levels while still producing a comparatively narrow confidence interval. The ""probability"" in coverage probability is interpreted with respect to a set of hypothetical repetitions of the entire data collection and analysis procedure. In these hypothetical repetitions, independent data sets following the same probability distribution as the actual data are considered, and a confidence interval is computed from each of these data sets; see Neyman construction. The coverage probability is the fraction of these computed confidence intervals that include the desired but unobservable parameter value.",9105942194156661507,related_concept,Confidence interval,2024-06-23 21:17:09.728,['Neyman construction'],['Neyman construction'],set(),set(),0,2.870967741935484,1.3274405627748438
910,1771,Confidence interval,http://dbpedia.org/resource/Confidence_interval,http://en.wikipedia.org/wiki/Confidence_interval,"In frequentist statistics, a confidence interval (CI) is a range of estimates for an unknown parameter. A confidence interval is computed at a designated confidence level; the 95% confidence level is most common, but other levels, such as 90% or 99%, are sometimes used. The confidence level represents the long-run proportion of corresponding CIs that contain the true value of the parameter. For example, out of all intervals computed at the 95% level, 95% of them should contain the parameter's true value. Factors affecting the width of the CI include the confidence level, the sample size, and the variability in the sample. All else being the same, a larger sample would produce a narrower confidence interval. Likewise, greater variability in the sample produces a wider confidence interval, and a higher confidence level would demand a wider confidence interval.",8769006623542004500,main_concept,,2024-06-23 21:17:09.728,[],"['Confidence interval', 'Bayesian inference', 'ANOVA', ""Bayes' theorem""]",set(),set(),0,3.8217054263565893,1.3289037861449133
911,1773,Interval estimation,http://dbpedia.org/resource/Interval_estimation,http://en.wikipedia.org/wiki/Interval_estimation,"In statistics, interval estimation is the use of sample data to estimate an interval of plausible values of a parameter of interest. This is in contrast to point estimation, which gives a single value. The most prevalent forms of interval estimation are confidence intervals (a frequentist method) and credible intervals (a Bayesian method);less common forms include likelihood intervals and fiducial intervals.Other forms of statistical intervals include tolerance intervals (covering a proportion of a sampled population) and prediction intervals (an estimate of a future observation, used mainly in regression analysis). Non-statistical methods that can lead to interval estimates include fuzzy logic.",511692850906993133,related_concept,Posterior probability,2024-06-23 21:17:09.728,[],"['Confidence interval', 'Binomial distribution', 'Poisson distribution', 'Bayes factor', 'Fiducial inference', 'Theorem', 'Inference', 'Prediction interval', 'Prediction', 'Fuzzy logic', 'Bayesian statistics']",set(),set(),0,1.5982905982905984,1.3321661714897175
912,1774,Prediction interval,http://dbpedia.org/resource/Prediction_interval,http://en.wikipedia.org/wiki/Prediction_interval,"In statistical inference, specifically predictive inference, a prediction interval is an estimate of an interval in which a future observation will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis. Prediction intervals are used in both frequentist statistics and Bayesian statistics: a prediction interval bears the same relationship to a future observation that a frequentist confidence interval or Bayesian credible interval bears to an unobservable population parameter: prediction intervals predict the distribution of individual future points, whereas confidence intervals and credible intervals of parameters predict the distribution of estimates of the true population mean or other quantity of interest that cannot be observed.",1359979839262964302,related_concept,Posterior probability,2024-06-23 21:17:09.728,"['Prediction interval', 'Bayesian statistics', 'Prediction']","['Prediction interval', 'Prediction', 'Bayesian statistics', ""Student's t-statistic"", ""Student's t-distribution""]",set(),set(),0,1.6042780748663101,1.2998462755059148
913,1776,Variational Bayesian methods,http://dbpedia.org/resource/Variational_Bayesian_methods,http://en.wikipedia.org/wiki/Variational_Bayesian_methods,"Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning. They are typically used in complex statistical models consisting of observed variables (usually termed ""data"") as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model. As typical in Bayesian inference, the parameters and latent variables are grouped together as ""unobserved variables"". Variational Bayesian methods are primarily used for two purposes: 1. 
* To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables. 2. 
* To derive a lower bound for the marginal likelihood (sometimes called the evidence) of the observed data (i.e. the marginal probability of the data given the model, with marginalization performed over unobserved variables). This is typically used for performing model selection, the general idea being that a higher marginal likelihood for a given model indicates a better fit of the data by that model and hence a greater probability that the model in question was the one that generated the data. (See also the Bayes factor article.) In the former purpose (that of approximating a posterior probability), variational Bayes is an alternative to Monte Carlo sampling methods—particularly, Markov chain Monte Carlo methods such as Gibbs sampling—for taking a fully Bayesian approach to statistical inference over complex distributions that are difficult to evaluate directly or sample. In particular, whereas Monte Carlo techniques provide a numerical approximation to the exact posterior using a set of samples, variational Bayes provides a locally-optimal, exact analytical solution to an approximation of the posterior. Variational Bayes can be seen as an extension of the expectation-maximization (EM) algorithm from maximum a posteriori estimation (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables. As in EM, it finds a set of optimal parameter values, and it has the same alternating structure as does EM, based on a set of interlocked (mutually dependent) equations that cannot be solved analytically. For many applications, variational Bayes produces solutions of comparable accuracy to Gibbs sampling at greater speed. However, deriving the set of equations used to update the parameters iteratively often requires a large amount of work compared with deriving the comparable Gibbs sampling equations. This is the case even for many models that are conceptually quite simple, as is demonstrated below in the case of a basic non-hierarchical model with only two parameters and no latent variables.",7914618669921963122,related_concept,Posterior probability,2024-06-23 21:17:09.728,"['Gibbs sampling', 'Variational Bayesian methods', 'Bayesian inference', 'Bayes factor']","['Variational Bayesian methods', 'Bayesian inference', 'Gibbs sampling', 'Kullback–Leibler divergence', 'Theorem', 'Bayesian network', 'Wishart distribution']",set(),set(),0,1.075,4.812185634426875e-304
914,1777,Credible interval,http://dbpedia.org/resource/Credible_interval,http://en.wikipedia.org/wiki/Credible_interval,"In Bayesian statistics, a credible interval is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are analogous to confidence intervals and confidence regions in frequentist statistics, although they differ on a philosophical basis: Bayesian intervals treat their bounds as fixed and the estimated parameter as a random variable, whereas frequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value. Also, Bayesian credible intervals use (and indeed, require) knowledge of the situation-specific prior distribution, while the frequentist confidence intervals do not. For example, in an experiment that determines the distribution of possible values of the parameter , if the subjective probability that lies between 35 and 45 is 0.95, then is a 95% credible interval.",8258232635271795139,related_concept,Posterior probability,2024-06-23 21:17:09.728,"['Credible interval', 'Bayesian statistics']","['Bayesian statistics', 'Credible interval']",set(),set(),0,1.78125,1.2816328545910038
915,1778,Maximum a posteriori estimation,http://dbpedia.org/resource/Maximum_a_posteriori_estimation,http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation,"In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of maximum likelihood estimation.",4778141520905648614,related_concept,Posterior probability,2024-06-23 21:17:09.728,['Bayesian statistics'],"['Bayesian statistics', ""Bayes' theorem""]",set(),set(),0,1.8310626702997275,4.826011688670705e-304
916,1779,Bayesian epistemology,http://dbpedia.org/resource/Bayesian_epistemology,http://en.wikipedia.org/wiki/Bayesian_epistemology,"Bayesian epistemology is a formal approach to various topics in epistemology that has its roots in Thomas Bayes' work in the field of probability theory. One advantage of its formal method in contrast to traditional epistemology is that its concepts and theorems can be defined with a high degree of precision. It is based on the idea that beliefs can be interpreted as subjective probabilities. As such, they are subject to the laws of probability theory, which act as the norms of rationality. These norms can be divided into static constraints, governing the rationality of beliefs at any moment, and dynamic constraints, governing how rational agents should change their beliefs upon receiving new evidence. The most characteristic Bayesian expression of these principles is found in the form of Dutch books, which illustrate irrationality in agents through a series of bets that lead to a loss for the agent no matter which of the probabilistic events occurs. Bayesians have applied these fundamental principles to various epistemological topics but Bayesianism does not cover all topics of traditional epistemology. The problem of confirmation in the philosophy of science, for example, can be approached through the Bayesian principle of conditionalization by holding that a piece of evidence confirms a theory if it raises the likelihood that this theory is true. Various proposals have been made to define the concept of coherence in terms of probability, usually in the sense that two propositions cohere if the probability of their conjunction is higher than if they were neutrally related to each other. The Bayesian approach has also been fruitful in the field of social epistemology, for example, concerning the problem of testimony or the problem of group belief. Bayesianism still faces various theoretical objections that have not been fully solved.",7796246931631566538,related_concept,Posterior probability,2024-06-23 21:17:09.728,"['Bayesian epistemology', 'Bayesianism']","['Bayesian epistemology', 'Bayesianism', ""Bayes' theorem""]",set(),set(),0,0.8309859154929577,1.2398621962111172
917,1781,Class membership probabilities,http://dbpedia.org/resource/Class_membership_probabilities,http://en.wikipedia.org/wiki/Class_membership_probabilities,,6256567246815694873,related_concept,Posterior probability,2024-06-23 21:17:09.728,[],"['Bayes classifier', 'Brier score']",set(),set(),0,0.2865853658536585,0.6982376479021863
918,1782,Bayesian statistics,http://dbpedia.org/resource/Bayesian_statistics,http://en.wikipedia.org/wiki/Bayesian_statistics,"Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability where probability expresses a degree of belief in an event. The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. This differs from a number of other interpretations of probability, such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials. Bayesian statistical methods use Bayes' theorem to compute and update probabilities after obtaining new data. Bayes' theorem describes the conditional probability of an event based on data as well as prior information or beliefs about the event or conditions related to the event. For example, in Bayesian inference, Bayes' theorem can be used to estimate the parameters of a probability distribution or statistical model. Since Bayesian statistics treats probability as a degree of belief, Bayes' theorem can directly assign a probability distribution that quantifies the belief to the parameter or set of parameters. Bayesian statistics is named after Thomas Bayes, who formulated a specific case of Bayes' theorem in a paper published in 1763. In several papers spanning from the late 18th to the early 19th centuries, Pierre-Simon Laplace developed the Bayesian interpretation of probability. Laplace used methods that would now be considered Bayesian to solve a number of statistical problems. Many Bayesian methods were developed by later authors, but the term was not commonly used to describe such methods until the 1950s. During much of the 20th century, Bayesian methods were viewed unfavorably by many statisticians due to philosophical and practical considerations. Many Bayesian methods required much computation to complete, and most methods that were widely used during the century were based on the frequentist interpretation. However, with the advent of powerful computers and new algorithms like Markov chain Monte Carlo, Bayesian methods have seen increasing use within statistics in the 21st century.",4931675403413436170,related_concept,Posterior probability,2024-06-23 21:17:09.728,"['Bayesian statistics', 'Bayesian inference', ""Bayes' theorem""]","['Bayesian statistics', 'Bayesian inference', ""Bayes' theorem"", 'Bernoulli distribution', 'Statistical model', 'Bayesian hierarchical modeling', 'Bayesian network', 'Exploratory data analysis']",set(),set(),0,4.686274509803922,1.2739407722643894
919,1784,Probability of success,http://dbpedia.org/resource/Probability_of_success,http://en.wikipedia.org/wiki/Probability_of_success,"The probability of success (POS) is a statistics concept commonly used in the pharmaceutical industry including by health authorities to support decision making. The probability of success is a concept closely related to conditional power and predictive power. Conditional power is the probability of observing statistical significance given the observed data assuming the treatment effect parameter equals a specific value. Conditional power is often criticized for this assumption. If we know the exact value of the treatment effect, there is no need to do the experiment. To address this issue, we can consider conditional power in a Bayesian setting by considering the treatment effect parameter to be a random variable. Taking the expected value of the conditional power with respect to the posterior distribution of the parameter gives the predictive power. Predictive power can also be calculated in a frequentist setting. No matter how it is calculated, predictive power is a random variable since it is a conditional probability conditioned on randomly observed data. Both conditional power and predictive power use statistical significance as the success criterion. However, statistical significance is often not sufficient to define success. For example, a health authority often requires the magnitude of the treatment effect to be bigger than an effect which is merely statistically significant in order to support successful registration. In order to address this issue, we can extend conditional power and predictive power to the concept of probability of success. For probability of success, the success criterion is not restricted to statistical significance. It can be something else such as a clinical meaningful result.",1885255834610037357,related_concept,Posterior probability,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.55,1.140528826833381
920,1785,Law of total probability,http://dbpedia.org/resource/Law_of_total_probability,http://en.wikipedia.org/wiki/Law_of_total_probability,"In probability theory, the law (or formula) of total probability is a fundamental rule relating marginal probabilities to conditional probabilities. It expresses the total probability of an outcome which can be realized via several distinct events, hence the name.",7881550575888541892,related_concept,Posterior probability,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.462686567164179,1.321648221467172
921,1788,Bayesian inference,http://dbpedia.org/resource/Bayesian_inference,http://en.wikipedia.org/wiki/Bayesian_inference,"Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called ""Bayesian probability"".",3390727817120659423,related_concept,Posterior probability,2024-06-23 21:17:09.728,"['Bayesian inference', ""Bayes' theorem""]","['Bayesian inference', ""Bayes' theorem"", 'Bayesianism', ""Student's t-distribution"", 'Bayesian statistics', 'Bayes factor', 'Gibbs sampling', 'Bayes classifier', 'Inductive inference', 'Bioinformatics', 'Theorem', 'Bayesian epistemology']",set(),set(),0,2.686046511627907,1.2539882273274012
922,1789,Normalizing constant,http://dbpedia.org/resource/Normalizing_constant,http://en.wikipedia.org/wiki/Normalizing_constant,The concept of a normalizing constant arises in probability theory and a variety of other areas of mathematics. The normalizing constant is used to reduce any probability function to a probability density function with total probability of one.,3199590189465247949,related_concept,Posterior probability,2024-06-23 21:17:09.728,[],"[""Bayes' theorem"", 'Poisson distribution']",set(),set(),0,4.071428571428571,1.3352895439598382
923,1791,Point estimate,http://dbpedia.org/resource/Point_estimate,http://en.wikipedia.org/wiki/Point_estimate,,8295737840797690967,related_concept,Posterior probability,2024-06-23 21:17:09.728,[],"['Bayesian inference', 'Consistency', 'Bayesian estimator', 'Median', 'Confidence interval']",set(),set(),0,0.30666666666666664,0.4994708355912603
924,1793,Logistic regression,http://dbpedia.org/resource/Logistic_regression,http://en.wikipedia.org/wiki/Logistic_regression,"In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled ""0"" and ""1"", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See and for formal mathematics, and for a worked example. Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see ), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see . The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the ""simplest"" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see . The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see . Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in , where he coined ""logit""; see .",1051434138200890446,related_concept,Regression analysis,2024-06-23 21:17:09.728,"['Binary variable', 'Bernoulli distribution', 'Logistic regression']","['Binary variable', 'Bernoulli distribution', 'Logistic regression', 'Multinomial logistic regression', ""Newton's method"", 'Bayesian statistics', 'Bayesian inference', 'Linear regression', 'Goodness of fit', 'F-test', 'SPSS', 'Poisson regression', 'Kullback–Leibler divergence', 'Probit', 'Probit model']",set(),set(),0,2.111111111111111,1.4380129359721545
925,1794,F-test,http://dbpedia.org/resource/F-test,http://en.wikipedia.org/wiki/F-test,"An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact ""F-tests"" mainly arise when the models have been fitted to the data using least squares. The name was coined by George W. Snedecor, in honour of Ronald Fisher. Fisher initially developed the statistic as the variance ratio in the 1920s.",1252610258109312294,related_concept,Regression analysis,2024-06-23 21:17:09.728,['F-test'],"['F-test', 'ANOVA']",set(),set(),0,1.772079772079772,1.2421607012051483
926,1796,Probit model,http://dbpedia.org/resource/Probit_model,http://en.wikipedia.org/wiki/Probit_model,"In statistics, a probit model is a type of regression where the dependent variable can take only two values, for example married or not married. The word is a portmanteau, coming from probability + unit. The purpose of the model is to estimate the probability that an observation with particular characteristics will fall into a specific one of the categories; moreover, classifying observations based on their predicted probabilities is a type of binary classification model. A probit model is a popular specification for a binary response model. As such it treats the same set of problems as does logistic regression using similar techniques. When viewed in the generalized linear model framework, the probit model employs a probit link function. It is most often estimated using the maximum likelihood procedure, such an estimation being called a probit regression.",4110369461378723616,related_concept,Regression analysis,2024-06-23 21:17:09.728,[],"['Probability', 'PDF', 'Gibbs sampling', 'Bayesian linear regression', 'Logistic regression']",set(),set(),0,1.2462686567164178,1.3913788217857619
927,1797,Stepwise regression,http://dbpedia.org/resource/Stepwise_regression,http://en.wikipedia.org/wiki/Stepwise_regression,"In statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion. Usually, this takes the form of a forward, backward, or combined sequence of F-tests or t-tests. The frequent practice of fitting the final selected model followed by reporting estimates and confidence intervals without adjusting them to take the model building process into account has led to calls to stop using stepwise model building altogether or to at least make sure model uncertainty is correctly reflected.Alternatives include other model selection techniques, such as adjusted R2, Akaike information criterion, Bayesian information criterion, Mallows's Cp, PRESS, or false discovery rate.",4947489256115713961,main_concept,Regression analysis,2024-06-23 21:17:09.728,"[""Mallows's Cp"", 'Akaike information criterion', 'F-test']","['F-test', 'Mean', 'Accuracy', 'Stepwise regression']",set(),set(),0,2.590909090909091,1.3864446911887416
928,1798,Ordered logit,http://dbpedia.org/resource/Ordered_logit,http://en.wikipedia.org/wiki/Ordered_logit,"In statistics, the ordered logit model (also ordered logistic regression or proportional odds model) is an ordinal regression model—that is, a regression model for ordinal dependent variables—first considered by Peter McCullagh. For example, if one question on a survey is to be answered by a choice among ""poor"", ""fair"", ""good"", ""very good"" and ""excellent"", and the purpose of the analysis is to see how well that response can be predicted by the responses to other questions, some of which may be quantitative, then ordered logistic regression may be used. It can be thought of as an extension of the logistic regression model that applies to dichotomous dependent variables, allowing for more than two (ordered) response categories.",8903054572492980667,related_concept,Regression analysis,2024-06-23 21:17:09.728,[],"['Ordered logit', 'Maximum likelihood estimation']",set(),set(),0,1.564102564102564,1.4445422855493948
929,1799,Poisson regression,http://dbpedia.org/resource/Poisson_regression,http://en.wikipedia.org/wiki/Poisson_regression,"In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables. Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables. Negative binomial regression is a popular generalization of Poisson regression because it loosens the highly restrictive assumption that the variance is equal to the mean made by the Poisson model. The traditional negative binomial regression model is based on the Poisson-gamma mixture distribution. This model is popular because it models the Poisson heterogeneity with a gamma distribution. Poisson regression models are generalized linear models with the logarithm as the (canonical) link function, and the Poisson distribution function as the assumed probability distribution of the response.",8176872218656878078,main_concept,Regression analysis,2024-06-23 21:17:09.728,"['Poisson distribution', 'Poisson regression']","['Poisson distribution', 'Poisson regression']",set(),set(),0,1.7051597051597052,1.365213167915923
930,1801,Bayesian linear regression,http://dbpedia.org/resource/Bayesian_linear_regression,http://en.wikipedia.org/wiki/Bayesian_linear_regression,"Bayesian linear regression is a type of conditional modeling in which the mean of one variable is described by a linear combination of other variables, with the goal of obtaining the posterior probability of the regression coefficients (as well as other parameters describing the distribution of the regressand) and ultimately allowing the out-of-sample prediction of the regressand (often labelled ) conditional on observed values of the regressors (usually ). The simplest and most widely used version of this model is the normal linear model, in which given is distributed Gaussian. In this model, and under a particular choice of prior probabilities for the parameters—so-called conjugate priors—the posterior can be found analytically. With more arbitrarily chosen priors, the posteriors generally have to be approximated.",8573245839878505311,related_concept,Regression analysis,2024-06-23 21:17:09.728,['Bayesian linear regression'],"['Bayesian linear regression', 'Bayesian inference']",set(),set(),0,1.6210045662100456,1.3751117451901174
931,1802,Least squares,http://dbpedia.org/resource/Least_squares,http://en.wikipedia.org/wiki/Least_squares,"The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation. The most important application is in data fitting. When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares. Least squares problems fall into two categories: linear or ordinary least squares and nonlinear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases. Polynomial least squares describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve. When the observations come from an exponential family with identity as its natural sufficient statistics and mild-conditions are satisfied (e.g. for normal, exponential, Poisson and binomial distributions), standardized least-squares estimates and maximum-likelihood estimates are identical. The method of least squares can also be derived as a method of moments estimator. The following discussion is mostly presented in terms of linear functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit a generalized linear model. The least-squares method was officially discovered and published by Adrien-Marie Legendre (1805), though it is usually also co-credited to Carl Friedrich Gauss (1795) who contributed significant theoretical advances to the method and may have previously used it in his work.",741304093815520741,related_concept,Regression analysis,2024-06-23 21:17:09.728,"['Polynomial', 'Fisher information', 'Least squares']","['Least squares', 'Polynomial', 'Fisher information', 'Vapnik–Chervonenkis dimension', 'Ridge regression']",set(),set(),0,1.4766734279918865,4.975999310147343e-304
932,1803,Simple linear regression,http://dbpedia.org/resource/Simple_linear_regression,http://en.wikipedia.org/wiki/Simple_linear_regression,"In statistics, simple linear regression is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that, as accurately as possible, predicts the dependent variable values as a function of the independent variable.The adjective simple refers to the fact that the outcome variable is related to a single predictor. It is common to make the additional stipulation that the ordinary least squares (OLS) method should be used: the accuracy of each predicted value is measured by its squared residual (vertical distance between the point of the data set and the fitted line), and the goal is to make the sum of these squared deviations as small as possible. Other regression methods that can be used in place of ordinary least squares include least absolute deviations (minimizing the sum of absolute values of residuals) and the Theil–Sen estimator (which chooses a line whose slope is the median of the slopes determined by pairs of sample points). Deming regression (total least squares) also finds a line that fits a set of two-dimensional sample points, but (unlike ordinary least squares, least absolute deviations, and median slope regression) it is not really an instance of simple linear regression, because it does not separate the coordinates into one dependent and one independent variable and could potentially return a vertical line as its fit. The remainder of the article assumes an ordinary least squares regression.In this case, the slope of the fitted line is equal to the correlation between y and x corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that the line passes through the center of mass (x, y) of the data points.",8815987170041243615,related_concept,Regression analysis,2024-06-23 21:17:09.728,[],"['Errors and residuals', 'Confidence interval', ""Student's t-distribution""]",set(),set(),0,1.7051597051597052,4.96332801270561e-304
933,1804,Local regression,http://dbpedia.org/resource/Local_regression,http://en.wikipedia.org/wiki/Local_regression,"Local regression or local polynomial regression, also known as moving regression, is a generalization of the moving average and polynomial regression.Its most common methods, initially developed for scatterplot smoothing, are LOESS (locally estimated scatterplot smoothing) and LOWESS (locally weighted scatterplot smoothing), both pronounced /ˈloʊɛs/. They are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model.In some fields, LOESS is known and commonly referred to as Savitzky–Golay filter (proposed 15 years before LOESS). LOESS and LOWESS thus build on ""classical"" methods, such as linear and nonlinear least squares regression. They address situations in which the classical procedures do not perform well or cannot be effectively applied without undue labor. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data. The trade-off for these features is increased computation. Because it is so computationally intensive, LOESS would have been practically impossible to use in the era when least squares regression was being developed. Most other modern methods for process modeling are similar to LOESS in this respect. These methods have been consciously designed to use our current computational ability to the fullest possible advantage to achieve goals not easily achieved by traditional approaches. A smooth curve through a set of data points obtained with this statistical technique is called a loess curve, particularly when each smoothed value is given by a weighted quadratic least squares regression over the span of values of the y-axis scattergram criterion variable. When each smoothed value is given by a weighted linear least squares regression over the span, this is known as a lowess curve; however, some authorities treat lowess and loess as synonyms.",5093408000451313235,related_concept,Regression analysis,2024-06-23 21:17:09.728,['Local regression'],['Local regression'],set(),set(),0,4.339449541284404,0.7404486210286784
934,1805,Regression validation,http://dbpedia.org/resource/Regression_validation,http://en.wikipedia.org/wiki/Regression_validation,"In statistics, regression validation is the process of deciding whether the numerical results quantifying hypothesized relationships between variables, obtained from regression analysis, are acceptable as descriptions of the data. The validation process can involve analyzing the goodness of fit of the regression, analyzing whether the regression residuals are random, and checking whether the model's predictive performance deteriorates substantially when applied to data that were not used in model estimation.",4927471301626942795,related_concept,Regression analysis,2024-06-23 21:17:09.728,[],"['F-test', 'Logistic regression']",set(),set(),0,5.2792792792792795,1.3909288193212819
935,1806,Prediction,http://dbpedia.org/resource/Prediction,http://en.wikipedia.org/wiki/Prediction,"A prediction (Latin præ-, ""before,"" and dicere, ""to say""), or forecast, is a statement about a future event or data. They are often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference from ""estimation""; different authors and disciplines ascribe different connotations. Future events are necessarily uncertain, so guaranteed accurate information about the future is impossible. Prediction can be useful to assist in making plans about possible developments.",442052514399172945,related_concept,Regression analysis,2024-06-23 21:17:09.728,['Prediction'],"['Prediction', 'Probit', 'Poisson regression', 'Kalman filter', 'Mathematical model', 'Bayesian network']",{'Prediction'},set(),0,2.5045871559633026,1.3000359242935788
936,1807,Robust regression,http://dbpedia.org/resource/Robust_regression,http://en.wikipedia.org/wiki/Robust_regression,"In robust statistics, robust regression seeks to overcome some limitations of traditional regression analysis. A regression analysis models the relationship between one or more independent variables and a dependent variable. Standard types of regression, such as ordinary least squares, have favourable properties if their underlying assumptions are true, but can give misleading results otherwise (i.e. are not robust to assumption violations). Robust regression methods are designed to limit the effect that violations of assumptions by the underlying data-generating process have on regression estimates. For example, least squares estimates for regression models are highly sensitive to outliers: an outlier with twice the error magnitude of a typical observation contributes four (two squared) times as much to the squared error loss, and therefore has more leverage over the regression estimates. The Huber loss function is a robust alternative to standard square error loss that reduces outliers' contributions to the squared error loss, thereby limiting their impact on regression estimates.",5530370726415021125,related_concept,Regression analysis,2024-06-23 21:17:09.728,['Robust regression'],['Robust regression'],set(),set(),0,1.5060532687651331,1.2065483031438333
937,1808,Kriging,http://dbpedia.org/resource/Kriging,http://en.wikipedia.org/wiki/Kriging,"In statistics, originally in geostatistics, kriging or Kriging, also known as Gaussian process regression, is a method of interpolation based on Gaussian process governed by prior covariances. Under suitable assumptions of the prior, kriging gives the best linear unbiased prediction (BLUP) at unsampled locations. Interpolating methods based on other criteria such as smoothness (e.g., smoothing spline) may not yield the BLUP. The method is widely used in the domain of spatial analysis and computer experiments. The technique is also known as Wiener–Kolmogorov prediction, after Norbert Wiener and Andrey Kolmogorov. The theoretical basis for the method was developed by the French mathematician Georges Matheron in 1960, based on the master's thesis of Danie G. Krige, the pioneering plotter of distance-weighted average gold grades at the Witwatersrand reef complex in South Africa. Krige sought to estimate the most likely distribution of gold based on samples from a few boreholes. The English verb is to krige, and the most common noun is kriging; both are often pronounced with a hard ""g"", following an Anglicized pronunciation of the name ""Krige"". The word is sometimes capitalized as Kriging in the literature. Though computationally intensive in its basic formulation, kriging can be scaled to larger problems using various approximation methods.",5724770464410181579,related_concept,Regression analysis,2024-06-23 21:17:09.728,['Kriging'],"['Kriging', 'Bayesian optimization', 'Polynomial']",{'Interpolation'},set(),0,6.462962962962963,1.4268405635441264
938,1809,Quantile regression,http://dbpedia.org/resource/Quantile_regression,http://en.wikipedia.org/wiki/Quantile_regression,"Quantile regression is a type of regression analysis used in statistics and econometrics. Whereas the method of least squares estimates the conditional mean of the response variable across values of the predictor variables, quantile regression estimates the conditional median (or other quantiles) of the response variable. Quantile regression is an extension of linear regression used when the conditions of linear regression are not met.",751842126405688599,related_concept,Regression analysis,2024-06-23 21:17:09.728,"['Quantile', 'Quantile regression']","['Quantile', 'Quantile regression', 'Median', 'Conditional probability', 'Conditional probability distribution', 'Inference']",set(),set(),0,1.2230769230769232,1.1591792519589
939,1810,Ordinary least squares,http://dbpedia.org/resource/Ordinary_least_squares,http://en.wikipedia.org/wiki/Ordinary_least_squares,"In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable. Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface—the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation. The OLS estimator is consistent for the level-one fixed effects when the regressors are exogenous and forms perfect colinearity (rank condition), consistent for the variance estimate of the residuals when regressors have finite fourth moments and—by the Gauss–Markov theorem—optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors are normally distributed with zero mean, OLS is the maximum likelihood estimator that outperforms any non-linear unbiased estimator.",2347920363389128931,related_concept,Regression analysis,2024-06-23 21:17:09.728,[],"['Polynomial', 'F-test', 'Ordinary least squares']",set(),set(),0,3.1526717557251906,4.978995287197834e-304
940,1811,Nonparametric regression,http://dbpedia.org/resource/Nonparametric_regression,http://en.wikipedia.org/wiki/Nonparametric_regression,"Nonparametric regression is a category of regression analysis in which the predictor does not take a predetermined form but is constructed according to information derived from the data. That is, no parametric form is assumed for the relationship between predictors and dependent variable. Nonparametric regression requires larger sample sizes than regression based on parametric models because the data must supply the model structure as well as the model estimates.",5755171442876234799,related_concept,Regression analysis,2024-06-23 21:17:09.728,['Nonparametric regression'],"['Nonparametric regression', 'Linear regression', 'Kriging', 'Smoothing']",set(),set(),0,1.6702127659574468,1.3386335607419202
941,1812,R-squared,http://dbpedia.org/resource/R-squared,http://en.wikipedia.org/wiki/R-squared,,3860104777082113046,related_concept,Regression analysis,2024-06-23 21:17:09.728,[],"['Pearson correlation coefficient', 'F-test', 'Mean', 'ANOVA', 'Bayesian linear regression']",set(),set(),0,0.05202312138728324,0.5637526224847234
942,1814,Kurtosis,http://dbpedia.org/resource/Kurtosis,http://en.wikipedia.org/wiki/Kurtosis,"In probability theory and statistics, kurtosis (from Greek: κυρτός, kyrtos or kurtos, meaning ""curved, arching"") is a measure of the ""tailedness"" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes a particular aspect of a probability distribution. There are different ways to quantify kurtosis for a theoretical distribution, and there are corresponding ways of estimating it using a sample from a population. Different measures of kurtosis may have different . The standard measure of a distribution's kurtosis, originating with Karl Pearson, is a scaled version of the fourth moment of the distribution. This number is related to the tails of the distribution, not its peak; hence, the sometimes-seen characterization of kurtosis as ""peakedness"" is incorrect. For this measure, higher kurtosis corresponds to greater extremity of deviations (or outliers), and not the configuration of data near the mean. It is common to compare the excess kurtosis (defined below) of a distribution to 0, which is the excess kurtosis of any univariate normal distribution. Distributions with negative excess kurtosis are said to be platykurtic, although this does not imply the distribution is ""flat-topped"" as is sometimes stated. Rather, it means the distribution produces fewer and/or less extreme outliers than the normal distribution. An example of a platykurtic distribution is the uniform distribution, which does not produce outliers. Distributions with a positive excess kurtosis are said to be leptokurtic. An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian, and therefore produces more outliers than the normal distribution. It is common practice to use excess kurtosis, which is defined as Pearson's kurtosis minus 3, to provide a simple comparison to the normal distribution. Some authors and software packages use ""kurtosis"" by itself to refer to the excess kurtosis. For clarity and generality, however, this article explicitly indicates where non-excess kurtosis is meant. Alternative measures of kurtosis are: the L-kurtosis, which is a scaled version of the fourth L-moment; measures based on four population or sample quantiles. These are analogous to the alternative measures of skewness that are not based on ordinary moments.",4196389818472102019,related_concept,Statistical population,2024-06-23 21:17:09.728,[],"['Bernoulli distribution', 'Kurtosis', ""Student's t-distribution"", 'Poisson distribution', 'SPSS']",set(),set(),0,1.8525798525798525,1.329407653396767
943,1815,Unbiased estimator,http://dbpedia.org/resource/Unbiased_estimator,http://en.wikipedia.org/wiki/Unbiased_estimator,,7572323851256520669,related_concept,Statistical population,2024-06-23 21:17:09.728,[],"['Mean', 'Poisson distribution', ""Bayes' theorem"", 'Normal distribution', 'Jeffreys prior']",set(),set(),0,0.11180124223602485,0.39046985503445736
944,1817,Horvitz–Thompson estimator,http://dbpedia.org/resource/Horvitz–Thompson_estimator,http://en.wikipedia.org/wiki/Horvitz–Thompson_estimator,"In statistics, the Horvitz–Thompson estimator, named after Daniel G. Horvitz and Donovan J. Thompson, is a method for estimating the total and mean of a in a stratified sample. Inverse probability weighting is applied to account for different proportions of observations within strata in a target population. The Horvitz–Thompson estimator is frequently applied in survey analyses and can be used to account for missing data, as well as many sources of unequal selection probabilities.",5739054909930800549,related_concept,Statistical population,2024-06-23 21:17:09.728,[],['Horvitz–Thompson estimator'],set(),set(),0,1.0,1.0837777448126837
945,1818,Overdispersion,http://dbpedia.org/resource/Overdispersion,http://en.wikipedia.org/wiki/Overdispersion,"In statistics, overdispersion is the presence of greater variability (statistical dispersion) in a data set than would be expected based on a given statistical model. A common task in applied statistics is choosing a parametric model to fit a given set of empirical observations. This necessitates an assessment of the fit of the chosen model. It is usually possible to choose the model parameters in such a way that the theoretical population mean of the model is approximately equal to the sample mean. However, especially for simple models with few parameters, theoretical predictions may not match empirical observations for higher moments. When the observed variance is higher than the variance of a theoretical model, overdispersion has occurred. Conversely, underdispersion means that there was less variation in the data than predicted. Overdispersion is a very common feature in applied data analysis because in practice, populations are frequently heterogeneous (non-uniform) contrary to the assumptions implicit within widely used simple parametric models.",6427094429672051264,related_concept,Statistical population,2024-06-23 21:17:09.728,['Overdispersion'],"['Overdispersion', 'Poisson distribution', 'Poisson regression']",set(),set(),0,1.392156862745098,1.3373298870336672
946,1819,Stratified sampling,http://dbpedia.org/resource/Stratified_sampling,http://en.wikipedia.org/wiki/Stratified_sampling,"In statistics, stratified sampling is a method of sampling from a population which can be partitioned into subpopulations. In statistical surveys, when subpopulations within an overall population vary, it could be advantageous to sample each subpopulation (stratum) independently. Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should define a partition of the population. That is, it should be collectively exhaustive and mutually exclusive: every element in the population must be assigned to one and only one stratum. Then simple random sampling is applied within each stratum. The objective is to improve the precision of the sample by reducing sampling error. It can produce a weighted mean that has less variability than the arithmetic mean of a simple random sample of the population. In computational statistics, stratified sampling is a method of variance reduction when Monte Carlo methods are used to estimate population statistics from a known population.",1283184353191423234,related_concept,Statistical population,2024-06-23 21:17:09.728,[],['Data'],set(),set(),0,2.055072463768116,5.892686618361353e-304
947,1820,Expected value,http://dbpedia.org/resource/Expected_value,http://en.wikipedia.org/wiki/Expected_value,"In probability theory, the expected value (also called expectation, expectancy, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. Informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable. The expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes. In the case of a continuum of possible outcomes, the expectation is defined by integration. In the axiomatic foundation for probability provided by measure theory, the expectation is given by Lebesgue integration. The expected value of a random variable X is often denoted by E(X), E[X], or EX, with E also often stylized as E or",3520811127609624224,related_concept,Statistical population,2024-06-23 21:17:09.728,[],"['Cauchy distribution', 'Expected value']",set(),set(),0,8.035,1.2040209598279823
948,1821,Mixture model,http://dbpedia.org/resource/Mixture_model,http://en.wikipedia.org/wiki/Mixture_model,"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information. Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.",4013849627315703524,main_concept,Statistical population,2024-06-23 21:17:09.728,['Mixture model'],"['Mixture model', 'Bernoulli distribution', ""Student's t-distribution"", 'Gibbs sampling', ""Bayes' theorem"", 'Mixture distribution']",set(),set(),0,1.835820895522388,1.2146408479186788
949,1822,Sample mean,http://dbpedia.org/resource/Sample_mean,http://en.wikipedia.org/wiki/Sample_mean,,3952155500553376470,related_concept,Statistical population,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.8363636363636364,0.5629273877658926
950,1823,Sampling fraction,http://dbpedia.org/resource/Sampling_fraction,http://en.wikipedia.org/wiki/Sampling_fraction,"In sampling theory, the sampling fraction is the ratio of sample size to population size or, in the context of stratified sampling, the ratio of the sample size to the size of the stratum.The formula for the sampling fraction is where n is the sample size and N is the population size. A sampling fraction value close to 1 will occur if the sample size is relatively close to the population size. When sampling from a finite population without replacement, this may cause dependence between individual samples. To correct for this dependence when calculating the sample variance, a finite population correction (or finite population multiplier) of (N-n)/(N-1) may be used. If the sampling fraction is small, less than 0.05, then the sample variance is not appreciably affected by dependence, and the finite population correction may be ignored.",2176458377209590877,related_concept,Statistical population,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.2857142857142858,5.888758281346348e-304
951,1824,Cauchy distribution,http://dbpedia.org/resource/Cauchy_distribution,http://en.wikipedia.org/wiki/Cauchy_distribution,"The Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution. The Cauchy distribution is the distribution of the x-intercept of a ray issuing from with a uniformly distributed angle. It is also the distribution of the ratio of two independent normally distributed random variables with mean zero. The Cauchy distribution is often used in statistics as the canonical example of a ""pathological"" distribution since both its expected value and its variance are undefined (but see below). The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. The Cauchy distribution has no moment generating function. In mathematics, it is closely related to the Poisson kernel, which is the fundamental solution for the Laplace equation in the upper half-plane. It is one of the few distributions that is stable and has a probability density function that can be expressed analytically, the others being the normal distribution and the Lévy distribution.",1937212846606711369,related_concept,Statistical population,2024-06-23 21:17:09.728,['Cauchy distribution'],"['Cauchy distribution', 'PDF', ""Student's t-distribution"", 'Kullback–Leibler divergence', ""Newton's method""]",set(),set(),0,1.5621301775147929,1.3292258513822592
952,1826,Sample (statistics),http://dbpedia.org/resource/Sample_(statistics),http://en.wikipedia.org/wiki/Sample_(statistics),,1477791539257909409,related_concept,Statistical population,2024-06-23 21:17:09.728,[],"[""Bayes' theorem"", 'Probability', 'ANOVA']",set(),set(),0,0.8909465020576132,0.44082477385750657
953,1827,Sample statistics,http://dbpedia.org/resource/Sample_statistics,http://en.wikipedia.org/wiki/Sample_statistics,,2278141050177296168,related_concept,Statistical population,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.011869436201780416,0.4294167165776107
954,1828,Accuracy,http://dbpedia.org/resource/Accuracy,http://en.wikipedia.org/wiki/Accuracy,,2537908944407073984,related_concept,Statistical population,2024-06-23 21:17:09.728,[],"['Accuracy', 'False negative', 'Information retrieval']",set(),set(),0,0.4983277591973244,0.5716069289841201
955,1829,Estimation theory,http://dbpedia.org/resource/Estimation_theory,http://en.wikipedia.org/wiki/Estimation_theory,"Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.In estimation theory, two approaches are generally considered: 
* The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest 
* The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.",5290779510304503917,related_concept,Statistical population,2024-06-23 21:17:09.728,['Estimation theory'],"['Estimation theory', 'Bayesian statistics', 'Fisher information']",set(),set(),0,2.7419354838709675,1.343339086323745
956,1830,Central tendency,http://dbpedia.org/resource/Central_tendency,http://en.wikipedia.org/wiki/Central_tendency,"In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s. The most common measures of central tendency are the arithmetic mean, the median, and the mode. A middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote ""the tendency of quantitative data to cluster around some central value."" The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysis may judge whether data has a strong or a weak central tendency based on its dispersion.",1897202304103834727,main_concept,Statistical population,2024-06-23 21:17:09.728,[],['Kullback–Leibler divergence'],set(),set(),0,1.7615176151761518,4.175353933103786e-304
957,1833,Statistical parameter,http://dbpedia.org/resource/Statistical_parameter,http://en.wikipedia.org/wiki/Statistical_parameter,"In statistics, as opposed to its general use in mathematics, a parameter is any measured quantity of a statistical population that summarises or describes an aspect of the population, such as a mean or a standard deviation. If a population exactly follows a known and defined distribution, for example the normal distribution, then a small set of parameters can be measured which completely describes the population, and can be considered to define a probability distribution for the purposes of extracting samples from this population. A parameter is to a population as a statistic is to a sample; that is to say, a parameter describes the true value calculated from the full population, whereas a statistic is an estimated measurement of the parameter based on a subsample. Thus a ""statistical parameter"" can be more specifically referred to as a population parameter.",8662901788763122955,related_concept,Statistical population,2024-06-23 21:17:09.728,[],"['Poisson distribution', 'Estimator', ""Pearson's chi-squared test""]",set(),set(),0,2.4124629080118694,1.3035443300544274
958,1835,Normal distribution,http://dbpedia.org/resource/Normal_distribution,http://en.wikipedia.org/wiki/Normal_distribution,"In statistics, a normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is The parameter is the mean or expectation of the distribution (and also its median and mode), while the parameter is its standard deviation. The variance of the distribution is . A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal. Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed. A normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions). For other names, see . The univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution.",7558409386080656288,related_concept,Prior probability,2024-06-23 21:17:09.728,['Normal distribution'],"['Normal distribution', 'Bayesian inference', ""Newton's method"", 'Q–Q plot', 'Cauchy distribution', 'Estimator', 'Fisher information', ""Student's t-distribution"", 'Logically']",set(),set(),0,3.961730449251248,1.323104536730887
959,1836,Conjugate prior,http://dbpedia.org/resource/Conjugate_prior,http://en.wikipedia.org/wiki/Conjugate_prior,"In Bayesian probability theory, if the posterior distribution is in the same probability distribution family as the prior probability distribution , the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function . A conjugate prior is an algebraic convenience, giving a closed-form expression for the posterior; otherwise, numerical integration may be necessary. Further, conjugate priors may give intuition by more transparently showing how a likelihood function updates a prior distribution. The concept, as well as the term ""conjugate prior"", were introduced by Howard Raiffa and Robert Schlaifer in their work on Bayesian decision theory. A similar concept had been discovered independently by George Alfred Barnard.",4532040665441739120,related_concept,Prior probability,2024-06-23 21:17:09.728,[],"['Wishart distribution', 'Beta distribution', 'Data', 'Poisson distribution', ""Bayes' theorem"", 'Gamma distribution']",set(),set(),0,1.7075471698113207,0.9238552937314921
960,1837,Beta distribution,http://dbpedia.org/resource/Beta_distribution,http://en.wikipedia.org/wiki/Beta_distribution,"In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] in terms of two positive parameters, denoted by alpha (α) and beta (β), that appear as exponents of the random variable and control the shape of the distribution. The beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines. The beta distribution is a suitable model for the random behavior of percentages and proportions. In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. The formulation of the beta distribution discussed here is also known as the beta distribution of the first kind, whereas beta distribution of the second kind is an alternative name for the beta prime distribution. The generalization to multiple variables is called a Dirichlet distribution.",7934356868732190792,related_concept,Prior probability,2024-06-23 21:17:09.728,['Bayesian inference'],"['Geometric mean', 'Bayesian inference', 'PDF', 'Bernoulli distribution', 'Mean', 'Fisher information', 'Variance', 'Kurtosis', 'Harmonic mean', 'Expected value', 'Maximum likelihood estimation', 'Kullback–Leibler divergence', 'SPSS', 'Jeffreys prior', 'Beta distribution', 'Probability', 'Binomial distribution', ""Bayes' theorem"", 'Pearson distribution', ""Fisher's method""]",set(),set(),0,1.2100456621004567,1.3323914089259208
961,1838,Principle of maximum entropy,http://dbpedia.org/resource/Principle_of_maximum_entropy,http://en.wikipedia.org/wiki/Principle_of_maximum_entropy,"The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge about a system is the one with largest entropy, in the context of precisely stated prior data (such as a proposition that expresses ). Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the best choice.",2275872584950802005,related_concept,Prior probability,2024-06-23 21:17:09.728,[],"['Bayesian inference', 'Kullback–Leibler divergence', ""Bayes' theorem""]",set(),set(),0,1.5780346820809248,5.092529699453184e-304
962,1839,Frequentist inference,http://dbpedia.org/resource/Frequentist_inference,http://en.wikipedia.org/wiki/Frequentist_inference,"Frequentist inference is a type of statistical inference based in frequentist probability, which treats “probability” in equivalent terms to “frequency” and draws conclusions from sample-data by means of emphasizing the frequency or proportion of findings in the data. Frequentist-inference underlies frequentist statistics, in which the well-established methodologies of statistical hypothesis testing and confidence intervals are founded.",1108112844326145295,related_concept,Prior probability,2024-06-23 21:17:09.728,['Frequentist inference'],"['Frequentist inference', 'Bayesian statistics', 'Bayesian inference', ""Bayes' theorem""]",set(),set(),0,1.7084548104956268,1.3062294974876851
963,1840,Uniform distribution (continuous),http://dbpedia.org/resource/Uniform_distribution_(continuous),http://en.wikipedia.org/wiki/Uniform_distribution_(continuous),,3450205262926321304,related_concept,Prior probability,2024-06-23 21:17:09.728,[],"['Conditional probability', 'Q–Q plot']",set(),set(),0,1.296551724137931,0.5136165062392193
964,1843,Hyperprior,http://dbpedia.org/resource/Hyperprior,http://en.wikipedia.org/wiki/Hyperprior,"In Bayesian statistics, a hyperprior is a prior distribution on a hyperparameter, that is, on a parameter of a prior distribution. As with the term hyperparameter, the use of hyper is to distinguish it from a prior distribution of a parameter of the model for the underlying system. They arise particularly in the use of hierarchical models. For example, if one is using a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: 
* The Bernoulli distribution (with parameter p) is the model of the underlying system; 
* p is a parameter of the underlying system (Bernoulli distribution); 
* The beta distribution (with parameters α and β) is the prior distribution of p; 
* α and β are parameters of the prior distribution (beta distribution), hence hyperparameters; 
* A prior distribution of α and β is thus a hyperprior. In principle, one can iterate the above: if the hyperprior itself has hyperparameters, these may be called hyperhyperparameters, and so forth. One can analogously call the posterior distribution on the hyperparameter the hyperposterior, and, if these are in the same family, call them conjugate hyperdistributions or a conjugate hyperprior. However, this rapidly becomes very abstract and removed from the original problem.",1548457652973993066,related_concept,Prior probability,2024-06-23 21:17:09.728,"['Bernoulli distribution', 'Bayesian statistics']","['Bayesian statistics', 'Bernoulli distribution', 'Bayesian inference', 'Hyperprior', 'Conjugate prior']",set(),set(),0,0.8043478260869565,4.632580047517146e-304
965,1845,Bernstein–von Mises theorem,http://dbpedia.org/resource/Bernstein–von_Mises_theorem,http://en.wikipedia.org/wiki/Bernstein–von_Mises_theorem,"In Bayesian inference, the Bernstein-von Mises theorem provides the basis for using Bayesian credible sets for confidence statements in parametric models. It states that under some conditions, a posterior distribution converges in the limit of infinite data to a multivariate normal distribution centered at the maximum likelihood estimator with covariance matrix given by , where is the true population parameter and is the Fisher information matrix at the true population parameter value. The Bernstein-von Mises theorem links Bayesian inference with frequentist inference. It assumes there is some true probabilistic process that generates the observations, as in frequentism, and then studies the quality of Bayesian methods of recovering that process, and making uncertainty statements about that process. In particular, it states that Bayesian credible sets of a certain credibility level will asymptotically be confidence sets of confidence level , which allows for the interpretation of Bayesian credible sets.",345070784048347618,related_concept,Prior probability,2024-06-23 21:17:09.728,"['Fisher information', 'Bayesian inference']","['Fisher information', 'Bayesian inference', 'Bernstein–von Mises theorem']",set(),set(),0,1.3859649122807018,1.1628229515899458
966,1847,Posterior probability,http://dbpedia.org/resource/Posterior_probability,http://en.wikipedia.org/wiki/Posterior_probability,"The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood, through an application of Bayes' theorem. From an epistemological perspective, the posterior probability contains everything there is to know about an uncertain proposition (such as a scientific hypothesis, or parameter values), given prior knowledge and a mathematical model describing the observations available at a particular time. After the arrival of new information, the current posterior probability may serve as the prior in another round of Bayesian updating. In the context of Bayesian statistics, the posterior probability distribution usually describes the epistemic uncertainty about statistical parameters conditional on a collection of observed data. From a given posterior distribution, various point and interval estimates can be derived, such as the maximum a posteriori (MAP) or the highest posterior density interval (HPDI). But while conceptually simple, the posterior distribution is generally not tractable and therefore needs to be either analytically or numerically approximated.",822634768011151421,main_concept,Prior probability,2024-06-23 21:17:09.728,"['Bayesian statistics', ""Bayes' theorem""]","['Bayesian statistics', 'Prior probability', ""Bayes' theorem"", 'Posterior probability']",set(),set(),0,11.444444444444445,4.68824517828162e-304
967,1848,Jeffreys prior,http://dbpedia.org/resource/Jeffreys_prior,http://en.wikipedia.org/wiki/Jeffreys_prior,"In Bayesian probability, the Jeffreys prior, named after Sir Harold Jeffreys, is a non-informative (objective) prior distribution for a parameter space; its density function is proportional to the square root of the determinant of the Fisher information matrix: It has the key feature that it is invariant under a change of coordinates for the parameter vector . That is, the relative probability assigned to a volume of a probability space using a Jeffreys prior will be the same regardless of the parameterization used to define the Jeffreys prior. This makes it of special interest for use with scale parameters.",8193755873633391188,related_concept,Prior probability,2024-06-23 21:17:09.728,"['Fisher information', 'Jeffreys prior']","['Fisher information', 'Jeffreys prior', 'Bernoulli distribution', 'Poisson distribution']",set(),set(),0,1.446808510638298,4.6895797798384225e-304
968,1850,Hyperparameter,http://dbpedia.org/resource/Hyperparameter,http://en.wikipedia.org/wiki/Hyperparameter,"In Bayesian statistics, a hyperparameter is a parameter of a prior distribution; the term is used to distinguish them from parameters of the model for the underlying system under analysis. For example, if one is using a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: 
* p is a parameter of the underlying system (Bernoulli distribution), and 
* α and β are parameters of the prior distribution (beta distribution), hence hyperparameters. One may take a single value for a given hyperparameter, or one can iterate and take a probability distribution on the hyperparameter itself, called a hyperprior.",8024413700344536131,related_concept,Prior probability,2024-06-23 21:17:09.728,"['Bernoulli distribution', 'Bayesian statistics']","['Bayesian statistics', 'Bernoulli distribution', 'Hyperparameter']",set(),set(),0,1.037037037037037,4.603616807382136e-304
969,1851,Bayesianism,http://dbpedia.org/resource/Bayesianism,http://en.wikipedia.org/wiki/Bayesianism,,1376060602797021651,related_concept,Prior probability,2024-06-23 21:17:09.728,[],"['Bayesian inference', 'Bayesian statistics', ""Bayes' theorem"", 'Probability', 'Bayesianism']",set(),set(),0,0.21710526315789475,0.2800547789006788
970,1853,Algorithmic probability,http://dbpedia.org/resource/Algorithmic_probability,http://en.wikipedia.org/wiki/Algorithmic_probability,"In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. It is used in inductive inference theory and analyses of algorithms. In his general theory of inductive inference, Solomonoff uses the method together with Bayes' rule to obtain probabilities of prediction for an algorithm's future outputs. In the mathematical formalism used, the observations have the form of finite binary strings viewed as outputs of Turing machines, and the universal prior is a probability distribution over the set of finite binary strings calculated from a probability distribution over programs (that is, inputs to a universal Turing machine). The prior is universal in theTuring-computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated.",1344390592463621777,related_concept,Prior probability,2024-06-23 21:17:09.728,[],"[""Solomonoff's theory of inductive inference"", 'Algorithm', 'Algorithmic probability', 'Kolmogorov complexity']",set(),set(),0,2.9310344827586206,1.3365652638091776
971,1854,Prior probability,http://dbpedia.org/resource/Prior_probability,http://en.wikipedia.org/wiki/Prior_probability,"In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable. Bayes' theorem calculates the renormalized pointwise product of the prior and the likelihood function, to produce the posterior probability distribution, which is the conditional distribution of the uncertain quantity given the data. Similarly, the prior probability of a random event or an uncertain proposition is the unconditional probability that is assigned before any relevant evidence is taken into account. Priors can be created using a number of methods. A prior can be determined from past information, such as previous experiments. A prior can be elicited from the purely subjective assessment of an experienced expert. An uninformative prior can be created to reflect a balance among outcomes when no information is available. Priors can also be chosen according to some principle, such as symmetry or maximizing entropy given constraints; examples are the Jeffreys prior or Bernardo's reference prior. When a family of conjugate priors exists, choosing a prior from that family simplifies calculation of the posterior distribution. Parameters of prior distributions are a kind of hyperparameter. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: 
* p is a parameter of the underlying system (Bernoulli distribution), and 
* α and β are parameters of the prior distribution (beta distribution); hence hyperparameters. Hyperparameters themselves may have hyperprior distributions expressing beliefs about their values. A Bayesian model with more than one level of prior like this is called a hierarchical Bayes model.",1666345687720182644,main_concept,,2024-06-23 21:17:09.728,"['Bernoulli distribution', 'Jeffreys prior', 'Hyperparameter', ""Bayes' theorem""]","['Bayesian statistics', 'Bernoulli distribution', 'Bayesianism', 'Jeffreys prior', ""Bayes' theorem"", 'Kullback–Leibler divergence', 'Fisher information', 'Bayesian inference', ""Solomonoff's theory of inductive inference"", 'Likelihood function']",set(),set(),0,6.75,4.645300619252851e-304
972,1855,Activation function,http://dbpedia.org/resource/Activation_function,http://en.wikipedia.org/wiki/Activation_function,"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input. This is similar to the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.",4971103478282016161,related_concept,Artificial neural network,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.055921052631579,0.6363774519702556
973,1856,Neural gas,http://dbpedia.org/resource/Neural_gas,http://en.wikipedia.org/wiki/Neural_gas,"Neural gas is an artificial neural network, inspired by the self-organizing map and introduced in 1991 by Thomas Martinetz and Klaus Schulten. The neural gas is a simple algorithm for finding optimal data representations based on feature vectors. The algorithm was coined ""neural gas"" because of the dynamics of the feature vectors during the adaptation process, which distribute themselves like a gas within the data space. It is applied where data compression or vector quantization is an issue, for example speech recognition, image processing or pattern recognition. As a robustly converging alternative to the k-means clustering it is also used for cluster analysis.",9071897541557615770,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['Neural gas'],['Neural gas'],set(),set(),0,2.142857142857143,1.3808396667419445
974,1857,Spiking neural network,http://dbpedia.org/resource/Spiking_neural_network,http://en.wikipedia.org/wiki/Spiking_neural_network,"Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential – an intrinsic quality of the neuron related to its membrane electrical charge – reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model. The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or - if the firing threshold is reached - the neuron fires. After firing the state variable is reset to a lower value. Various decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.",5868974329185497805,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['Spiking neural network'],"['Spiking neural network', 'Encoding', 'Adaptation']",set(),set(),0,3.8823529411764706,0.3645262369028327
975,1858,Regularization (mathematics),http://dbpedia.org/resource/Regularization_(mathematics),http://en.wikipedia.org/wiki/Regularization_(mathematics),"In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be ""simpler"". It is often used to obtain results for ill-posed problems or to prevent overfitting. Although regularization procedures can be divided in many ways, following delineation is particularly helpful: 
* Explicit regularization is regularization whenever one explicitly adds a term to the optimization problem. These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique. 
* Implicit regularization is all other forms of regularization. This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees). In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one chooses to be more addictive to the data or to enforce generalization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. The work flow usually is, that one tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition. In machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set and not the training data. One of the earliest uses of regularization is Tikhonov regularization, related to the method of least squares.",8680775727872300722,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['Bayesian statistics'],"['Bayesian statistics', 'Early stopping', 'Laplacian matrix', 'Akaike information criterion', 'AI']",set(),set(),0,1.2312312312312312,1.3844300049536449
976,1859,Large width limits of neural networks,http://dbpedia.org/resource/Large_width_limits_of_neural_networks,http://en.wikipedia.org/wiki/Large_width_limits_of_neural_networks,"Artificial neural networks are a class of models used in machine learning, and inspired by biological neural networks. They are the core component of modern deep learning algorithms. Computation in artificial neural networks is usually organized into sequential layers of artificial neurons. The number of neurons in a layer is called the layer width. Theoretical analysis of artificial neural networks sometimes considers the limiting case that layer width becomes large or infinite. This limit enables simple analytic statements to be made about neural network predictions, training dynamics, generalization, and loss surfaces. This wide layer limit is also of practical interest, since finite width neural networks often perform strictly better as layer width is increased.",2595785085111669449,related_concept,Artificial neural network,2024-06-23 21:17:09.728,"['Computation', 'Artificial neural network']",[],set(),set(),0,0.2916666666666667,3.789432627345361e-304
977,1860,Neuroevolution,http://dbpedia.org/resource/Neuroevolution,http://en.wikipedia.org/wiki/Neuroevolution,"Neuroevolution, or neuro-evolution, is a form of artificial intelligence that uses evolutionary algorithms to generate artificial neural networks (ANN), parameters, and rules. It is most commonly applied in artificial life, general game playing and evolutionary robotics. The main benefit is that neuroevolution can be applied more widely than supervised learning algorithms, which require a syllabus of correct input-output pairs. In contrast, neuroevolution requires only a measure of a network's performance at a task. For example, the outcome of a game (i.e. whether one player won or lost) can be easily measured without providing labeled examples of desired strategies. Neuroevolution is commonly used as part of the reinforcement learning paradigm, and it can be contrasted with conventional deep learning techniques that use gradient descent on a neural network with a fixed topology.",559820825640450536,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['Neuroevolution'],"['Neuroevolution', 'Evolution', 'Evolutionary algorithms']",set(),set(),0,0.5891089108910891,1.3249589342914212
978,1861,Feedforward neural network,http://dbpedia.org/resource/Feedforward_neural_network,http://en.wikipedia.org/wiki/Feedforward_neural_network,"A feedforward neural network (FNN) is an artificial neural network wherein connections between the nodes do not form a cycle. As such, it is different from its descendant: recurrent neural networks. The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.",298743645797592616,main_concept,Artificial neural network,2024-06-23 21:17:09.728,[],['Perceptron'],set(),set(),0,1.2218649517684887,3.804617859071408e-304
979,1862,Hybrid neural network,http://dbpedia.org/resource/Hybrid_neural_network,http://en.wikipedia.org/wiki/Hybrid_neural_network,"The term hybrid neural network can have two meanings: 1. 
* Biological neural networks interacting with artificial neuronal models, and 2. 
* Artificial neural networks with a symbolic part (or, conversely, symbolic computations with a connectionist part). As for the first meaning, the artificial neurons and synapses in hybrid networks can be digital or analog. For the digital variant voltage clamps are used to monitor the membrane potential of neurons, to computationally simulate artificial neurons and synapses and to stimulate biological neurons by inducing synaptic. For the analog variant, specially designed electronic circuits connect to a network of living neurons through electrodes. As for the second meaning, incorporating elements of symbolic computation and artificial neural networks into one model was an attempt to combine the advantages of both paradigms while avoiding the shortcomings. Symbolic representations have advantages with respect to explicit, direct control, fast initial coding, dynamic variable binding and knowledge abstraction. Representations of artificial neural networks, on the other hand, show advantages for biological plausibility, learning, robustness (fault-tolerant processing and graceful decay), and generalization to similar input. Since the early 1990s many attempts have been made to reconcile the two approaches.",2817968058070062769,related_concept,Artificial neural network,2024-06-23 21:17:09.728,"['Biological neural network', 'Artificial neural network']",[],set(),set(),0,2.736842105263158,3.69774216229804e-304
980,1863,List of machine learning concepts,http://dbpedia.org/resource/List_of_machine_learning_concepts,http://en.wikipedia.org/wiki/List_of_machine_learning_concepts,,3635283648935042072,related_concept,Artificial neural network,2024-06-23 21:17:09.728,[],"['Machine learning', 'Dimension', 'Dimensionality reduction', 'Ensemble learning', 'Supervised learning', 'Bayesian statistics', 'Decision tree', 'Linear classifier', 'Unsupervised learning', 'Artificial neural network', 'Association rule learning', 'Hierarchical clustering', 'Cluster analysis', 'Anomaly detection', 'Semi-supervised learning', 'Deep learning']",set(),set(),0,0.014273719563392108,0.46460287685491597
981,1864,Convolutional neural network,http://dbpedia.org/resource/Convolutional_neural_network,http://en.wikipedia.org/wiki/Convolutional_neural_network,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network (ANN), most commonly applied to analyze visual imagery. CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series. CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.",4579450746840239281,related_concept,Artificial neural network,2024-06-23 21:17:09.728,[],"['Convolutional neural network', 'Mean', 'Hyperparameter', 'Unsupervised learning', 'Monte Carlo tree search', 'Recurrent neural network']",set(),set(),0,1.49375,3.787230244410609e-304
982,1865,Biological neural network,http://dbpedia.org/resource/Biological_neural_network,http://en.wikipedia.org/wiki/Biological_neural_network,,7693560581558736968,related_concept,Artificial neural network,2024-06-23 21:17:09.728,[],"['Biological neural network', 'Neural network', 'Artificial neural network']",set(),set(),0,3.6774193548387095,0.35180671348473347
983,1866,Vanishing gradient problem,http://dbpedia.org/resource/Vanishing_gradient_problem,http://en.wikipedia.org/wiki/Vanishing_gradient_problem,"In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0,1], and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the early layers train very slowly. Back-propagation allowed researchers to train supervised deep artificial neural networks from scratch, initially with little success. Hochreiter's diplom thesis of 1991 formally identified the reason for this failure in the ""vanishing gradient problem"", which not only affects many-layered feedforward networks, but also recurrent networks. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network. (The combination of unfolding and backpropagation is termed backpropagation through time.) When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.",6377695364154622596,related_concept,Artificial neural network,2024-06-23 21:17:09.728,[],"['Rprop', 'Neural network']",set(),set(),0,0.38461538461538464,1.2375637442550198
984,1867,Unsupervised learning,http://dbpedia.org/resource/Unsupervised_learning,http://en.wikipedia.org/wiki/Unsupervised_learning,"Unsupervised learning is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, which is an important mode of learning in people, the machine is forced to build a concise representation of its world and then generate imaginative content from it. In contrast to supervised learning where data is tagged by an expert, e.g. tagged as a ""ball"" or ""fish"", unsupervised methods exhibit self-organization that captures patterns as probability densities or a combination of neural feature preferences encoded in the machine's weights and activations. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as guidance, and semi-supervised learning where a small portion of the data is tagged.",5516417219869230928,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['Unsupervised learning'],"['Unsupervised learning', 'Neural network', 'Inference', 'Cluster analysis', 'Anomaly detection', 'Latent variable']",set(),set(),0,2.432926829268293,4.864121431927117e-304
985,1869,Neural network,http://dbpedia.org/resource/Neural_network,http://en.wikipedia.org/wiki/Neural_network,"A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1. These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.",6697692840068665057,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['AI'],"['AI', 'Neural network']",set(),set(),0,16.03448275862069,3.7377376979472395e-304
986,1870,Neural network software,http://dbpedia.org/resource/Neural_network_software,http://en.wikipedia.org/wiki/Neural_network_software,"Neural network software is used to simulate, research, develop, and apply artificial neural networks, software concepts adapted from biological neural networks, and in some cases, a wider array of adaptive systems such as artificial intelligence and machine learning.",210478199354691879,related_concept,Artificial neural network,2024-06-23 21:17:09.728,"['Neural network', 'Neural network software']","['Neural network', 'Neural network software', 'Data', 'Data analysis', 'Dimension']",set(),set(),0,1.6,0.6825252099203627
987,1871,Artificial neurons,http://dbpedia.org/resource/Artificial_neurons,http://en.wikipedia.org/wiki/Artificial_neurons,,198699186307768050,related_concept,Artificial neural network,2024-06-23 21:17:09.728,[],"['Artificial neurons', 'Neural network', 'Principal component analysis']",set(),set(),0,0.1346153846153846,0.3551627180957285
988,1872,Extreme learning machine,http://dbpedia.org/resource/Extreme_learning_machine,http://en.wikipedia.org/wiki/Extreme_learning_machine,"Extreme learning machines are feedforward neural networks for classification, regression, clustering, sparse approximation, compression and feature learning with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need to be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. The name ""extreme learning machine"" (ELM) was given to such models by its main inventor Guang-Bin Huang. According to their creators, these models are able to produce good generalization performance and learn thousands of times faster than networks trained using backpropagation. In literature, it also shows that these models can outperform support vector machines in both classification and regression applications.",4275270606068685984,related_concept,Artificial neural network,2024-06-23 21:17:09.728,['Extreme learning machine'],"['Extreme learning machine', 'Perceptron', 'AI']",set(),set(),0,0.25477707006369427,1.2356008212086427
989,1873,Deep learning,http://dbpedia.org/resource/Deep_learning,http://en.wikipedia.org/wiki/Deep_learning,"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as , deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, hence the ""structured"" part.",3385796064849052568,related_concept,Artificial neural network,2024-06-23 21:17:09.728,"['Deep learning', 'Artificial neural network']","['Deep learning', 'Speech recognition', 'Convolutional neural network', 'AI', 'Data', 'Artificial neural network', 'Neural network', 'Recurrent neural network', 'Estimator', 'Bayesian inference']",set(),set(),0,2.8768939393939394,3.766986158037862e-304
990,1874,Backpropagation,http://dbpedia.org/resource/Backpropagation,http://en.wikipedia.org/wiki/Backpropagation,"In machine learning, backpropagation (backprop, BP) is a widely used algorithm for training feedforward artificial neural networks. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally. These classes of algorithms are all referred to generically as ""backpropagation"". In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming. The term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent. Backpropagation generalizes the gradient computation in the delta rule, which is the single-layer version of backpropagation, and is in turn generalized by automatic differentiation, where backpropagation is a special case of reverse accumulation (or ""reverse mode""). The term backpropagation and its general use in neural networks was announced in , then elaborated and popularized in , but the technique was independently rediscovered many times, and had many predecessors dating to the 1960s; see . A modern overview is given in the deep learning textbook by .",9027490568043579304,main_concept,Artificial neural network,2024-06-23 21:17:09.728,['Backpropagation'],"['Gradient', 'Gradient descent', 'Backpropagation', 'Fisher information', 'Euclidean distance']",set(),set(),0,1.3736263736263736,1.2001789798960418
991,1875,Artificial neural network,http://dbpedia.org/resource/Artificial_neural_network,http://en.wikipedia.org/wiki/Artificial_neural_network,"Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets, are computing systems inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The ""signal"" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.",8706730753504478240,main_concept,,2024-06-23 21:17:09.728,['Artificial neural network'],"['Artificial neural network', 'Gradient', 'Neural network', 'AI', 'Data', 'Perceptron', 'Hopfield network', 'Computation', 'Backpropagation', 'Machine learning', 'Supervised learning', 'Dynamic programming', 'Neuroevolution', 'Stochastic', 'Evolution', 'Hyperparameter', 'Dimension']",set(),set(),0,2.1723380900109768,3.719713154091821e-304
992,1876,Probability theory,http://dbpedia.org/resource/Probability_theory,http://en.wikipedia.org/wiki/Probability_theory,"Probability theory is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.Central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes (which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion).Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem. As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics or sequential estimation. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.",1791629570534952553,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,"['Probability', 'Probability theory']","['Probability', 'Probability theory', 'PDF']",set(),set(),0,7.372093023255814,4.671437739161478e-304
993,1877,Statistical independence,http://dbpedia.org/resource/Statistical_independence,http://en.wikipedia.org/wiki/Statistical_independence,,9013711708030586935,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],"['Information content', 'Probability', 'Random variable', 'Theorem']",set(),set(),0,3.197802197802198,0.3578310763488423
994,1878,Random variable,http://dbpedia.org/resource/Random_variable,http://en.wikipedia.org/wiki/Random_variable,"A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events. It is a mapping or a function from possible outcomes (e.g., the possible upper sides of a flipped coin such as heads and tails ) in a sample space (e.g., the set ) to a measurable space, often the real numbers (e.g., in which 1 corresponding to and -1 corresponding to ). Informally, randomness typically represents some fundamental element of chance, such as in the roll of a dice; it may also represent uncertainty, such as measurement error. However, the interpretation of probability is philosophically complicated, and even in specific cases is not always straightforward. The purely mathematical analysis of random variables is independent of such interpretational difficulties, and can be based upon a rigorous axiomatic setup. In the formal mathematical language of measure theory, a random variable is defined as a measurable function from a probability measure space (called the sample space) to a measurable space. This allows consideration of the pushforward measure, which is called the distribution of the random variable; the distribution is thus a probability measure on the set of all possible values of the random variable. It is possible for two random variables to have identical distributions but to differ in significant ways; for instance, they may be independent. It is common to consider the special cases of discrete random variables and absolutely continuous random variables, corresponding to whether a random variable is valued in a discrete set (such as a finite set) or in an interval of real numbers. There are other important possibilities, especially in the theory of stochastic processes, wherein it is natural to consider random sequences or random functions. Sometimes a random variable is taken to be automatically valued in the real numbers, with more general random quantities instead being called random elements. According to George Mackey, Pafnuty Chebyshev was the first person ""to think systematically in terms of random variables"".",2407450117666376235,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],['PDF'],set(),set(),0,3.002016129032258,4.618838531769407e-304
995,1879,Joint probability distribution,http://dbpedia.org/resource/Joint_probability_distribution,http://en.wikipedia.org/wiki/Joint_probability_distribution,"Given two random variables that are defined on the same probability space, the joint probability distribution is the corresponding probability distribution on all possible pairs of outputs. The joint distribution can just as well be considered for any given number of random variables. The joint distribution encodes the marginal distributions, i.e. the distributions of each of the individual random variables. It also encodes the conditional probability distributions, which deal with how the outputs of one random variable are distributed when given information on the outputs of the other random variable(s). In the formal mathematical setup of measure theory, the joint distribution is given by the pushforward measure, by the map obtained by pairing together the given random variables, of the sample space's probability measure. In the case of real-valued random variables, the joint distribution, as a particular multivariate distribution, may be expressed by a multivariate cumulative distribution function, or by a multivariate probability density function together with a multivariate probability mass function. In the special case of continuous random variables, it is sufficient to consider probability density functions, and in the case of discrete random variables, it is sufficient to consider probability mass functions.",1649558242861442338,main_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],"['Bernoulli distribution', 'Bayesian network', 'Covariance']",set(),set(),0,1.8445229681978799,1.2292164579086666
996,1880,Marginal distribution,http://dbpedia.org/resource/Marginal_distribution,http://en.wikipedia.org/wiki/Marginal_distribution,"In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a conditional distribution, which gives the probabilities contingent upon the values of the other variables. Marginal variables are those variables in the subset of variables being retained. These concepts are ""marginal"" because they can be found by summing values in a table along rows or columns, and writing the sum in the margins of the table. The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing (that is, focusing on the sums in the margin) over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out. The context here is that the theoretical studies being undertaken, or the data analysis being done, involves a wider set of random variables but that attention is being limited to a reduced number of those variables. In many applications, an analysis may start with a given collection of random variables, then first extend the set by defining new ones (such as the sum of the original random variables) and finally reduce the number by placing interest in the marginal distribution of a subset (such as the sum). Several different analyses may be done, each treating a different subset of variables as the marginal variables.",379098815007772027,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],[],set(),set(),0,6.757575757575758,1.2838167787910648
997,1882,Conditional probability,http://dbpedia.org/resource/Conditional_probability,http://en.wikipedia.org/wiki/Conditional_probability,"In probability theory, conditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) has already occurred. This particular method relies on event B occurring with some sort of relationship with another event A. In this event, the event B can be analyzed by a conditional probability with respect to A. If the event of interest is A and the event B is known or assumed to have occurred, ""the conditional probability of A given B"", or ""the probability of A under the condition B"", is usually written as P(A|B) or occasionally PB(A). This can also be understood as the fraction of probability B that intersects with A: . For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person is sick, then they are much more likely to be coughing. For example, the conditional probability that someone unwell (sick) is coughing might be 75%, in which case we would have that P(Cough) = 5% and P(Cough|Sick) = 75 %. Although there is a relationship between A and B in this example, such a relationship or dependence between A and B is not necessary, nor do they have to occur simultaneously. P(A|B) may or may not be equal to P(A) (the unconditional probability of A). If P(A|B) = P(A), then events A and B are said to be independent: in such a case, knowledge about either event does not alter the likelihood of each other. P(A|B) (the conditional probability of A given B) typically differs from P(B|A). For example, if a person has dengue fever, the person might have a 90% chance of being tested as positive for the disease. In this case, what is being measured is that if event B (having dengue) has occurred, the probability of A (tested as positive) given that B occurred is 90%, simply writing P(A|B) = 90%. Alternatively, if a person is tested as positive for dengue fever, they may have only a 15% chance of actually having this rare disease due to high false positive rates. In this case, the probability of the event B (having dengue) given that the event A (testing positive) has occurred is 15% or P(B|A) = 15%. It should be apparent now that falsely equating the two probabilities can lead to various errors of reasoning, which is commonly seen through base rate fallacies. While conditional probabilities can provide extremely useful information, limited information is often supplied or at hand. Therefore, it can be useful to reverse or convert a conditional probability using Bayes' theorem: . Another option is to display conditional probabilities in conditional probability table to illuminate the relationship between events.",5295851773880501949,main_concept,Conditional probability distribution,2024-06-23 21:17:09.728,"[""Bayes' theorem""]","[""Bayes' theorem"", 'Conditional probability', 'Probability']",set(),set(),0,5.715686274509804,1.3281936865753088
998,1883,Conditional variance,http://dbpedia.org/resource/Conditional_variance,http://en.wikipedia.org/wiki/Conditional_variance,"In probability theory and statistics, a conditional variance is the variance of a random variable given the value(s) of one or more other variables.Particularly in econometrics, the conditional variance is also known as the scedastic function or skedastic function. Conditional variances are important parts of autoregressive conditional heteroskedasticity (ARCH) models.",8329989923569004412,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,['Conditional variance'],['Conditional variance'],set(),set(),0,1.4782608695652173,1.3439103998953545
999,1884,Likelihood function,http://dbpedia.org/resource/Likelihood_function,http://en.wikipedia.org/wiki/Likelihood_function,"The likelihood function (often simply called the likelihood) is the joint probability of the observed data viewed as a function of the parameters of the chosen statistical model. To emphasize that the likelihood is a function of the parameters, the sample is taken as observed, and the likelihood function is often written as . Equivalently, the likelihood may be written to emphasize that it is the probability of observing sample given , but this notation is less commonly used. According to the likelihood principle, all of the information a given sample provides about is expressed in the likelihood function. In maximum likelihood estimation, the value which maximizes the probability of observing the given sample, i.e. , serves as a point estimate for . Meanwhile in Bayesian statistics, the likelihood function is the conduit through which sample information influences , the posterior probability of the parameter, via Bayes' rule.",2007011940678523848,main_concept,Conditional probability distribution,2024-06-23 21:17:09.728,"['Mean', 'Bayesian statistics']","['Fisher information', 'Bayesian statistics', ""Bayes' theorem"", 'Neyman–Pearson lemma', 'Bayesian inference', 'Bayes factor', 'AI', 'Bayesianism', 'Heuristic']",set(),set(),0,1.74559686888454,0.7956193070208936
1000,1886,Conditional probability table,http://dbpedia.org/resource/Conditional_probability_table,http://en.wikipedia.org/wiki/Conditional_probability_table,"In statistics, the conditional probability table (CPT) is defined for a set of discrete and mutually dependent random variables to display conditional probabilities of a single variable with respect to the others (i.e., the probability of each possible value of one variable if we know the values taken on by the other variables). For example, assume there are three random variables where each has states. Then, the conditional probability table of provides the conditional probability values – where the vertical bar means “given the values of” – for each of the K possible values of the variable and for each possible combination of values of This table has cells. In general, for variables with states for each variable the CPT for any one of them has the number of cells equal to the product A conditional probability table can be put into matrix form. As an example with only two variables, the values of with k and j ranging over K values, create a K×K matrix. This matrix is a stochastic matrix since the columns sum to 1; i.e. for all j. For example, suppose that two binary variables x and y have the joint probability distribution given in this table: Each of the four central cells shows the probability of a particular combination of x and y values. The first column sum is the probability that x =0 and y equals any of the values it can have – that is, the column sum 6/9 is the marginal probability that x=0. If we want to find the probability that y=0 given that x=0, we compute the fraction of the probabilities in the x=0 column that have the value y=0, which is 4/9 ÷ 6/9 = 4/6. Likewise, in the same column we find that the probability that y=1 given that x=0 is 2/9 ÷ 6/9 = 2/6. In the same way, we can also find the conditional probabilities for y equalling 0 or 1 given that x=1. Combining these pieces of information gives us this table of conditional probabilities for y: With more than one conditioning variable, the table would still have one row for each potential value of the variable whose conditional probabilities are to be given, and there would be one column for each possible combination of values of the conditioning variables. Moreover, the number of columns in the table could be substantially expanded to display the probabilities of the variable of interest conditional on specific values of only some, rather than all, of the other variables.",7578713902824284105,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.9230769230769231,1.3075703150599258
1001,1887,Conditioning (probability),http://dbpedia.org/resource/Conditioning_(probability),http://en.wikipedia.org/wiki/Conditioning_(probability),"Beliefs depend on the available information. This idea is formalized in probability theory by conditioning. Conditional probabilities, conditional expectations, and conditional probability distributions are treated on three levels: discrete probabilities, probability density functions, and measure theory. Conditioning leads to a non-random result if the condition is completely specified; otherwise, if the condition is left random, the result of conditioning is also random.",4032995535127752374,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],['Conditional probability'],set(),set(),0,0.9361702127659575,1.231228954061641
1002,1888,Bayes' theorem,http://dbpedia.org/resource/Bayes'_theorem,http://en.wikipedia.org/wiki/Bayes'_theorem,"In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule), named after Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on their age) than simply assuming that the individual is typical of the population as a whole. One of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in the theorem may have different probability interpretations. With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence. Bayesian inference is fundamental to Bayesian statistics, being considered ""to the theory of probability what Pythagoras's theorem is to geometry.""",9019130958432850700,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,"['Bayesian statistics', 'Bayesian inference', ""Bayes' theorem""]","[""Bayes' theorem"", 'Bayesian statistics', 'Bayesian inference', 'Probability', 'Bayes factor', 'Hypothesis']",set(),set(),0,5.877697841726619,1.2214593794751296
1003,1890,Probability density function,http://dbpedia.org/resource/Probability_density_function,http://en.wikipedia.org/wiki/Probability_density_function,"In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be close to that sample. Probability density is the probability per unit length, in other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample. In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and the area under the entire curve is equal to 1. The terms ""probability distribution function"" and ""probability function"" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, ""probability distribution function"" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. ""Density function"" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.",5809915610438688214,main_concept,Conditional probability distribution,2024-06-23 21:17:09.728,"['Probability', 'PDF']","['Probability', 'PDF', 'Law of the unconscious statistician']",set(),set(),0,11.386138613861386,4.720292239338941e-304
1004,1891,Probability measure,http://dbpedia.org/resource/Probability_measure,http://en.wikipedia.org/wiki/Probability_measure,"In mathematics, a probability measure is a real-valued function defined on a set of events in a probability space that satisfies measure properties such as countable additivity. The difference between a probability measure and the more general notion of measure (which includes concepts like area or volume) is that a probability measure must assign value 1 to the entire probability space. Intuitively, the additivity property says that the probability assigned to the union of two disjoint events by the measure should be the sum of the probabilities of the events; for example, the value assigned to ""1 or 2"" in a throw of a dice should be the sum of the values assigned to ""1"" and ""2"". Probability measures have applications in diverse fields, from physics to finance and biology.",6723215142508551336,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,"['Probability', 'Probability measure']","['Probability', 'Probability measure']",set(),set(),0,4.933333333333334,4.634498954995073e-304
1005,1892,Regular conditional probability,http://dbpedia.org/resource/Regular_conditional_probability,http://en.wikipedia.org/wiki/Regular_conditional_probability,"In probability theory, regular conditional probability is a concept that formalizes the notion of conditioning on the outcome of a random variable. The resulting conditional probability distribution is a parametrized family of probability measures called a Markov kernel.",4840547955355792598,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.5769230769230769,1.321474398235167
1006,1893,Moment (mathematics),http://dbpedia.org/resource/Moment_(mathematics),http://en.wikipedia.org/wiki/Moment_(mathematics),"In mathematics, the moments of a function are certain quantitative measures related to the shape of the function's graph. If the function represents mass density, then the zeroth moment is the total mass, the first moment (normalized by total mass) is the center of mass, and the second moment is the moment of inertia. If the function is a probability distribution, then the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis. The mathematical concept is closely related to the concept of moment in physics. For a distribution of mass or probability on a bounded interval, the collection of all the moments (of all orders, from 0 to ∞) uniquely determines the distribution (Hausdorff moment problem). The same is not true on unbounded intervals (Hamburger moment problem). In the mid-nineteenth century, Pafnuty Chebyshev became the first person to think systematically in terms of the moments of random variables.",7139330465315756104,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.31025641025641,1.3308411919923884
1007,1894,Bivariate normal distribution,http://dbpedia.org/resource/Bivariate_normal_distribution,http://en.wikipedia.org/wiki/Bivariate_normal_distribution,,7713091275210768421,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],"['Kullback–Leibler divergence', 'Fisher information', 'Wishart distribution', 'Bayesian statistics']",set(),set(),0,0.042789223454833596,0.47666094657725205
1008,1895,Conditional expectation,http://dbpedia.org/resource/Conditional_expectation,http://en.wikipedia.org/wiki/Conditional_expectation,"In probability theory, the conditional expectation, conditional expected value, or conditional mean of a random variable is its expected value – the value it would take “on average” over an arbitrarily large number of occurrences – given that a certain set of ""conditions"" is known to occur. If the random variable can take on only a finite number of values, the “conditions” are that the variable can only take on a subset of those values. More formally, in the case when the random variable is defined over a discrete probability space, the ""conditions"" are a partition of this probability space. Depending on the context, the conditional expectation can be either a random variable or a function. The random variable is denoted analogously to conditional probability. The function form is either denoted or a separate function symbol such as is introduced with the meaning .",6285414819415380210,related_concept,Conditional probability distribution,2024-06-23 21:17:09.728,[],"['Conditional expectation', 'Law of the unconscious statistician']",set(),set(),0,1.6063829787234043,1.2156516012676828
1009,1896,Conditional probability distribution,http://dbpedia.org/resource/Conditional_probability_distribution,http://en.wikipedia.org/wiki/Conditional_probability_distribution,"In probability theory and statistics, given two jointly distributed random variables and , the conditional probability distribution of given is the probability distribution of when is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value of as a parameter. When both and are categorical variables, a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable. If the conditional distribution of given is a continuous distribution, then its probability density function is known as the conditional density function. The properties of a conditional distribution, such as the moments, are often referred to by corresponding names such as the conditional mean and conditional variance. More generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables.",8846297284780740461,main_concept,,2024-06-23 21:17:09.728,[],['Random variable'],set(),set(),0,3.1463414634146343,1.2699848040748436
1010,1897,Radio noise,http://dbpedia.org/resource/Radio_noise,http://en.wikipedia.org/wiki/Radio_noise,"In radio reception, radio noise is unwanted random radio frequency electrical signals, fluctuating voltages, always present in a radio receiver in addition to the desired radio signal. Radio noise near in frequency to the radio signal being received (in the receiver's passband) interferes with it in the receiver's circuits. Radio noise is a combination of natural electromagnetic atmospheric noise (""spherics"", static) created by electrical processes in the atmosphere like lightning, manmade radio frequency interference (RFI) from other electrical devices picked up by the receiver's antenna, and thermal noise present in the receiver input circuits, caused by the random thermal motion of molecules. The level of noise determines the maximum sensitivity and reception range of a radio receiver; if no noise were picked up with radio signals, even weak transmissions could be received at virtually any distance by making a radio receiver that was sensitive enough. With noise present, if a radio source is so weak and far away that the radio signal in the receiver has a lower amplitude than the average noise, the noise will drown out the signal. The level of noise in a communications circuit is measured by the signal-to-noise ratio (S/N), the ratio of the average amplitude of the signal voltage to the average amplitude of the noise voltage. When this ratio is below one (0 dB) the noise is greater than the signal, requiring special processing to recover the information. The limiting noise source in a receiver depends on the frequency range in use. At frequencies below about 40 MHz, particularly in the mediumwave and longwave bands and below, atmospheric noise and nearby radio frequency interference from electrical switches, motors, vehicle ignition circuits, computers, and other man-made sources tends to be above the thermal noise floor in the receiver's circuits. These noises are often referred to as static. Conversely, at very high frequency and ultra high frequency and above, these sources are often lower, and thermal noise is usually the limiting factor. In the most sensitive receivers at these frequencies, radio telescopes and satellite communication antennas, thermal noise is reduced by cooling the RF front end of the receiver to cryogenic temperatures. Cosmic background noise is experienced at frequencies above about 15 MHz when highly directional antennas are pointed toward the sun or to certain other regions of the sky such as the center of the Milky Way Galaxy. Electromagnetic noise can interfere with electronic equipment in general, causing malfunction, and in recent years standards have been laid down for the levels of electromagnetic radiation that electronic equipment is permitted to radiate. These standards are aimed at ensuring what is referred to as electromagnetic compatibility (EMC).",816225799074724465,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Radio noise', 'Radio']","['Radio noise', 'Radio']",set(),set(),0,0.4049586776859504,1.0675084198343605
1011,1898,Carrier signal,http://dbpedia.org/resource/Carrier_signal,http://en.wikipedia.org/wiki/Carrier_signal,,4352611342147608067,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5384615384615384,0.5729508409226217
1012,1899,Radio,http://dbpedia.org/resource/Radio,http://en.wikipedia.org/wiki/Radio,"Radio is the technology of signaling and communicating using radio waves. Radio waves are electromagnetic waves of frequency between 30 hertz (Hz) and 300 gigahertz (GHz). They are generated by an electronic device called a transmitter connected to an antenna which radiates the waves, and received by another antenna connected to a radio receiver. Radio is very widely used in modern technology, in radio communication, radar, radio navigation, remote control, remote sensing, and other applications. In radio communication, used in radio and television broadcasting, cell phones, two-way radios, wireless networking, and satellite communication, among numerous other uses, radio waves are used to carry information across space from a transmitter to a receiver, by modulating the radio signal (impressing an information signal on the radio wave by varying some aspect of the wave) in the transmitter. In radar, used to locate and track objects like aircraft, ships, spacecraft and missiles, a beam of radio waves emitted by a radar transmitter reflects off the target object, and the reflected waves reveal the object's location. In radio navigation systems such as GPS and VOR, a mobile receiver accepts radio signals from navigational radio beacons whose position is known, and by precisely measuring the arrival time of the radio waves the receiver can calculate its position on Earth. In wireless radio remote control devices like drones, garage door openers, and keyless entry systems, radio signals transmitted from a controller device control the actions of a remote device. Applications of radio waves that do not involve transmitting the waves significant distances, such as RF heating used in industrial processes and microwave ovens, and medical uses such as diathermy and MRI machines, are not usually called radio. The noun radio is also used to mean a broadcast radio receiver. The existence of radio waves was first proven by German physicist Heinrich Hertz on November 11, 1886. In the mid 1890s, building on techniques physicists were using to study electromagnetic waves, Guglielmo Marconi developed the first apparatus for long-distance radio communication, sending a wireless Morse Code message to a source over a kilometer away in 1895, and the first transatlantic signal on December 12, 1901. The first commercial radio broadcast was transmitted on November 2, 1920 when the live returns of the Harding-Cox presidential election were broadcast by Westinghouse Electric and Manufacturing Company in Pittsburgh, under the call sign KDKA. The emission of radio waves is regulated by law, coordinated by the International Telecommunication Union (ITU), which allocates frequency bands in the radio spectrum for different uses.",3906438081457870113,related_concept,Radio receiver,2024-06-23 21:17:09.728,['Radio'],"['Radio', 'Radar']",{'Radio'},set(),0,25.865407319952773,3.647826453289534e-304
1013,1900,Transmitter,http://dbpedia.org/resource/Transmitter,http://en.wikipedia.org/wiki/Transmitter,"In electronics and telecommunications, a radio transmitter or just transmitter is an electronic device which produces radio waves with an antenna. The transmitter itself generates a radio frequency alternating current, which is applied to the antenna. When excited by this alternating current, the antenna radiates radio waves. Transmitters are necessary component parts of all electronic devices that communicate by radio, such as radio and television broadcasting stations, cell phones, walkie-talkies, wireless computer networks, Bluetooth enabled devices, garage door openers, two-way radios in aircraft, ships, spacecraft, radar sets and navigational beacons. The term transmitter is usually limited to equipment that generates radio waves for communication purposes; or radiolocation, such as radar and navigational transmitters. Generators of radio waves for heating or industrial purposes, such as microwave ovens or diathermy equipment, are not usually called transmitters, even though they often have similar circuits. The term is popularly used more specifically to refer to a broadcast transmitter, a transmitter used in broadcasting, as in FM radio transmitter or television transmitter. This usage typically includes both the transmitter proper, the antenna, and often the building it is housed in.",1140947824941016863,related_concept,Radio receiver,2024-06-23 21:17:09.728,['Transmitter'],"['Transmitter', 'Radio']",{'Radar'},set(),0,10.217008797653959,3.665067025120526e-304
1014,1901,Digital audio broadcasting,http://dbpedia.org/resource/Digital_audio_broadcasting,http://en.wikipedia.org/wiki/Digital_audio_broadcasting,,5379476357273919688,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],"['Radio', 'Modulation']",set(),set(),0,0.7727272727272727,0.6036687661876483
1015,1902,Single-sideband modulation,http://dbpedia.org/resource/Single-sideband_modulation,http://en.wikipedia.org/wiki/Single-sideband_modulation,"In radio communications, single-sideband modulation (SSB) or single-sideband suppressed-carrier modulation (SSB-SC) is a type of modulation used to transmit information, such as an audio signal, by radio waves. A refinement of amplitude modulation, it uses transmitter power and bandwidth more efficiently. Amplitude modulation produces an output signal the bandwidth of which is twice the maximum frequency of the original baseband signal. Single-sideband modulation avoids this bandwidth increase, and the power wasted on a carrier, at the cost of increased device complexity and more difficult tuning at the receiver.",1445192176918746988,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Single-sideband modulation', 'Amplitude modulation']","['Single-sideband modulation', 'Amplitude modulation', 'Radio', 'Modulation']",set(),set(),0,2.0580912863070537,3.7183696817260505e-304
1016,1903,Demodulation,http://dbpedia.org/resource/Demodulation,http://en.wikipedia.org/wiki/Demodulation,"Demodulation is extracting the original information-bearing signal from a carrier wave. A demodulator is an electronic circuit (or computer program in a software-defined radio) that is used to recover the information content from the modulated carrier wave. There are many types of modulation so there are many types of demodulators. The signal output from a demodulator may represent sound (an analog audio signal), images (an analog video signal) or binary data (a digital signal). These terms are traditionally used in connection with radio receivers, but many other systems use many kinds of demodulators. For example, in a modem, which is a contraction of the terms modulator/demodulator, a demodulator is used to extract a serial digital data stream from a carrier signal which is used to carry it through a telephone line, coaxial cable, or optical fiber.",6606487825403555695,related_concept,Radio receiver,2024-06-23 21:17:09.728,['Demodulation'],"['Demodulation', 'Frequency modulation']",{'Demodulation'},set(),0,0.7686832740213523,0.7758519396190354
1017,1904,Frequency modulation,http://dbpedia.org/resource/Frequency_modulation,http://en.wikipedia.org/wiki/Frequency_modulation,"Frequency modulation (FM) is the encoding of information in a carrier wave by varying the instantaneous frequency of the wave. The technology is used in telecommunications, radio broadcasting, signal processing, and computing. In analog frequency modulation, such as radio broadcasting, of an audio signal representing voice or music, the instantaneous frequency deviation, i.e. the difference between the frequency of the carrier and its center frequency, has a functional relation to the modulating signal amplitude. Digital data can be encoded and transmitted with a type of frequency modulation known as frequency-shift keying (FSK), in which the instantaneous frequency of the carrier is shifted among a set of frequencies. The frequencies may represent digits, such as '0' and '1'. FSK is widely used in computer modems, such as fax modems, telephone caller ID systems, garage door openers, and other low-frequency transmissions. Radioteletype also uses FSK. Frequency modulation is widely used for FM radio broadcasting. It is also used in telemetry, radar, seismic prospecting, and monitoring newborns for seizures via EEG, two-way radio systems, sound synthesis, magnetic tape-recording systems and some video-transmission systems. In radio transmission, an advantage of frequency modulation is that it has a larger signal-to-noise ratio and therefore rejects radio frequency interference better than an equal power amplitude modulation (AM) signal. For this reason, most music is broadcast over FM radio. However, under severe enough multipath conditions it performs much more poorly than AM, with distinct high frequency noise artifacts that are audible with lower volumes and less complex tones. With high enough volume and carrier deviation audio distortion starts to occur that otherwise wouldn't be present without multipath or with an AM signal. Frequency modulation and phase modulation are the two complementary principal methods of angle modulation; phase modulation is often used as an intermediate step to achieve frequency modulation. These methods contrast with amplitude modulation, in which the amplitude of the carrier wave varies, while the frequency and phase remain constant.",5982689024525226143,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Radio', 'Frequency modulation']","['Frequency modulation', 'Radio', 'Modulation']",set(),set(),0,3.0552380952380953,0.31505287424469774
1018,1905,Digital signal,http://dbpedia.org/resource/Digital_signal,http://en.wikipedia.org/wiki/Digital_signal,"A digital signal is a signal that represents data as a sequence of discrete values; at any given time it can only take on, at most, one of a finite number of values. This contrasts with an analog signal, which represents continuous values; at any given time it represents a real number within a continuous range of values. Simple digital signals represent information in discrete bands of analog levels. All levels within a band of values represent the same information state. In most digital circuits, the signal can have two possible valid values; this is called a binary signal or logic signal. They are represented by two voltage bands: one near a reference value (typically termed as ground or zero volts), and the other a value near the supply voltage. These correspond to the two values ""zero"" and ""one"" (or ""false"" and ""true"") of the Boolean domain, so at any given time a binary signal represents one binary digit (bit). Because of this discretization, relatively small changes to the analog signal levels do not leave the discrete envelope, and as a result are ignored by signal state sensing circuitry. As a result, digital signals have noise immunity; electronic noise, provided it is not too great, will not affect digital circuits, whereas noise always degrades the operation of analog signals to some degree. Digital signals having more than two states are occasionally used; circuitry using such signals is called multivalued logic. For example, signals that can assume three possible states are called three-valued logic. In a digital signal, the physical quantity representing the information may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etcetera. Digital signals are used in all digital electronics, notably computing equipment and data transmission.",655649728334900822,related_concept,Radio receiver,2024-06-23 21:17:09.728,['Digital signal'],['Digital signal'],set(),set(),0,0.8703170028818443,1.3016985251795725
1019,1906,Digital signal processing,http://dbpedia.org/resource/Digital_signal_processing,http://en.wikipedia.org/wiki/Digital_signal_processing,"Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations. The digital signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency. In digital electronics, a digital signal is represented as a pulse train, which is typically generated by the switching of a transistor. Digital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech processing, sonar, radar and other sensor array processing, spectral density estimation, statistical signal processing, digital image processing, data compression, video coding, audio coding, image compression, signal processing for telecommunications, control systems, biomedical engineering, and seismology, among others. DSP can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains. The application of digital computation to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression. Digital signal processing is also fundamental to digital technology, such as digital telecommunication and wireless communications. DSP is applicable to both streaming data and static (stored) data.",8917197734437376783,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Digital signal', 'Digital signal processing']","['Digital signal', 'Digital signal processing', 'Discretization']",set(),set(),0,4.854251012145749,1.3191088689122032
1020,1907,Radar,http://dbpedia.org/resource/Radar,http://en.wikipedia.org/wiki/Radar,"Radar (originally acronym for radio detection and ranging) is a detection system that uses radio waves to determine the distance (ranging), angle, and radial velocity of objects relative to the site. It can be used to detect aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations, and terrain. A radar system consists of a transmitter producing electromagnetic waves in the radio or microwaves domain, a transmitting antenna, a receiving antenna (often the same antenna is used for transmitting and receiving) and a receiver and processor to determine properties of the objects. Radio waves (pulsed or continuous) from the transmitter reflect off the objects and return to the receiver, giving information about the objects' locations and speeds. Radar was developed secretly for military use by several countries in the period before and during World War II. A key development was the cavity magnetron in the United Kingdom, which allowed the creation of relatively small systems with sub-meter resolution. The term RADAR was coined in 1940 by the United States Navy as an acronym for ""radio detection and ranging"". The term radar has since entered English and other languages as a common noun, losing all capitalization. The modern uses of radar are highly diverse, including air and terrestrial traffic control, radar astronomy, air-defense systems, anti-missile systems, marine radars to locate landmarks and other ships, aircraft anti-collision systems, ocean surveillance systems, outer space surveillance and rendezvous systems, meteorological precipitation monitoring, altimetry and flight control systems, guided missile target locating systems, self-driving cars, and ground-penetrating radar for geological observations. High tech radar systems are associated with digital signal processing, machine learning and are capable of extracting useful information from very high noise levels. Other systems similar to radar make use of other parts of the electromagnetic spectrum. One example is lidar, which uses predominantly infrared light from lasers rather than radio waves. With the emergence of driver-less vehicles, radar is expected to assist the automated platform to monitor its environment, thus preventing unwanted incidents.",3364997766288205094,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Radio', 'Radar']","['Radar', 'Radio', 'Matched filter']",{'Radar'},set(),0,16.194154488517746,3.682927936128274e-304
1021,1908,Modulation,http://dbpedia.org/resource/Modulation,http://en.wikipedia.org/wiki/Modulation,"In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a separate signal called the modulation signal that typically contains information to be transmitted. For example, the modulation signal might be an audio signal representing sound from a microphone, a video signal representing moving images from a video camera, or a digital signal representing a sequence of binary digits, a bitstream from a computer. The carrier is higher in frequency than the modulation signal. In radio communication the modulated carrier is transmitted through space as a radio wave to a radio receiver. Another purpose is to transmit multiple channels of information through a single communication medium, using frequency-division multiplexing (FDM). For example in cable television which uses FDM, many carrier signals, each modulated with a different television channel, are transported through a single cable to customers. Since each carrier occupies a different frequency, the channels do not interfere with each other. At the destination end, the carrier signal is demodulated to extract the information bearing modulation signal. A modulator is a device or circuit that performs modulation. A demodulator (sometimes detector) is a circuit that performs demodulation, the inverse of modulation. A modem (from modulator–demodulator), used in bidirectional communication, can perform both operations. The frequency band occupied by the modulation signal is called the baseband, while the higher frequency band occupied by the modulated carrier is called the passband. In analog modulation an analog modulation signal is impressed on the carrier. Examples are amplitude modulation (AM) in which the amplitude (strength) of the carrier wave is varied by the modulation signal, and frequency modulation (FM) in which the frequency of the carrier wave is varied by the modulation signal. These were the earliest types of modulation, and are used to transmit an audio signal representing sound, in AM and FM radio broadcasting. More recent systems use digital modulation, which impresses a digital signal consisting of a sequence of binary digits (bits), a bitstream, on the carrier, by means of mapping bits to elements from a discrete alphabet to be transmitted. This alphabet can consist of a set of real or complex numbers, or sequences, like oscillations of different frequencies, so-called frequency-shift keying (FSK) modulation. A more complicated digital modulation method that employs multiple carriers, orthogonal frequency-division multiplexing (OFDM), is used in WiFi networks, digital radio stations and digital cable television transmission.",6708608863029985329,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.4633569739952716,3.744168695124546e-304
1022,1909,Measuring receiver,http://dbpedia.org/resource/Measuring_receiver,http://en.wikipedia.org/wiki/Measuring_receiver,"In telecommunication, a measuring receiver or measurement receiver is a calibrated laboratory-grade radio receiver designed to measure the characteristics of radio signals. The parameters of such receivers (tuning frequency, receiving bandwidth, gain) can usually be adjusted over a much wider range of values than is the case with other radio receivers. Their circuitry is optimized for stability and to enable calibration and reproducible results. Some measurement receivers also have especially robust input circuits that can survive brief impulses of more than 1000 V, as they can occur during measurements of radio signals on power lines and other conductors.",5022592109486176349,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],['Measuring receiver'],set(),set(),0,0.6071428571428571,3.7295315614478915e-304
1023,1910,Detector (radio),http://dbpedia.org/resource/Detector_(radio),http://en.wikipedia.org/wiki/Detector_(radio),"In radio, a detector is a device or circuit that extracts information from a modulated radio frequency current or voltage. The term dates from the first three decades of radio (1888-1918). Unlike modern radio stations which transmit sound (an audio signal) on an uninterrupted carrier wave, early radio stations transmitted information by radiotelegraphy. The transmitter was switched on and off to produce long or short periods of radio waves, spelling out text messages in Morse code. Therefore, early radio receivers had only to distinguish between the presence or absence of a radio signal. The device that performed this function in the receiver circuit was called a detector. A variety of different detector devices, such as the coherer, electrolytic detector, magnetic detector and the crystal detector, were used during the wireless telegraphy era until superseded by vacuum tube technology. After sound (amplitude modulation, AM) transmission began around 1920, the term evolved to mean a demodulator, (usually a vacuum tube) which extracted the audio signal from the radio frequency carrier wave. This is its current meaning, although modern detectors usually consist of semiconductor diodes, transistors, or integrated circuits. In a superheterodyne receiver the term is also sometimes used to refer to the mixer, the tube or transistor which converts the incoming radio frequency signal to the intermediate frequency. The mixer is called the first detector, while the demodulator that extracts the audio signal from the intermediate frequency is called the second detector. In microwave and millimeter wave technology the terms detector and crystal detector refer to waveguide or coaxial transmission line components, used for power or SWR measurement, that typically incorporate point contact diodes or surface barrier Schottky diodes.",6306101127712791493,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],['Mean'],{'Demodulation'},set(),0,1.128,3.742434105495454e-304
1024,1911,Amplitude modulation,http://dbpedia.org/resource/Amplitude_modulation,http://en.wikipedia.org/wiki/Amplitude_modulation,"Amplitude modulation (AM) is a modulation technique used in electronic communication, most commonly for transmitting messages with a radio wave. In amplitude modulation, the amplitude (signal strength) of the wave is varied in proportion to that of the message signal, such as an audio signal. This technique contrasts with angle modulation, in which either the frequency of the carrier wave is varied, as in frequency modulation, or its phase, as in phase modulation. AM was the earliest modulation method used for transmitting audio in radio broadcasting. It was developed during the first quarter of the 20th century beginning with Roberto Landell de Moura and Reginald Fessenden's radiotelephone experiments in 1900. This original form of AM is sometimes called double-sideband amplitude modulation (DSBAM), because the standard method produces sidebands on either side of the carrier frequency. Single-sideband modulation uses bandpass filters to eliminate one of the sidebands and possibly the carrier signal, which improves the ratio of message power to total transmission power, reduces power handling requirements of line repeaters, and permits better bandwidth utilization of the transmission medium. AM remains in use in many forms of communication in addition to AM broadcasting: shortwave radio, amateur radio, two-way radios, VHF aircraft radio, citizens band radio, and in computer modems in the form of QAM.",7656761819275881546,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Single-sideband modulation', 'Amplitude modulation']","['Amplitude modulation', 'Single-sideband modulation', 'Modulation', 'Transmitter', 'Radio']",set(),set(),0,2.650872817955112,3.728529087526157e-304
1025,1912,Transceiver,http://dbpedia.org/resource/Transceiver,http://en.wikipedia.org/wiki/Transceiver,"In radio communication, a transceiver is an electronic device which is a combination of a radio transmitter and a receiver, hence the name. It can both transmit and receive radio waves using an antenna, for communication purposes. These two related functions are often combined in a single device to reduce manufacturing costs. The term is also used for other devices which can both transmit and receive through a communications channel, such as optical transceivers which transmit and receive light in optical fiber systems, and bus transceivers which transmit and receive digital data in computer data buses. Radio transceivers are widely used in wireless devices. One large use is in two-way radios, which are audio transceivers used for bidirectional person-to-person voice communication. Examples are cell phones, which transmit and receive the two sides of a phone conversation using radio waves to a cell tower, cordless phones in which both the phone handset and the base station have transceivers to communicate both sides of the conversation, and land mobile radio systems like walkie-talkies and CB radios. Another large use is in wireless modems in mobile networked computer devices such laptops, pads, and cellphones, which both transmit digital data to and receive data from a wireless router. Aircraft carry automated microwave transceivers called transponders which, when they are triggered by microwaves from an air traffic control radar, transmit a coded signal back to the radar to identify the aircraft. Satellite transponders in communication satellites receive digital telecommunication data from a satellite ground station, and retransmit it to another ground station.",6046886874761406634,related_concept,Radio receiver,2024-06-23 21:17:09.728,['Radio'],"['Radio', 'Transceiver']",set(),set(),0,5.46067415730337,3.6142671171253052e-304
1026,1913,Passband,http://dbpedia.org/resource/Passband,http://en.wikipedia.org/wiki/Passband,"A passband is the range of frequencies or wavelengths that can pass through a filter. For example, a radio receiver contains a bandpass filter to select the frequency of the desired radio signal out of all the radio waves picked up by its antenna. The passband of a receiver is the range of frequencies it can receive when it is tuned into the desired frequency (channel). A bandpass-filtered signal (that is, a signal with energy only in a passband), is known as a bandpass signal, in contrast to a baseband signal.",1130739376408484769,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],"['Radio', 'Radio receiver']",set(),set(),0,2.923076923076923,3.6320794054670053e-304
1027,1914,Base station,http://dbpedia.org/resource/Base_station,http://en.wikipedia.org/wiki/Base_station,"Base station (or base radio station) is – according to the International Telecommunication Union's (ITU) Radio Regulations (RR) – a ""land station in the land mobile service."" The term is used in the context of mobile telephony, wireless computer networking and other wireless communications and in land surveying. In surveying, it is a GPS receiver at a known position, while in wireless communications it is a transceiver connecting a number of other devices to one another and/or to a wider area.In mobile telephony, it provides the connection between mobile phones and the wider telephone network. In a computer network, it is a transceiver acting as a switch for computers in the network, possibly connecting them to a/another local area network and/or the Internet. In traditional wireless communications, it can refer to the hub of a dispatch fleet such as a taxi or delivery fleet, the base of a TETRA network as used by government and emergency services or a CB shack.",2892783940046549971,related_concept,Radio receiver,2024-06-23 21:17:09.728,"['Radio', 'Base station']","['Radio', 'Base station']",set(),set(),0,0.7989690721649485,0.17016958325317103
1028,1915,Communications receiver,http://dbpedia.org/resource/Communications_receiver,http://en.wikipedia.org/wiki/Communications_receiver,"A communications receiver is a type of radio receiver used as a component of a radio communication link. This is in contrast to a broadcast receiver which is used to receive radio broadcasts. A communication receiver receives parts of the radio spectrum not used for broadcasting, that includes amateur, military, aircraft, marine, and other bands. They are often used with a radio transmitter as part of a two-way radio link for shortwave radio or amateur radio communication, although they are also used for shortwave listening.",2580222654490416375,related_concept,Radio receiver,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.21395348837209302,0.4545204834762959
1029,1916,Radio receiver,http://dbpedia.org/resource/Radio_receiver,http://en.wikipedia.org/wiki/Radio_receiver,"In radio communications, a radio receiver, also known as a receiver, a wireless, or simply a radio, is an electronic device that receives radio waves and converts the information carried by them to a usable form. It is used with an antenna. The antenna intercepts radio waves (electromagnetic waves of radio frequency) and converts them to tiny alternating currents which are applied to the receiver, and the receiver extracts the desired information. The receiver uses electronic filters to separate the desired radio frequency signal from all the other signals picked up by the antenna, an electronic amplifier to increase the power of the signal for further processing, and finally recovers the desired information through demodulation. Radio receivers are essential components of all systems that use radio. The information produced by the receiver may be in the form of sound, video (television), or digital data. A radio receiver may be a separate piece of electronic equipment, or an electronic circuit within another device. The most familiar type of radio receiver for most people is a broadcast radio receiver, which reproduces sound transmitted by radio broadcasting stations, historically the first mass-market radio application. A broadcast receiver is commonly called a ""radio"". However radio receivers are very widely used in other areas of modern technology, in televisions, cell phones, wireless modems, radio clocks and other components of communications, remote control, and wireless networking systems.",781029214910187389,main_concept,,2024-06-23 21:17:09.728,"['Radio', 'Radio receiver']","['Radio', 'Radio receiver', 'Modulation', 'Digital audio broadcasting', 'Digital signal', 'Digital signal processing']",set(),set(),0,0.8139204545454546,0.44571867565676243
1030,1917,Feature vector,http://dbpedia.org/resource/Feature_vector,http://en.wikipedia.org/wiki/Feature_vector,,8546363379930838445,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],"['Algorithm', 'Feature vector']",set(),set(),0,0.7108433734939759,0.581510000352924
1031,1918,Covariance,http://dbpedia.org/resource/Covariance,http://en.wikipedia.org/wiki/Covariance,"In probability theory and statistics, covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values (that is, the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (that is, the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. A distinction must be made between (1) the covariance of two random variables, which is a population parameter that can be seen as a property of the joint probability distribution, and (2) the sample covariance, which in addition to serving as a descriptor of the sample, also serves as an estimated value of the population parameter.",1591162023383468916,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],"['Covariance', 'Random variable', 'Pearson correlation coefficient', 'Kalman filter']",set(),set(),0,1.0792838874680306,1.3241186313796254
1032,1919,Statistics,http://dbpedia.org/resource/Statistics,http://en.wikipedia.org/wiki/Statistics,"Statistics (from German: Statistik, orig. ""description of a state, a country"") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments. When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena. A standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a ""false positive"") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a ""false negative""). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.",8660028679101744923,main_concept,Dependent and independent variables,2024-06-23 21:17:09.728,"['Inference', 'Descriptive statistics', 'Statistics']","['Statistics', 'Inference', 'Descriptive statistics', 'Mathematical statistics', 'Pearson distribution', 'The Genetical Theory of Natural Selection', 'Fisher information', 'Statistical Methods for Research Workers', 'The Design of Experiments', 'Probability', 'Statistical inference', 'Boolean data type', 'Standard deviation', 'Mean', 'Residual sum of squares', 'Least squares', 'Confidence interval', 'Bayesian statistics', 'Exploratory data analysis', 'Machine learning', ""Student's t-test"", 'ANOVA', 'SPSS', 'Gibbs sampling', 'Econometric']","{'Data', 'Statistics'}",set(),0,17.928571428571427,6.122723869230743e-304
1033,1920,Regression analysis,http://dbpedia.org/resource/Regression_analysis,http://en.wikipedia.org/wiki/Regression_analysis,"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression). Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using observational data.",5788507960506895977,main_concept,Dependent and independent variables,2024-06-23 21:17:09.728,['Regression analysis'],"['Regression analysis', 'F-test', 'R-squared', 'Statistical significance', 'Poisson regression', 'Prediction']",set(),set(),0,2.445945945945946,5.050754835729134e-304
1034,1921,Linear regression,http://dbpedia.org/resource/Linear_regression,http://en.wikipedia.org/wiki/Linear_regression,"In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis. Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine. Linear regression has many practical uses. Most applications fall into one of the following two broad categories: 
* If the goal is error reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response. 
* If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response. Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms ""least squares"" and ""linear model"" are closely linked, they are not synonymous.",6304001396758595971,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,['Linear regression'],"['Linear regression', 'Mean', 'General linear model', 'Generalized linear models']",set(),set(),0,2.7940630797773656,5.008472368228615e-304
1035,1922,Linear model,http://dbpedia.org/resource/Linear_model,http://en.wikipedia.org/wiki/Linear_model,"In statistics, the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model. However, the term is also used in time series analysis with a different meaning. In each case, the designation ""linear"" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible.",2713245823498494002,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.5030120481927711,1.2133627988191449
1036,1923,Goodness of fit,http://dbpedia.org/resource/Goodness_of_fit,http://en.wikipedia.org/wiki/Goodness_of_fit,"The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions (see Kolmogorov–Smirnov test), or whether outcome frequencies follow a specified distribution (see Pearson's chi-square test). In the analysis of variance, one of the components into which the variance is partitioned may be a lack-of-fit sum of squares.",3396195690758184561,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],[],set(),set(),0,6.017543859649122,1.3822969589248164
1037,1925,Experimental design,http://dbpedia.org/resource/Experimental_design,http://en.wikipedia.org/wiki/Experimental_design,,5114985916369126380,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],"['Experimental design', 'Inference', 'The Design of Experiments', 'Bayesian statistics']",set(),set(),0,0.2391304347826087,0.4225553964441139
1038,1926,Omitted variable bias,http://dbpedia.org/resource/Omitted_variable_bias,http://en.wikipedia.org/wiki/Omitted_variable_bias,,4928535860932320947,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.08163265306122448,0.3632968424854927
1039,1927,Supervised learning,http://dbpedia.org/resource/Supervised_learning,http://en.wikipedia.org/wiki/Supervised_learning,"Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labelled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.",6441289989262334348,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,['Supervised learning'],['Supervised learning'],set(),set(),0,2.5555555555555554,4.878411078954414e-304
1040,1928,Control variable,http://dbpedia.org/resource/Control_variable,http://en.wikipedia.org/wiki/Control_variable,A control variable (or scientific constant) in scientific experimentation is an experimental element which is constant (controlled) and unchanged throughout the course of the investigation. Control variables could strongly influence experimental results were they not held constant during the experiment in order to test the relative relationship of the dependent variable (DV) and independent variable (IV). The control variables themselves are not of primary interest to the experimenter.,3470149488847560555,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,['Control variable'],['Control variable'],set(),set(),0,5.888888888888889,1.3789750291475165
1041,1929,Vector-valued functions,http://dbpedia.org/resource/Vector-valued_functions,http://en.wikipedia.org/wiki/Vector-valued_functions,,1353130999945100446,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.07920792079207921,0.5854486994201347
1042,1930,Latent variable,http://dbpedia.org/resource/Latent_variable,http://en.wikipedia.org/wiki/Latent_variable,,5021177087741216216,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],['Latent variable'],set(),set(),0,2.5921052631578947,0.607080498185027
1043,1932,Continuous or discrete variable,http://dbpedia.org/resource/Continuous_or_discrete_variable,http://en.wikipedia.org/wiki/Continuous_or_discrete_variable,"In mathematics and statistics, a quantitative variable may be continuous or discrete if they are typically obtained by measuring or counting, respectively. If it can take on two particular real values such that it can also take on all real values between them (even values that are arbitrarily close together), the variable is continuous in that interval. If it can take on a value such that there is a non-infinitesimal gap on each side of it containing no values that the variable can take on, then it is discrete around that value. In some contexts a variable can be discrete in some ranges of the number line and continuous in others.",8091474200859455908,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.9439252336448598,1.2783152830202276
1044,1933,Mathematical model,http://dbpedia.org/resource/Mathematical_model,http://en.wikipedia.org/wiki/Mathematical_model,"A mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in non-physical systems such as the social sciences (such as economics, psychology, sociology, political science). The use of mathematical models to solve problems in business or military operations is a large part of the field of operations research. Mathematical models are also used in music, linguistics, and philosophy (for example, intensively in analytic philosophy). A model may help to explain a system and to study the effects of different components, and to make predictions about behavior.",3753868802776315390,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,['Mathematical model'],"['Mathematical model', 'Bayesian statistics', 'Statistical model', 'Euclidean geometry']",set(),set(),0,6.557788944723618,1.4074057869870482
1045,1934,Stochastic,http://dbpedia.org/resource/Stochastic,http://en.wikipedia.org/wiki/Stochastic,"Stochastic (/stəˈkæstɪk/, from Greek στόχος (stókhos) 'aim, guess') refers to the property of being well described by a random probability distribution. Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves, these two terms are often used synonymously. Furthermore, in probability theory, the formal concept of a stochastic process is also referred to as a random process. Stochasticity is used in many different fields, including the natural sciences such as biology, chemistry, ecology, neuroscience, and physics, as well as technology and engineering fields such as image processing, signal processing, information theory, computer science, cryptography, and telecommunications. It is also used in finance, due to seemingly random changes in financial markets as well as in medicine, linguistics, music, media, colour theory, botany, manufacturing, and geomorphology.",931313774803525425,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,['Stochastic'],"['Stochastic', 'Statistical model']",set(),set(),0,6.3164556962025316,1.044126821870549
1046,1935,Categorical data,http://dbpedia.org/resource/Categorical_data,http://en.wikipedia.org/wiki/Categorical_data,,5178663073733704418,related_concept,Dependent and independent variables,2024-06-23 21:17:09.728,[],"['Categorical data', 'Discretization', 'Regression analysis', 'Bernoulli distribution', 'ANOVA', 'Hypothesis']",set(),set(),0,0.11963882618510158,0.5932976402182449
1047,1936,Dependent and independent variables,http://dbpedia.org/resource/Dependent_and_independent_variables,http://en.wikipedia.org/wiki/Dependent_and_independent_variables,"Dependent and independent variables are variables in mathematical modeling, statistical modeling and experimental sciences. Dependent variables receive this name because, in an experiment, their values are studied under the supposition or demand that they depend, by some law or rule (e.g., by a mathematical function), on the values of other variables. Independent variables, in turn, are not seen as depending on any other variable in the scope of the experiment in question. In this sense, some common independent variables are time, space, density, mass, fluid flow rate, and previous values of some observed value of interest (e.g. human population size) to predict future values (the dependent variable). Of the two, it is always the dependent variable whose variation is being studied, by altering inputs, also known as regressors in a statistical context. In an experiment, any variable that can be attributed a value without attributing a value to any other variable is called an independent variable. Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables may be included for other reasons, such as to account for their potential confounding effect.",573572110128836961,main_concept,,2024-06-23 21:17:09.728,['Dependent and independent variables'],[],set(),set(),0,4.120300751879699,1.3584135950960212
1048,1937,Evolutionary robotics,http://dbpedia.org/resource/Evolutionary_robotics,http://en.wikipedia.org/wiki/Evolutionary_robotics,"Evolutionary robotics is an embodied approach to Artificial Intelligence (AI) in which robots are automatically designed using Darwinian principles of natural selection. The design of a robot, or a subsystem of a robot such as a neural controller, is optimized against a behavioral goal (e.g. run as fast as possible). Usually, designs are evaluated in simulations as fabricating thousands or millions of designs and testing them in the real world is prohibitively expensive in terms of time, money, and safety. An evolutionary robotics experiment starts with a population of randomly generated robot designs. The worst performing designs are discarded and replaced with mutations and/or combinations of the better designs. This evolutionary algorithm continues until a prespecified amount of time elapses or some target performance metric is surpassed. Evolutionary robotics methods are particularly useful for engineering machines that must operate in environments in which humans have limited intuition (nanoscale, space, etc.). Evolved simulated robots can also be used as scientific tools to generate new hypotheses in biology and cognitive science, and to test old hypothesis that require experiments that have proven difficult or impossible to carry out in reality.",907045059531289834,related_concept,Fitness function,2024-06-23 21:17:09.728,"['Evolution', 'AI', 'Evolutionary robotics']","['Evolution', 'AI', 'Evolutionary robotics']",set(),set(),0,2.226950354609929,1.1445478091397734
1049,1938,Figure of merit,http://dbpedia.org/resource/Figure_of_merit,http://en.wikipedia.org/wiki/Figure_of_merit,"A figure of merit is a quantity used to characterize the performance of a device, system or method, relative to its alternatives.",6737265341937688875,related_concept,Fitness function,2024-06-23 21:17:09.728,[],[],set(),set(),0,3.3333333333333335,1.2965706088466098
1050,1939,Genetic programming,http://dbpedia.org/resource/Genetic_programming,http://en.wikipedia.org/wiki/Genetic_programming,"In artificial intelligence, genetic programming (GP) is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs. The operations are: selection of the fittest programs for reproduction (crossover) and mutation according to a predefined fitness measure, usually proficiency at the desired task. The crossover operation involves swapping random parts of selected pairs (parents) to produce new and different offspring that become part of the new generation of programs. Mutation involves substitution of some random part of a program with some other random part of a program. Some programs not selected for reproduction are copied from the current generation to the new generation. Then the selection and other operations are recursively applied to the new generation of programs. Typically, members of each new generation are on average more fit than the members of the previous generation, and the best-of-generation program is often better than the best-of-generation programs from previous generations. Termination of the evolution usually occurs when some individual program reaches a predefined proficiency or fitness level. It may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which is not a globally optimal or even good solution. Multiple runs (dozens to hundreds) are usually necessary to produce a very good result. It may also be necessary to have a large starting population size and variability of the individuals to avoid pathologies.",598465073379868583,related_concept,Fitness function,2024-06-23 21:17:09.728,[],"['Adaptation', 'AI', 'Algorithm', 'Multi expression programming', 'Computation', 'Evolution', 'Evolutionary Computation']",set(),set(),0,2.906779661016949,3.2948829501132293e-304
1051,1940,Co-evolution,http://dbpedia.org/resource/Co-evolution,http://en.wikipedia.org/wiki/Co-evolution,,5488581593729451427,related_concept,Fitness function,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.1552346570397112,0.560770389422282
1052,1941,Interactive genetic algorithms,http://dbpedia.org/resource/Interactive_genetic_algorithms,http://en.wikipedia.org/wiki/Interactive_genetic_algorithms,,5105677923557749764,related_concept,Fitness function,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.03125,0.5862105647342184
1053,1942,Evolutionary Computation,http://dbpedia.org/resource/Evolutionary_Computation,http://en.wikipedia.org/wiki/Evolutionary_Computation,,136943968683449859,related_concept,Fitness function,2024-06-23 21:17:09.728,[],"['Evolution', 'Evolutionary algorithms', 'Computation', 'Evolutionary Computation', 'Genetic algorithm']",set(),set(),0,0.03272727272727273,0.6329752186578319
1054,1943,Genetic algorithm,http://dbpedia.org/resource/Genetic_algorithm,http://en.wikipedia.org/wiki/Genetic_algorithm,"In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles, hyperparameter optimization, etc.",5956348294560512946,main_concept,Fitness function,2024-06-23 21:17:09.728,['Genetic algorithm'],"['Genetic algorithm', 'Algorithm', 'Hypothesis', 'Computer simulation', 'Evolution', 'Adaptation', 'Theorem', 'MATLAB', 'Evolutionary algorithms']",set(),set(),0,3.5,3.3522516622170907e-304
1055,1944,Fitness landscape,http://dbpedia.org/resource/Fitness_landscape,http://en.wikipedia.org/wiki/Fitness_landscape,"In evolutionary biology, fitness landscapes or adaptive landscapes (types of evolutionary landscapes) are used to visualize the relationship between genotypes and reproductive success. It is assumed that every genotype has a well-defined replication rate (often referred to as fitness). This fitness is the ""height"" of the landscape. Genotypes which are similar are said to be ""close"" to each other, while those that are very different are ""far"" from each other. The set of all possible genotypes, their degree of similarity, and their related fitness values is then called a fitness landscape. The idea of a fitness landscape is a metaphor to help explain flawed forms in evolution by natural selection, including exploits and glitches in animals like their reactions to supernormal stimuli. The idea of studying evolution by visualizing the distribution of fitness values as a kind of landscape was first introduced by Sewall Wright in 1932. In evolutionary optimization problems, fitness landscapes are evaluations of a fitness function for all candidate solutions (see below).",3502278575856624322,related_concept,Fitness function,2024-06-23 21:17:09.728,[],"['Fitness landscape', 'Evolution']",set(),set(),0,1.316017316017316,0.6896388383063121
1056,1945,Loss function,http://dbpedia.org/resource/Loss_function,http://en.wikipedia.org/wiki/Loss_function,"In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy. In statistics, typically a loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century. In the context of economics, for example, this is usually economic cost or regret. In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss.",7749621104906875757,related_concept,Fitness function,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.597378277153558,1.4299868719573297
1057,1946,Test functions for optimization,http://dbpedia.org/resource/Test_functions_for_optimization,http://en.wikipedia.org/wiki/Test_functions_for_optimization,"In applied mathematics, test functions, known as artificial landscapes, are useful to evaluate characteristics of optimization algorithms, such as: 
* Convergence rate. 
* Precision. 
* Robustness. 
* General performance. Here some test functions are presented with the aim of giving an idea about the different situations that optimization algorithms have to face when coping with these kinds of problems. In the first part, some objective functions for single-objective optimization cases are presented. In the second part, test functions with their respective Pareto fronts for multi-objective optimization problems (MOP) are given. The artificial landscapes presented herein for single-objective optimization problems are taken from Bäck, Haupt et al. and from Rody Oldenhuis software. Given the number of problems (55 in total), just a few are presented here. The test functions used to evaluate the algorithms for MOP were taken from Deb, Binh et al. and Binh. The software developed by Deb can be downloaded, which implements the NSGA-II procedure with GAs, or the program posted on Internet, which implements the NSGA-II procedure with ES. Just a general form of the equation, a plot of the objective function, boundaries of the object variables and the coordinates of global minima are given herein.",5560535986540362403,related_concept,Fitness function,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.857142857142857,0.8406411596617775
1058,1947,PDF,http://dbpedia.org/resource/PDF,http://en.wikipedia.org/wiki/PDF,"Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems. Based on the PostScript language, each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, fonts, vector graphics, raster images and other information needed to display it. PDF has its roots in ""The Camelot Project"" initiated by Adobe co-founder John Warnock in 1991. PDF was standardized as ISO 32000 in 2008. The last edition as ISO 32000-2:2020 was published in December 2020. PDF files may contain a variety of content besides flat text and graphics including logical structuring elements, interactive elements such as annotations and form-fields, layers, rich media (including video content), three-dimensional objects using U3D or PRC, and various other data formats. The PDF specification also provides for encryption and digital signatures, file attachments, and metadata to enable workflows requiring these features.",1971679976810828729,related_concept,Fitness function,2024-06-23 21:17:09.728,['PDF'],"['PDF', 'Radio', 'Data']",set(),set(),0,14.245977011494253,1.4089940690508622
1059,1948,Fitness approximation,http://dbpedia.org/resource/Fitness_approximation,http://en.wikipedia.org/wiki/Fitness_approximation,"Fitness approximation aims to approximate the objective or fitness functions in evolutionary optimization by building up machine learning models based on data collected from numerical simulations or physical experiments. The machine learning models for fitness approximation are also known as meta-models or surrogates, and evolutionary optimization based on approximated fitness evaluations are also known as surrogate-assisted evolutionary approximation. Fitness approximation in evolutionary optimization can be seen as a sub-area of data-driven evolutionary optimization.",7838338687469606181,related_concept,Fitness function,2024-06-23 21:17:09.728,['Fitness approximation'],['Fitness approximation'],set(),set(),0,4.478260869565218,1.0481989072095297
1060,1949,Crossover (genetic algorithm),http://dbpedia.org/resource/Crossover_(genetic_algorithm),http://en.wikipedia.org/wiki/Crossover_(genetic_algorithm),"In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions are typically mutated before being added to the population. Different algorithms in evolutionary computation may use different data structures to store genetic information, and each genetic representation can be recombined with different crossover operators. Typical data structures that can be recombined with crossover are bit arrays, vectors of real numbers, or trees.",2711397148072320699,related_concept,Fitness function,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.3645833333333333,3.264292812988253e-304
1061,1950,Niche differentiation,http://dbpedia.org/resource/Niche_differentiation,http://en.wikipedia.org/wiki/Niche_differentiation,"In ecology, niche differentiation (also known as niche segregation, niche separation and niche partitioning) refers to the process by which competing species use the environment differently in a way that helps them to coexist. The competitive exclusion principle states that if two species with identical niches (ecological roles) compete, then one will inevitably drive the other to extinction. This rule also states that two species cannot occupy the same exact niche in a habitat and coexist together, at least in a stable manner. When two species differentiate their niches, they tend to compete less strongly, and are thus more likely to coexist. Species can differentiate their niches in many ways, such as by consuming different foods, or using different areas of the environment. As an example of niche partitioning, several anole lizards in the Caribbean islands share common diets—mainly insects. They avoid competition by occupying different physical locations. Although these lizards might occupy different locations, some species can be found inhabiting the same range, with up to 15 in certain areas. For example, some live on the ground while others are arboreal. Species who live in different areas compete less for food and other resources, which minimizes competition between species. However, species who live in similar areas typically compete with each other.",2992486111707275292,related_concept,Fitness function,2024-06-23 21:17:09.728,[],"['Statistics', 'Niche differentiation']",set(),set(),0,0.6655405405405406,2.4346332872793507e-304
1062,1951,Chromosome (genetic algorithm),http://dbpedia.org/resource/Chromosome_(genetic_algorithm),http://en.wikipedia.org/wiki/Chromosome_(genetic_algorithm),"In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used.",35627926342461339,related_concept,Fitness function,2024-06-23 21:17:09.728,[],"['Evolution', 'Algorithm']",set(),set(),0,1.5384615384615385,0.15258490802058275
1063,1952,Inferential programming,http://dbpedia.org/resource/Inferential_programming,http://en.wikipedia.org/wiki/Inferential_programming,"In ordinary computer programming, the programmer keeps the program's intended results in mind and painstakingly constructs a computer program to achieve those results. Inferential programming refers to (still mostly hypothetical) techniques and technologies enabling the inverse. Inferential programming would allow the programmer to describe the intended result to the computer using a metaphor such as a fitness function, a test specification, or a logical specification and then the computer would construct its own program to meet the supplied criteria. During the 1980s, approaches to achieve inferential programming mostly revolved around techniques for logical inference. Today the term is sometimes used in connection with evolutionary computation techniques that enable the computer to evolve a solution in response to a problem posed as a fitness or reward function.",3853697331528675675,related_concept,Fitness function,2024-06-23 21:17:09.728,['Inferential programming'],['Inferential programming'],set(),set(),0,5.045454545454546,1.1889451121495327
1064,1953,Fitness function,http://dbpedia.org/resource/Fitness_function,http://en.wikipedia.org/wiki/Fitness_function,"A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims. Fitness functions are used in genetic programming and genetic algorithms to guide simulations towards optimal design solutions.",6319959381167129140,main_concept,,2024-06-23 21:17:09.728,['Fitness function'],"['Fitness function', 'Interactive genetic algorithms', 'Fitness approximation']",set(),set(),0,4.473684210526316,0.7917582913166381
1065,1954,P-value,http://dbpedia.org/resource/P-value,http://en.wikipedia.org/wiki/P-value,"In null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis. Reporting p-values of statistical tests is common practice in academic publications of many quantitative fields. Since the precise meaning of p-value is hard to grasp, misuse is widespread and has been a major topic in metascience.",7189984969511040671,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Z-test', 'Bayes factor', ""Student's t-distribution"", ""Pearson's chi-squared test"", 'F-test', 'P-value', 'Statistical Methods for Research Workers', '68–95–99.7 rule', 'The Design of Experiments', 'Probability']",set(),set(),0,1.0894117647058823,5.355033648717948e-304
1066,1955,Bayes factor,http://dbpedia.org/resource/Bayes_factor,http://en.wikipedia.org/wiki/Bayes_factor,"The Bayes factor is a ratio of two competing statistical models represented by their marginal likelihood, and is used to quantify the support for one model over the other. The models in questions can have a common set of parameters, such as a null hypothesis and an alternative, but this is not necessary; for instance, it could also be a non-linear model compared to its linear approximation. The Bayes factor can be thought of as a Bayesian analog to the likelihood-ratio test, but since it uses the (integrated) marginal likelihood instead of the maximized likelihood, both tests only coincide under simple hypotheses (e.g., two specific parameter values). Also, in contrast with null hypothesis significance testing, Bayes factors support evaluation of evidence in favor of a null hypothesis, rather than only allowing the null to be rejected or not rejected. Although conceptually simple, the computation of the Bayes factor can be challenging depending on the complexity of the model and the hypotheses. Since closed-form expressions of the marginal likelihood are generally not available, numerical approximations based on MCMC samples have been suggested. For certain special cases, simplified algebraic expressions can be derived; for instance, the Savage–Dickey density ratio in the case of a precise (equality constrained) hypothesis against an unrestricted alternative. Another approximation, derived by applying Laplace's method to the integrated likelihoods, is known as the Bayesian information criterion (BIC); in large data sets the Bayes factor will approach the BIC as the influence of the priors wanes. In small data sets, priors generally matter and must not be improper since the Bayes factor will be undefined if either of the two integrals in its ratio is not finite.",7723601664412224037,related_concept,Null hypothesis,2024-06-23 21:17:09.728,['Bayes factor'],"['Bayes factor', ""Bayes' theorem"", 'Bayesian inference']",set(),set(),0,7.363636363636363,4.7257872823835556e-304
1067,1956,Model selection,http://dbpedia.org/resource/Model_selection,http://en.wikipedia.org/wiki/Model_selection,"Model selection is the task of selecting a statistical model from a set of candidate models, given data. In the simplest cases, a pre-existing set of data is considered. However, the task can also involve the design of experiments such that the data collected is well-suited to the problem of model selection. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice (Occam's razor). , p. 75) state, ""The majority of the problems in statistical inference can be considered to be problems related to statistical modeling"". Relatedly, , p. 197) has said, ""How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis"". Model selection may also refer to the problem of selecting a few representative models from a large set of computational models for the purpose of decision making or optimization under uncertainty.",3023496735575068183,main_concept,Null hypothesis,2024-06-23 21:17:09.728,['Model selection'],"['Model selection', 'Goodness of fit', 'Prediction', 'Akaike information criterion', 'Bayes factor']",set(),set(),0,1.505720823798627,1.389937887491284
1068,1957,Statistical significance,http://dbpedia.org/resource/Statistical_significance,http://en.wikipedia.org/wiki/Statistical_significance,"In statistical hypothesis testing, a result has statistical significance when it is very unlikely to have occurred given the null hypothesis (simply by chance alone). More precisely, a study's defined significance level, denoted by , is the probability of the study rejecting the null hypothesis, given that the null hypothesis is true; and the p-value of a result, , is the probability of obtaining a result at least as extreme, given that the null hypothesis is true. The result is statistically significant, by the standards of the study, when . The significance level for a study is chosen before data collection, and is typically set to 5% or much lower—depending on the field of study. In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. But if the p-value of an observed effect is less than (or equal to) the significance level, an investigator may conclude that the effect reflects the characteristics of the whole population, thereby rejecting the null hypothesis. This technique for testing the statistical significance of results was developed in the early 20th century. The term significance does not imply importance here, and the term statistical significance is not the same as research significance, theoretical significance, or practical significance. For example, the term clinical significance refers to the practical importance of a treatment effect.",979277990785669664,main_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Statistical significance', 'Statistical Methods for Research Workers', 'Inference', 'Effect size', 'Bayesian statistics', 'Bayes factor']",set(),set(),0,2.3474801061007957,5.330474384459885e-304
1069,1958,Bias (statistics),http://dbpedia.org/resource/Bias_(statistics),http://en.wikipedia.org/wiki/Bias_(statistics),"Statistical bias is a systematic tendency which causes differences between results and facts. The bias exists in numbers of the process of data analysis, including the source of the data, the estimator chosen, and the ways the data was analyzed. Bias may have a serious impact on results, for example, to investigate people's buying habits. If the sample size is not large enough, the results may not be representative of the buying habits of all the people. That is, there may be discrepancies between the survey results and the actual results. Therefore, understanding the source of statistical bias can help to assess whether the observed results are close to the real results. Bias can be differentiated from other mistakes such as accuracy (instrument failure/inadequacy), lack of data, or mistakes in transcription (typos). Bias implies that the data selection may have been skewed by the collection criteria. Bias does not preclude the existence of any other mistakes. One may have a poorly designed sample, an inaccurate measurement device, and typos in recording data simultaneously. Also it is useful to recognize that the term “error” specifically refers to the outcome rather than the process (errors of rejection or acceptance of the hypothesis being tested). Use of flaw or mistake to differentiate procedural errors from these specifically defined outcome-based terms is recommended.",746746960144737485,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Data', 'Type I and type II errors']",set(),set(),0,2.3987341772151898,6.021583060661542e-304
1070,1959,Homogeneity (statistics),http://dbpedia.org/resource/Homogeneity_(statistics),http://en.wikipedia.org/wiki/Homogeneity_(statistics),,4058036956833284194,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.9818181818181818,0.5219307681602459
1071,1960,Statistical inference,http://dbpedia.org/resource/Statistical_inference,http://en.wikipedia.org/wiki/Statistical_inference,"Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean ""make a prediction, by evaluating an already trained model""; in this context inferring properties of the model is referred to as training or learning (rather than inference), and using a model for prediction is referred to as inference (instead of prediction); see also predictive inference.",5088944654233735661,main_concept,Null hypothesis,2024-06-23 21:17:09.728,"['Descriptive statistics', 'Statistical inference']","['Statistical inference', 'Descriptive statistics', 'Kullback–Leibler divergence', 'Bayesian inference', 'Loss function', 'Likelihoodism', 'Akaike information criterion', 'AI', 'Kolmogorov complexity', 'Fiducial inference', 'Bayesian statistics']",set(),set(),0,2.0885122410546137,1.3645308600426698
1072,1961,Statistical model,http://dbpedia.org/resource/Statistical_model,http://en.wikipedia.org/wiki/Statistical_model,"A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process. A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. As such, a statistical model is ""a formal representation of a theory"" (Herman Adèr quoting Kenneth Bollen). All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.",1771914582617289706,main_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Statistical model', 'Parametric model', 'Akaike information criterion', 'Bayes factor']",set(),set(),0,3.0361757105943155,1.3388815248728823
1073,1962,Probability distribution,http://dbpedia.org/resource/Probability_distribution,http://en.wikipedia.org/wiki/Probability_distribution,"In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space). For instance, if X is used to denote the outcome of a coin toss (""the experiment""), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). Examples of random phenomena include the weather conditions at some future date, the height of a randomly selected person, the fraction of male students in a school, the results of a survey to be conducted, etc.",3783667172820842842,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Probability', 'Probability distribution', 'Poisson distribution', 'Bernoulli distribution']",set(),set(),0,3.2441520467836256,0.9186534766252441
1074,1963,Statistical population,http://dbpedia.org/resource/Statistical_population,http://en.wikipedia.org/wiki/Statistical_population,"In statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all possible hands in a game of poker). A common aim of statistical analysis is to produce information about some chosen population. In statistical inference, a subset of the population (a statistical sample) is chosen to represent the population in a statistical analysis. Moreover, the statistical sample must be unbiased and accurately model the population (every unit of the population has an equal chance of selection). The ratio of the size of this statistical sample to the size of the population is called a sampling fraction. It is then possible to estimate the population parameters using the appropriate sample statistics.",7563791708494084033,main_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Cauchy distribution', 'Descriptive statistics']",set(),set(),0,2.131195335276968,6.144946857819388e-304
1075,1964,The Design of Experiments,http://dbpedia.org/resource/The_Design_of_Experiments,http://en.wikipedia.org/wiki/The_Design_of_Experiments,"The Design of Experiments is a 1935 book by the English statistician Ronald Fisher about the design of experiments and is considered a foundational work in experimental design. Among other contributions, the book introduced the concept of the null hypothesis in the context of the lady tasting tea experiment. A chapter is devoted to the Latin square.",7290576308136457281,related_concept,Null hypothesis,2024-06-23 21:17:09.728,['The Design of Experiments'],['The Design of Experiments'],set(),set(),0,1.9473684210526316,1.3609374344251646
1076,1965,Hypothesis,http://dbpedia.org/resource/Hypothesis,http://en.wikipedia.org/wiki/Hypothesis,"A hypothesis (plural hypotheses) is a proposed explanation for a phenomenon. For a hypothesis to be a scientific hypothesis, the scientific method requires that one can test it. Scientists generally base scientific hypotheses on previous observations that cannot satisfactorily be explained with the available scientific theories. Even though the words ""hypothesis"" and ""theory"" are often used interchangeably, a scientific hypothesis is not the same as a scientific theory. A working hypothesis is a provisionally accepted hypothesis proposed for further research in a process beginning with an educated guess or thought. A different meaning of the term hypothesis is used in formal logic, to denote the antecedent of a proposition; thus in the proposition ""If P, then Q"", P denotes the hypothesis (or antecedent); Q can be called a consequent. P is the assumption in a (possibly counterfactual) What If question. The adjective hypothetical, meaning ""having the nature of a hypothesis"", or ""being assumed to exist as an immediate consequence of a hypothesis"", can refer to any of these meanings of the term ""hypothesis"".",4209469493963369303,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],['Hypothesis'],set(),set(),0,15.314049586776859,5.302879876092094e-304
1077,1966,Alternative hypothesis,http://dbpedia.org/resource/Alternative_hypothesis,http://en.wikipedia.org/wiki/Alternative_hypothesis,"In statistical hypothesis testing, the alternative hypothesis is one of the proposed proposition in the hypothesis test. In general the goal of hypothesis test is to demonstrate that in the given condition, there is sufficient evidence supporting the credibility of alternative hypothesis instead of the exclusive proposition in the test (null hypothesis). It is usually consistent with the research hypothesis because it is constructed from literature review, previous studies, etc. However, the research hypothesis is sometimes consistent with the null hypothesis. In statistics, alternative hypothesis is often denoted as Ha or H1. Hypotheses are formulated to compare in a statistical hypothesis test. In the domain of inferential statistics two rival hypotheses can be compared by explanatory power and predictive power.",1074949315077692488,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Null hypothesis', 'Alternative hypothesis', 'Neyman–Pearson lemma']",set(),set(),0,0.45294117647058824,5.1671253834996635e-304
1078,1967,Estimation statistics,http://dbpedia.org/resource/Estimation_statistics,http://en.wikipedia.org/wiki/Estimation_statistics,"Estimation statistics, or simply estimation, is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It complements hypothesis testing approaches such as null hypothesis significance testing (NHST), by going beyond the question is an effect present or not, and provides information about how large an effect is. Estimation statistics is sometimes referred to as the new statistics. The primary aim of estimation methods is to report an effect size (a point estimate) along with its confidence interval, the latter of which is related to the precision of the estimate. The confidence interval summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a P value as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals, and believe that estimation should replace significance testing for data analysis.",6780869798496419218,related_concept,Null hypothesis,2024-06-23 21:17:09.728,['Estimation statistics'],"['Estimation statistics', ""Student's t-test"", 'Confidence interval']",set(),set(),0,0.1346704871060172,1.114805190623832
1079,1968,Testing hypotheses suggested by the data,http://dbpedia.org/resource/Testing_hypotheses_suggested_by_the_data,http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data,"In statistics, hypotheses suggested by a given dataset, when tested with the same dataset that suggested them, are likely to be accepted even when they are not true. This is because circular reasoning (double dipping) would be involved: something seems true in the limited data set; therefore we hypothesize that it is true in general; therefore we wrongly test it on the same, limited data set, which seems to confirm that it is true. Generating hypotheses based on data already observed, in the absence of testing them on new data, is referred to as post hoc theorizing (from Latin post hoc, ""after this""). The correct procedure is to test any hypothesis on a data set that was not used to generate the hypothesis.",7388594756976232308,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.380952380952381,5.074821312580685e-304
1080,1969,Likelihood-ratio test,http://dbpedia.org/resource/Likelihood-ratio_test,http://en.wikipedia.org/wiki/Likelihood-ratio_test,"In statistics, the likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint. If the constraint (i.e., the null hypothesis) is supported by the observed data, the two likelihoods should not differ by more than sampling error. Thus the likelihood-ratio test tests whether this ratio is significantly different from one, or equivalently whether its natural logarithm is significantly different from zero. The likelihood-ratio test, also known as Wilks test, is the oldest of the three classical approaches to hypothesis testing, together with the Lagrange multiplier test and the Wald test. In fact, the latter two can be conceptualized as approximations to the likelihood-ratio test, and are asymptotically equivalent. In the case of comparing two models each of which has no unknown parameters, use of the likelihood-ratio test can be justified by the Neyman–Pearson lemma. The lemma demonstrates that the test has the highest power among all competitors.",3488841616411477052,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['Neyman–Pearson lemma', 'Z-test', ""Pearson's chi-squared test"", 'F-test']",set(),set(),0,1.7459893048128343,1.2755356032365972
1081,1970,Statistical hypothesis testing,http://dbpedia.org/resource/Statistical_hypothesis_testing,http://en.wikipedia.org/wiki/Statistical_hypothesis_testing,A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.Hypothesis testing allows us to make probabilistic statements about population parameters.,7490937725218078829,main_concept,Null hypothesis,2024-06-23 21:17:09.728,['Hypothesis'],"[""Student's t-distribution"", ""Pearson's chi-squared test"", 'Hypothesis', ""Fisher's method"", 'Statistics', 'Statistical hypothesis testing', 'Boot', 'Bayesian inference', 'Neyman–Pearson lemma', 'Estimation statistics', 'Bayes factor']",set(),set(),0,2.048739495798319,1.149931689031877
1082,1971,One-tailed test,http://dbpedia.org/resource/One-tailed_test,http://en.wikipedia.org/wiki/One-tailed_test,,1415183819875194013,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],"['One-tailed test', ""Pearson's chi-squared test"", 'Statistical Methods for Research Workers', 'The Design of Experiments', 'Z-test', ""Student's t-distribution""]",set(),set(),0,0.05014749262536873,0.5372085729058892
1083,1972,Test statistic,http://dbpedia.org/resource/Test_statistic,http://en.wikipedia.org/wiki/Test_statistic,"A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing. A hypothesis test is typically specified in terms of a test statistic, considered as a numerical summary of a data-set that reduces the data to one value that can be used to perform the hypothesis test. In general, a test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis, where such an alternative is prescribed, or that would characterize the null hypothesis if there is no explicitly stated alternative hypothesis. An important property of a test statistic is that its sampling distribution under the null hypothesis must be calculable, either exactly or approximately, which allows p-values to be calculated. A test statistic shares some of the same qualities of a descriptive statistic, and many statistics can be used as both test statistics and descriptive statistics. However, a test statistic is specifically intended for use in statistical testing, whereas the main quality of a descriptive statistic is that it is easily interpretable. Some informative descriptive statistics, such as the sample range, do not make good test statistics since it is difficult to determine their sampling distribution. Two widely used test statistics are the t-statistic and the F-test.",5028787152848045953,related_concept,Null hypothesis,2024-06-23 21:17:09.728,['F-test'],"['Test statistic', 'F-test', 'Z-test', 'ANOVA']",set(),set(),0,3.1875,1.097573632876372
1084,1973,Counternull,http://dbpedia.org/resource/Counternull,http://en.wikipedia.org/wiki/Counternull,"In statistics, and especially in the statistical analysis of psychological data, the counternull is a statistic used to aid the understanding and presentation of research results. It revolves around the effect size, which is the mean magnitude of some effect divided by the standard deviation. The counternull value is the effect size that is just as well supported by the data as the null hypothesis. In particular, when results are drawn from a distribution that is symmetrical about its mean, the counternull value is exactly twice the observed effect size. The null hypothesis is a hypothesis set up to be tested against an alternative. Thus the counternull is an alternative hypothesis that, when used to replace the null hypothesis, generates the same p-value as had the original null hypothesis of “no difference.” Some researchers contend that reporting the counternull, in addition to the p-value, serves to counter two common errors of judgment: 
* assuming that failure to reject the null hypothesis at the chosen level of statistical significance means that the observed size of the ""effect"" is zero; and 
* assuming that rejection of the null hypothesis at a particular p-value means that the measured ""effect"" is not only statistically significant, but also scientifically important. These arbitrary statistical thresholds create a discontinuity, causing unnecessary confusion and artificial controversy. Other researchers prefer confidence intervals as a means of countering these common errors.",5029979412674594804,related_concept,Null hypothesis,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.25,5.333004135789711e-304
1085,1974,Null hypothesis,http://dbpedia.org/resource/Null_hypothesis,http://en.wikipedia.org/wiki/Null_hypothesis,"In inferential statistics, the null hypothesis (often denoted H0) is that two possibilities are the same. The null hypothesis is that the observed difference is due to chance alone. Using statistical tests, it is possible to calculate the likelihood that the null hypothesis is true.",4358042244473357566,main_concept,,2024-06-23 21:17:09.728,[],"['Akaike information criterion', 'Bayes factor', 'Statistical inference', 'Hypothesis', 'Testing hypotheses suggested by the data', 'One-tailed test', 'Statistical significance']",set(),set(),0,1.8659217877094971,5.1818090000960684e-304
1086,1975,LOBPCG,http://dbpedia.org/resource/LOBPCG,http://en.wikipedia.org/wiki/LOBPCG,"Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is a matrix-free method for finding the largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric generalized eigenvalue problem for a given pair of complex Hermitian or real symmetric matrices, wherethe matrix is also assumed positive-definite.",3831194796115556320,related_concept,Spectral clustering,2024-06-23 21:17:09.728,"['Gradient', 'LOBPCG']","['Gradient', 'LOBPCG', 'MATLAB', 'Laplacian matrix', 'Discretization', 'Iterative']",set(),set(),0,0.2125,4.5879782406847924e-304
1087,1976,Segmentation-based object categorization,http://dbpedia.org/resource/Segmentation-based_object_categorization,http://en.wikipedia.org/wiki/Segmentation-based_object_categorization,The image segmentation problem is concerned with partitioning an image into multiple regions according to some homogeneity criterion. This article is primarily concerned with graph theoretic approaches to image segmentation applying graph partitioning via minimum cut or maximum cut. Segmentation-based object categorization can be viewed as a specific case of spectral clustering applied to image segmentation.,3697571315911869271,related_concept,Spectral clustering,2024-06-23 21:17:09.728,['Segmentation-based object categorization'],"['Segmentation-based object categorization', 'LOBPCG']",set(),set(),0,0.9629629629629629,1.3377085665160253
1088,1978,Dimensionality reduction,http://dbpedia.org/resource/Dimensionality_reduction,http://en.wikipedia.org/wiki/Dimensionality_reduction,"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.",3230818864444843838,main_concept,Spectral clustering,2024-06-23 21:17:09.728,"['Dimension', 'Dimensionality reduction']","['Dimension', 'Dimensionality reduction', 'Feature selection', 'Data', 'Data analysis', 'Principal component analysis', 'Stochastic', 'K-nearest neighbor', 'Feature extraction']",set(),set(),0,3.8333333333333335,0.9413406742832986
1089,1979,Sparse matrix,http://dbpedia.org/resource/Sparse_matrix,http://en.wikipedia.org/wiki/Sparse_matrix,"In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix. Conceptually, sparsity corresponds to systems with few pairwise interactions. For example, consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls were to have springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory and numerical analysis, which typically have a low density of significant data or connections. Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations. When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Specialized computers have been made for sparse matrices, as they are common in the machine learning field. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeros. Sparse data is by nature more easily compressed and thus requires significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.",5644194356362865875,related_concept,Spectral clustering,2024-06-23 21:17:09.728,[],"['Iterative', 'Iterative method', 'MATLAB']",set(),set(),0,2.1270903010033444,4.248334504040549e-304
1090,1980,Preconditioner,http://dbpedia.org/resource/Preconditioner,http://en.wikipedia.org/wiki/Preconditioner,"In mathematics, preconditioning is the application of a transformation, called the preconditioner, that conditions a given problem into a form that is more suitable for numerical solving methods. Preconditioning is typically related to reducing a condition number of the problem. The preconditioned problem is then usually solved by an iterative method.",89566620008083800,related_concept,Spectral clustering,2024-06-23 21:17:09.728,[],"['Iterative', 'Preconditioner', 'Iterative method']",set(),set(),0,0.7777777777777778,1.3730024913370655
1091,1981,Nearest neighbor search,http://dbpedia.org/resource/Nearest_neighbor_search,http://en.wikipedia.org/wiki/Nearest_neighbor_search,"Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points. Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.",2158922701753342556,related_concept,Spectral clustering,2024-06-23 21:17:09.728,"['Manhattan distance', 'Nearest neighbor search', 'Euclidean distance']","['Nearest neighbor search', 'Manhattan distance', 'Euclidean distance', 'R* tree', 'Algorithm', 'Fixed-radius near neighbors']",set(),set(),0,2.2695652173913046,1.3909441207156077
1092,1982,Similarity matrix,http://dbpedia.org/resource/Similarity_matrix,http://en.wikipedia.org/wiki/Similarity_matrix,,6877705694361765754,related_concept,Spectral clustering,2024-06-23 21:17:09.728,[],"['Chebyshev distance', 'Manhattan distance', 'Minkowski distance', 'Euclidean distance', 'Cluster analysis', 'K-means clustering', 'Hierarchical clustering']",set(),set(),0,1.3333333333333333,0.3118760520901068
1093,1983,DBSCAN,http://dbpedia.org/resource/DBSCAN,http://en.wikipedia.org/wiki/DBSCAN,"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature. In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, ACM SIGKDD. As of July 2020, the follow-up paper ""DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN"" appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems (TODS) journal.",8098996346828720216,main_concept,Spectral clustering,2024-06-23 21:17:09.728,"['Database', 'Data', 'Hans-Peter Kriegel', 'DBSCAN']","['Hans-Peter Kriegel', 'DBSCAN', 'Database', 'Data', 'OPTICS algorithm', 'SUBCLU']",set(),set(),0,1.9190751445086704,1.1681013697534235
1094,1984,Multivariate statistics,http://dbpedia.org/resource/Multivariate_statistics,http://en.wikipedia.org/wiki/Multivariate_statistics,"Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. Multivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied. In addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both 
* how these can be used to represent the distributions of observed data; 
* how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis. Certain types of problems involving multivariate data, for example simple linear regression and multiple regression, are not usually considered to be special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.",1077832314720945936,main_concept,Spectral clustering,2024-06-23 21:17:09.728,['Multivariate statistics'],"['Multivariate statistics', 'Wishart distribution', 'Bayesian multivariate linear regression', ""Student's t-distribution"", 'Bayesian inference']",set(),set(),0,2.048223350253807,5.9211774893791796e-304
1095,1985,Affinity propagation,http://dbpedia.org/resource/Affinity_propagation,http://en.wikipedia.org/wiki/Affinity_propagation,"In statistics and data mining, affinity propagation (AP) is a clustering algorithm based on the concept of ""message passing"" between data points.Unlike clustering algorithms such as k-means or k-medoids, affinity propagation does not require the number of clusters to be determined or estimated before running the algorithm. Similar to k-medoids, affinity propagation finds ""exemplars,"" members of the input set that are representative of clusters.",7339400578436370012,related_concept,Spectral clustering,2024-06-23 21:17:09.728,[],[],set(),set(),0,1.6666666666666667,3.2942882908799273e-304
1096,1986,Power iteration,http://dbpedia.org/resource/Power_iteration,http://en.wikipedia.org/wiki/Power_iteration,"In mathematics, power iteration (also known as the power method) is an eigenvalue algorithm: given a diagonalizable matrix , the algorithm will produce a number , which is the greatest (in absolute value) eigenvalue of , and a nonzero vector , which is a corresponding eigenvector of , that is, .The algorithm is also known as the Von Mises iteration. Power iteration is a very simple algorithm, but it may converge slowly. The most time-consuming operation of the algorithm is the multiplication of matrix by a vector, so it is effective for a very large sparse matrix with appropriate implementation.",1114684604451999206,related_concept,Spectral clustering,2024-06-23 21:17:09.728,['Power iteration'],"['Power iteration', 'PageRank', 'LOBPCG']",set(),set(),0,1.0535714285714286,1.317673793128857
1097,1988,Nonlinear dimensionality reduction,http://dbpedia.org/resource/Nonlinear_dimensionality_reduction,http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction,"Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself. The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.",2195975232954879337,related_concept,Spectral clustering,2024-06-23 21:17:09.728,['Nonlinear dimensionality reduction'],"['Nonlinear dimensionality reduction', 'Algorithm', 'Laplacian matrix', 'Euclidean distance', 'Variance', 'Dimension', 'Diffusion map', 'Data']",set(),set(),0,1.4074074074074074,4.2176080998660806e-304
1098,1989,Kernel principal component analysis,http://dbpedia.org/resource/Kernel_principal_component_analysis,http://en.wikipedia.org/wiki/Kernel_principal_component_analysis,"In the field of multivariate statistics, kernel principal component analysis (kernel PCA)is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.",9211458091504963529,related_concept,Spectral clustering,2024-06-23 21:17:09.728,[],"['Covariance matrix', 'Covariance']",set(),set(),0,4.2727272727272725,4.189191234856091e-304
1099,1990,Fiedler vector,http://dbpedia.org/resource/Fiedler_vector,http://en.wikipedia.org/wiki/Fiedler_vector,,3477631616883323234,related_concept,Spectral clustering,2024-06-23 21:17:09.728,[],"['Laplacian matrix', 'Fiedler vector']",set(),set(),0,0.21212121212121213,0.3550391521465189
1100,1991,Spectral graph theory,http://dbpedia.org/resource/Spectral_graph_theory,http://en.wikipedia.org/wiki/Spectral_graph_theory,"In mathematics, spectral graph theory is the study of the properties of a graph in relationship to the characteristic polynomial, eigenvalues, and eigenvectors of matrices associated with the graph, such as its adjacency matrix or Laplacian matrix. The adjacency matrix of a simple undirected graph is a real symmetric matrix and is therefore orthogonally diagonalizable; its eigenvalues are real algebraic integers. While the adjacency matrix depends on the vertex labeling, its spectrum is a graph invariant, although not a complete one. Spectral graph theory is also concerned with graph parameters that are defined via multiplicities of eigenvalues of matrices associated to the graph, such as the Colin de Verdière number.",6657897356703051862,related_concept,Spectral clustering,2024-06-23 21:17:09.728,"['Spectral graph theory', 'Laplacian matrix']","['Laplacian matrix', 'Spectral graph theory']",set(),set(),0,1.7875,1.1219472159649126
1101,1992,Spectrum of a matrix,http://dbpedia.org/resource/Spectrum_of_a_matrix,http://en.wikipedia.org/wiki/Spectrum_of_a_matrix,"In mathematics, the spectrum of a matrix is the set of its eigenvalues. More generally, if is a linear operator on any finite-dimensional vector space, its spectrum is the set of scalars such that is not invertible. The determinant of the matrix equals the product of its eigenvalues. Similarly, the trace of the matrix equals the sum of its eigenvalues.From this point of view, we can define the pseudo-determinant for a singular matrix to be the product of its nonzero eigenvalues (the density of multivariate normal distribution will need this quantity). In many applications, such as PageRank, one is interested in the dominant eigenvalue, i.e. that which is largest in absolute value. In other applications, the smallest eigenvalue is important, but in general, the whole spectrum provides valuable information about a matrix.",5562851622438889405,related_concept,Spectral clustering,2024-06-23 21:17:09.728,['PageRank'],['PageRank'],set(),set(),0,1.3658536585365855,1.003896787442022
1102,1994,Laplacian matrix,http://dbpedia.org/resource/Laplacian_matrix,http://en.wikipedia.org/wiki/Laplacian_matrix,"In the mathematical field of graph theory, the Laplacian matrix, also called the graph Laplacian, admittance matrix, Kirchhoff matrix or discrete Laplacian, is a matrix representation of a graph. Named after Pierre-Simon Laplace, the graph Laplacian matrix can be viewed as a matrix form of the negative discrete Laplace operator on a graph approximating the negative continuous Laplacian obtained by the finite difference method. The Laplacian matrix relates to many useful properties of a graph. Together with Kirchhoff's theorem, it can be used to calculate the number of spanning trees for a given graph. The sparsest cut of a graph can be approximated through the Fiedler vector — the eigenvector corresponding to the second smallest eigenvalue of the graph Laplacian — as established by Cheeger's inequality. The spectral decomposition of the Laplacian matrix allows constructing low dimensional embeddings that appear in many machine learning applications and determines a spectral layout in graph drawing. Graph-based signal processing is based on the graph Fourier transform that extends the traditional discrete Fourier transform by substituting the standard basis of complex sinusoids for eigenvectors of the Laplacian matrix of a graph corresponding to the signal. The Laplacian matrix is the easiest to define for a simple graph, but more common in applications for an edge-weighted graph, i.e., with weights on its edges — the entries of the graph adjacency matrix. Spectral graph theory relates properties of a graph to a spectrum, i.e., eigenvalues, and eigenvectors of matrices associated with the graph, such as its adjacency matrix or Laplacian matrix. Imbalanced weights may undesirably affect the matrix spectrum, leading to the need of normalization — a column/row scaling of the matrix entries — resulting in normalized adjacency and Laplacian matrices.",6849555586572089561,related_concept,Spectral clustering,2024-06-23 21:17:09.728,"['Fiedler vector', 'Spectral graph theory', 'Laplacian matrix']","['Laplacian matrix', 'Fiedler vector', 'Spectral graph theory']",set(),set(),0,1.125,1.2211962350449501
1103,1995,Spectral clustering,http://dbpedia.org/resource/Spectral_clustering,http://en.wikipedia.org/wiki/Spectral_clustering,"In multivariate statistics, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset. In application to image segmentation, spectral clustering is known as segmentation-based object categorization.",1698427369543441495,main_concept,,2024-06-23 21:17:09.728,[],"['Laplacian matrix', 'Spectral clustering', 'Fiedler vector', 'DBSCAN', 'LOBPCG', 'Algorithm']",set(),set(),0,2.3207547169811322,1.3482509052051466
1104,1996,Farkas's lemma,http://dbpedia.org/resource/Farkas's_lemma,http://en.wikipedia.org/wiki/Farkas's_lemma,,8447180717960288347,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Hyperplane', 'Hyperplane separation theorem', ""Farkas's lemma""]",set(),set(),0,0.08,0.49966288224735395
1105,1997,Theorem,http://dbpedia.org/resource/Theorem,http://en.wikipedia.org/wiki/Theorem,"In mathematics, a theorem is a statement that has been proved, or can be proved. The proof of a theorem is a logical argument that uses the inference rules of a deductive system to establish that the theorem is a logical consequence of the axioms and previously proved theorems. In the mainstream of mathematics, the axioms and the inference rules are commonly left implicit, and, in this case, they are almost always those of Zermelo–Fraenkel set theory with the axiom of choice, or of a less powerful theory, such as Peano arithmetic. A notable exception is Wiles's proof of Fermat's Last Theorem, which involves the Grothendieck universes whose existence requires the addition of a new axiom to the set theory. Generally, an assertion that is explicitly called a theorem is a proved result that is not an immediate consequence of other known theorems. Moreover, many authors qualify as theorems only the most important results, and use the terms lemma, proposition and corollary for less important theorems. In mathematical logic, the concepts of theorems and proofs have been formalized in order to allow mathematical reasoning about them. In this context, statements become well-formed formulas of some formal language. A theory consists of some basis statements called axioms, and some deducing rules (sometimes included in the axioms). The theorems of the theory are the statements that can be derived from the axioms by using the deducing rules. This formalization led to proof theory, which allows proving general theorems about theorems and proofs. In particular, Gödel's incompleteness theorems show that every consistent theory containing the natural numbers has true statements on natural numbers that are not theorems of the theory (that is they cannot be proved inside the theory). As the axioms are often abstractions of properties of the physical world, theorems may be considered as expressing some truth, but in contrast to the notion of a scientific law, which is experimental, the justification of the truth of a theorem is purely deductive.",8945671561430701599,main_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,"['Peano arithmetic', 'Theorem']","['Peano arithmetic', ""Gödel's incompleteness theorems"", 'Euclidean geometry', 'Theorem', 'Logically']",set(),set(),0,2.807142857142857,3.645261363556463e-304
1106,1998,Convex geometry,http://dbpedia.org/resource/Convex_geometry,http://en.wikipedia.org/wiki/Convex_geometry,"In mathematics, convex geometry is the branch of geometry studying convex sets, mainly in Euclidean space. Convex sets occur naturally in many areas: computational geometry, convex analysis, discrete geometry, functional analysis, geometry of numbers, integral geometry, linear programming, probability theory, game theory, etc.",6470908994962049995,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,['Convex set'],"['Convex set', 'Classification', 'Mathematics', 'Geometry', 'Hermann Minkowski', 'Convex geometry']",set(),set(),0,8.129032258064516,0.640269544938613
1107,1999,Dual cone,http://dbpedia.org/resource/Dual_cone,http://en.wikipedia.org/wiki/Dual_cone,,7097554716792443455,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],['Dual cone'],set(),set(),0,0.328125,0.27449516334900775
1108,2000,Cauchy sequence,http://dbpedia.org/resource/Cauchy_sequence,http://en.wikipedia.org/wiki/Cauchy_sequence,"In mathematics, a Cauchy sequence (French pronunciation: ​[koʃi]; English: /ˈkoʊʃiː/ KOH-shee), named after Augustin-Louis Cauchy, is a sequence whose elements become arbitrarily close to each other as the sequence progresses. More precisely, given any small positive distance, all but a finite number of elements of the sequence are less than that given distance from each other. It is not sufficient for each term to become arbitrarily close to the preceding term. For instance, in the sequence of square roots of natural numbers: the consecutive terms become arbitrarily close to each other:However, with growing values of the index n, the terms become arbitrarily large. So, for any index n and distance d, there exists an index m big enough such that (Actually, any suffices.) As a result, despite how far one goes, the remaining terms of the sequence never get close to each other; hence the sequence is not Cauchy. The utility of Cauchy sequences lies in the fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. This is often exploited in algorithms, both theoretical and applied, where an iterative process can be shown relatively easily to produce a Cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination. Generalizations of Cauchy sequences in more abstract uniform spaces exist in the form of Cauchy filters and Cauchy nets.",3512989004628992807,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,['Cauchy sequence'],['Cauchy sequence'],set(),set(),0,1.934640522875817,1.205094375731219
1109,2001,Disjoint sets,http://dbpedia.org/resource/Disjoint_sets,http://en.wikipedia.org/wiki/Disjoint_sets,"In mathematics, two sets are said to be disjoint sets if they have no element in common. Equivalently, two disjoint sets are sets whose intersection is the empty set. For example, {1, 2, 3} and {4, 5, 6} are disjoint sets, while {1, 2, 3} and {3, 4, 5} are not disjoint. A collection of two or more sets is called disjoint if any two distinct sets of the collection are disjoint.",4333786470106487001,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],[],set(),set(),0,6.117647058823529,1.4103888929845942
1110,2002,Compact set,http://dbpedia.org/resource/Compact_set,http://en.wikipedia.org/wiki/Compact_set,,96119315111998580,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.6165413533834586,0.37531514751124506
1111,2003,Affine subspace,http://dbpedia.org/resource/Affine_subspace,http://en.wikipedia.org/wiki/Affine_subspace,,1903207432017319202,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Linear subspace', 'Axiom', 'Euclidean distance', 'Euclidean geometry', 'Euclidean plane']",set(),set(),0,0.195,0.1971129052960008
1112,2004,Projection (set theory),http://dbpedia.org/resource/Projection_(set_theory),http://en.wikipedia.org/wiki/Projection_(set_theory),"In set theory, a projection is one of two closely related types of functions or operations, namely: 
* A set-theoretic operation typified by the jth projection map, written , that takes an element of the Cartesian product to the value . 
* A function that sends an element x to its equivalence class under a specified equivalence relation E, or, equivalently, a surjection from a set to another set. The function from elements to equivalence classes is a surjection, and every surjection corresponds to an equivalence relation under which two elements are equivalent when they have the same image. The result of the mapping is written as [x] when E is understood, or written as [x]E when it is necessary to make E explicit.",6097985362094611891,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],[],set(),set(),0,2.764705882352941,1.4101488256677226
1113,2005,Supporting hyperplane theorem,http://dbpedia.org/resource/Supporting_hyperplane_theorem,http://en.wikipedia.org/wiki/Supporting_hyperplane_theorem,,3193791831953201770,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],[],set(),set(),0,0.3157894736842105,0.29428868138423686
1114,2006,Convex set,http://dbpedia.org/resource/Convex_set,http://en.wikipedia.org/wiki/Convex_set,"In geometry, a subset of a Euclidean space, or more generally an affine space over the reals, is convex if, given any two points in the subset, the subset contains the whole line segment that joins them. Equivalently, a convex set or a convex region is a subset that intersects every line into a single line segment (possibly empty).For example, a solid cube is a convex set, but anything that is hollow or has an indent, for example, a crescent shape, is not convex. The boundary of a convex set is always a convex curve. The intersection of all the convex sets that contain a given subset A of Euclidean space is called the convex hull of A. It is the smallest convex set containing A. A convex function is a real-valued function defined on an interval with the property that its epigraph (the set of points on or above the graph of the function) is a convex set. Convex minimization is a subfield of optimization that studies the problem of minimizing convex functions over convex sets. The branch of mathematics devoted to the study of properties of convex sets and convex functions is called convex analysis. The notion of a convex set can be generalized as described below.",7314868068133274069,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Euclidean plane', 'Hahn–Banach theorem', 'Minkowski sum', 'Theorem']",set(),set(),0,3.5411255411255413,1.1686757526232554
1115,2007,Kirchberger's theorem,http://dbpedia.org/resource/Kirchberger's_theorem,http://en.wikipedia.org/wiki/Kirchberger's_theorem,"Kirchberger's theorem is a theorem in discrete geometry, on linear separability. The two-dimensional version of the theorem states that, if a finite set of red and blue points in the Euclidean plane has the property that, for every four points, there exists a line separating the red and blue points within those four, then there exists a single line separating all the red points from all the blue points. Donald Watson phrases this result more colorfully, with a farmyard analogy: If sheep and goats are grazing in a field and for every four animals there exists a line separating the sheep from the goats then there exists such a line for all the animals. More generally, for finitely many red and blue points in -dimensional Euclidean space, if the red and blue points in every subset of of the points are linearly separable, then all the red points and all the blue points are linearly separable. Another equivalent way of stating the result is that, if the convex hulls of finitely many red and blue points have a nonempty intersection, then there exists a subset of points for which the convex hulls of the red and blue points in the subsets also intersect.",5852687337117738042,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,"['Euclidean plane', ""Kirchberger's theorem""]","['Euclidean plane', ""Kirchberger's theorem"", 'Hermann Minkowski']",set(),set(),0,0.3103448275862069,1.2668151939126144
1116,2008,Hahn–Banach theorem,http://dbpedia.org/resource/Hahn–Banach_theorem,http://en.wikipedia.org/wiki/Hahn–Banach_theorem,"The Hahn–Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are ""enough"" continuous linear functionals defined on every normed vector space to make the study of the dual space ""interesting"". Another version of the Hahn–Banach theorem is known as the Hahn–Banach separation theorem or the hyperplane separation theorem, and has numerous uses in convex geometry.",4640354756256853339,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Hahn–Banach theorem', 'Theorem', 'F-space']",set(),set(),0,1.4149484536082475,1.2022700749039357
1117,2009,Geometry,http://dbpedia.org/resource/Geometry,http://en.wikipedia.org/wiki/Geometry,"Geometry (from Ancient Greek γεωμετρία (geōmetría) 'land measurement'; from γῆ (gê) 'earth, land', and μέτρον (métron) 'a measure') is, with arithmetic, one of the oldest branches of mathematics. It is concerned with properties of space such as the distance, shape, size, and relative position of figures. A mathematician who works in the field of geometry is called a geometer. Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry, which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts. During the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Gauss' Theorema Egregium (""remarkable theorem"") that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in a Euclidean space. This implies that surfaces can be studied intrinsically, that is, as stand-alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry. Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry. Since then, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that omits continuity, and others. Originally developed to model the physical world, geometry has applications in almost all sciences, and also in art, architecture, and other activities that are related to graphics. Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental in Wiles's proof of Fermat's Last Theorem, a problem that was stated in terms of elementary arithmetic, and remained unsolved for several centuries.",6120024503375248460,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,"['Euclidean geometry', 'Theorem', 'Geometry']","['Euclidean geometry', 'Geometry', 'Theorem', 'Projective geometry', 'Euclidean plane', 'Computation', 'Convex geometry', 'Mathematics', 'Calculus']",{'Geometry'},set(),0,9.252854812398043,0.6242611403585009
1118,2010,Minkowski sum,http://dbpedia.org/resource/Minkowski_sum,http://en.wikipedia.org/wiki/Minkowski_sum,,838107527062694668,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Minkowski sum', 'Hermann Minkowski']",set(),set(),0,0.2671232876712329,0.2907397174705236
1119,2011,Topological vector spaces,http://dbpedia.org/resource/Topological_vector_spaces,http://en.wikipedia.org/wiki/Topological_vector_spaces,,1166320812035815107,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Theorem', 'Cauchy sequence']",set(),set(),0,0.08169014084507042,0.4376309095784381
1120,2012,Collision detection,http://dbpedia.org/resource/Collision_detection,http://en.wikipedia.org/wiki/Collision_detection,"Collision detection is the computational problem of detecting the intersection of two or more objects. Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. Collision detection algorithms can be divided into operating on 2D and 3D objects.",3800909191488834816,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,['Collision detection'],"['Collision detection', ""Newton's method"", 'Algorithm', 'AI']",set(),set(),0,4.228571428571429,1.4257811615794067
1121,2013,Convex hull,http://dbpedia.org/resource/Convex_hull,http://en.wikipedia.org/wiki/Convex_hull,"In geometry, the convex hull or convex envelope or convex closure of a shape is the smallest convex set that contains it. The convex hull may be defined either as the intersection of all convex sets containing a given subset of a Euclidean space, or equivalently as the set of all convex combinations of points in the subset. For a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around the subset. Convex hulls of open sets are open, and convex hulls of compact sets are compact. Every compact convex set is the convex hull of its extreme points. The convex hull operator is an example of a closure operator, and every antimatroid can be represented by applying this closure operator to finite sets of points.The algorithmic problems of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces, and its dual problem of intersecting half-spaces, are fundamental problems of computational geometry. They can be solved in time for two or three dimensional point sets, and in time matching the worst-case output complexity given by the upper bound theorem in higher dimensions. As well as for finite point sets, convex hulls have also been studied for simple polygons, Brownian motion, space curves, and epigraphs of functions. Convex hulls have wide applications in mathematics, statistics, combinatorial optimization, economics, geometric modeling, and ethology. Related structures include the orthogonal convex hull, convex layers, Delaunay triangulation and Voronoi diagram, and convex skull.",1351087460364140577,related_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,['Convex hull'],"['Convex hull', 'Euclidean plane', 'Euclidean geometry', 'Minkowski sum', 'Outlier']",set(),set(),0,2.536741214057508,1.188106061805293
1122,2014,Support-vector machine,http://dbpedia.org/resource/Support-vector_machine,http://en.wikipedia.org/wiki/Support-vector_machine,,2434547643173152745,main_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Euclidean distance', 'Bayesian optimization', 'MATLAB']",set(),set(),0,0.28174603174603174,0.36533751091117256
1123,2015,Hyperplane,http://dbpedia.org/resource/Hyperplane,http://en.wikipedia.org/wiki/Hyperplane,"In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. For example, if a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. This notion can be used in any general space in which the concept of the dimension of a subspace is defined. In different settings, hyperplanes may have different properties. For instance, a hyperplane of an n-dimensional affine space is a flat subset with dimension n − 1 and it separates the space into two half spaces. While a hyperplane of an n-dimensional projective space does not have this property. The difference in dimension between a subspace S and its ambient space X is known as the codimension of S with respect to X. Therefore, a necessary and sufficient condition for S to be a hyperplane in X is for S to have codimension one in X.",4678089120176901623,main_concept,Hyperplane separation theorem,2024-06-23 21:17:09.728,[],"['Euclidean geometry', 'Projective geometry']",set(),set(),0,3.908256880733945,0.3280381674677707
1124,2016,Hyperplane separation theorem,http://dbpedia.org/resource/Hyperplane_separation_theorem,http://en.wikipedia.org/wiki/Hyperplane_separation_theorem,"In geometry, the hyperplane separation theorem is a theorem about disjoint convex sets in n-dimensional Euclidean space. There are several rather similar versions. In one version of the theorem, if both these sets are closed and at least one of them is compact, then there is a hyperplane in between them and even two parallel hyperplanes in between them separated by a gap. In another version, if both disjoint convex sets are open, then there is a hyperplane in between them, but not necessarily any gap. An axis which is orthogonal to a separating hyperplane is a separating axis, because the orthogonal projections of the convex bodies onto the axis are disjoint. The hyperplane separation theorem is due to Hermann Minkowski. The Hahn–Banach separation theorem generalizes the result to topological vector spaces. A related result is the supporting hyperplane theorem. In the context of support-vector machines, the optimally separating hyperplane or maximum-margin hyperplane is a hyperplane which separates two convex hulls of points and is equidistant from the two.",9107239756200951092,main_concept,,2024-06-23 21:17:09.728,['Hermann Minkowski'],"['Hermann Minkowski', 'Hyperplane', 'Hyperplane separation theorem', 'Supporting hyperplane theorem']",set(),set(),0,1.148936170212766,1.2515542093119987
1125,2017,ELKI,http://dbpedia.org/resource/ELKI,http://en.wikipedia.org/wiki/ELKI,"ELKI (for Environment for DeveLoping KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.",2153617820667688959,related_concept,K-medoids,2024-06-23 21:17:09.728,"['Hans-Peter Kriegel', 'ELKI']","['Hans-Peter Kriegel', 'ELKI', 'SQL', 'Algorithm', 'DBSCAN', 'PDF', 'Database', 'Data']",set(),set(),0,0.47101449275362317,1.4068635454992846
1126,2018,KNIME,http://dbpedia.org/resource/KNIME,http://en.wikipedia.org/wiki/KNIME,"KNIME (/naɪm/), the Konstanz Information Miner, is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining ""Building Blocks of Analytics"" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming. Since 2006, KNIME has been used in pharmaceutical research, it also used in other areas such as CRM customer data analysis, business intelligence, text mining and financial data analysis. Recently attempts were made to use KNIME as robotic process automation (RPA) tool. KNIME's headquarters are based in Zurich, with additional offices in Konstanz, Berlin, and Austin (USA).",5511616679235935288,related_concept,K-medoids,2024-06-23 21:17:09.728,"['Analytics', 'KNIME']","['KNIME', 'Analytics', 'AI', 'Data']",set(),set(),0,1.1023622047244095,1.389042358038306
1127,2019,K-means clustering,http://dbpedia.org/resource/K-means_clustering,http://en.wikipedia.org/wiki/K-means_clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids. The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes. The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.",7700497135516975781,main_concept,K-medoids,2024-06-23 21:17:09.728,['Euclidean distance'],"['Euclidean distance', ""Lloyd's algorithm"", 'Vector quantization', 'Cluster analysis', 'Mean shift', 'Mean']",set(),set(),0,1.9708029197080292,3.311928263973308e-304
1128,2020,Greedy algorithm,http://dbpedia.org/resource/Greedy_algorithm,http://en.wikipedia.org/wiki/Greedy_algorithm,"A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. In many problems, a greedy strategy does not produce an optimal solution, but a greedy heuristic can yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time. For example, a greedy strategy for the travelling salesman problem (which is of high computational complexity) is the following heuristic: ""At each step of the journey, visit the nearest unvisited city."" This heuristic does not intend to find the best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids and give constant-factor approximations to optimization problems with the submodular structure.",7201510298321432435,related_concept,K-medoids,2024-06-23 21:17:09.728,[],"['Greedy algorithm', ""Kruskal's algorithm"", ""Prim's algorithm""]",set(),set(),0,2.4864864864864864,4.363951889264471e-304
1129,2021,R (programming language),http://dbpedia.org/resource/R_(programming_language),http://en.wikipedia.org/wiki/R_(programming_language),"R is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing. Created by statisticians Ross Ihaka and Robert Gentleman, R is used among data miners, bioinformaticians and statisticians for data analysis and developing statistical software. Users have created packages to augment the functions of the R language. According to user surveys and studies of scholarly literature databases, R is one of the most commonly used programming languages used in data mining. As of October 2022, R ranks 12th in the TIOBE index, a measure of programming language popularity, in which the language peaked in 8th place in August 2020. The official R software environment is an open-source free software environment within the GNU package, available under the GNU General Public License. It is written primarily in C, Fortran, and R itself (partially self-hosting). Precompiled executables are provided for various operating systems. R has a command line interface. Multiple third-party graphical user interfaces are also available, such as RStudio, an integrated development environment, and Jupyter, a notebook interface.",8962901602957010030,related_concept,K-medoids,2024-06-23 21:17:09.728,[],[],set(),set(),0,5.1891891891891895,1.3601994139686053
1130,2022,RapidMiner,http://dbpedia.org/resource/RapidMiner,http://en.wikipedia.org/wiki/RapidMiner,"RapidMiner is a data science platform designed for enterprises that analyses the collective impact of organizations’ employees, expertise and data. Rapid Miner's data science platform is intended to support many analytics users across a broad AI lifecycle. It was acquired by Altair Engineering in September 2022.",722008690140902586,related_concept,K-medoids,2024-06-23 21:17:09.728,"['AI', 'RapidMiner']","['RapidMiner', 'Data']",set(),set(),0,2.5128205128205128,1.3807101910540192
1131,2023,Julia language,http://dbpedia.org/resource/Julia_language,http://en.wikipedia.org/wiki/Julia_language,,7688104719468494952,related_concept,K-medoids,2024-06-23 21:17:09.728,[],"['Data', 'Julia language', 'Compiler', 'SQL', 'MATLAB', 'AI']",set(),set(),0,0.008583690987124463,0.42862674789452093
1132,2024,Euclidean distance,http://dbpedia.org/resource/Euclidean_distance,http://en.wikipedia.org/wiki/Euclidean_distance,"In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points.It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance. These names come from the ancient Greek mathematicians Euclid and Pythagoras, although Euclid did not represent distances as numbers, and the connection from the Pythagorean theorem to distance calculation was not made until the 18th century. The distance between two objects that are not points is usually defined to be the smallest distance among pairs of points from the two objects. Formulas are known for computing distances between different types of objects, such as the distance from a point to a line. In advanced mathematics, the concept of distance has been generalized to abstract metric spaces, and other distances than Euclidean have been studied. In some applications in statistics and optimization, the square of the Euclidean distance is used instead of the distance itself.",2200057527997668812,related_concept,K-medoids,2024-06-23 21:17:09.728,['Euclidean distance'],"['Euclidean distance', 'Euclidean plane']",set(),set(),0,2.5775862068965516,3.340628132392704e-304
1133,2025,Peter Rousseeuw,http://dbpedia.org/resource/Peter_Rousseeuw,http://en.wikipedia.org/wiki/Peter_Rousseeuw,"Peter J. Rousseeuw (born 13 October 1956) is a statistician known for his work on robust statistics and cluster analysis. He obtained his PhD in 1981 at the Vrije Universiteit Brussel, following research carried out at the ETH in Zurich, which led to a book on influence functions. Later he was professor at the Delft University of Technology, The Netherlands, at the University of Fribourg, Switzerland, and at the University of Antwerp, Belgium. Next he was a senior researcher at Renaissance Technologies. He then returned to Belgium as professor at KU Leuven, until becoming emeritus in 2022. His former PhD students include Annick Leroy, Hendrik Lopuhaä, Geert Molenberghs, Christophe Croux, Mia Hubert, Stefan Van Aelst, Tim Verdonck and Jakob Raymaekers.",7496858912675905360,related_concept,K-medoids,2024-06-23 21:17:09.728,[],"['Outlier', 'Covariance', 'Data', 'Medoids', 'Statistics']",set(),set(),0,0.4647887323943662,1.432339438561821
1134,2026,Rust (programming language),http://dbpedia.org/resource/Rust_(programming_language),http://en.wikipedia.org/wiki/Rust_(programming_language),"Rust is a multi-paradigm, general-purpose programming language. Rust emphasizes performance, type safety, and concurrency. Rust enforces memory safety—that is, that all references point to valid memory—without requiring the use of a garbage collector or reference counting present in other memory-safe languages. To simultaneously enforce memory safety and prevent concurrent data races, Rust's ""borrow checker"" tracks the object lifetime of all references in a program during compilation. Rust is popular for systems programming but also offers high-level features including some functional programming constructs. Software developer Graydon Hoare created Rust as a personal project while working at Mozilla Research in 2006. Mozilla officially sponsored the project in 2009. Since the first stable release in January 2014, Rust has been adopted by companies including Amazon, Discord, Dropbox, Facebook (Meta), Google (Alphabet), and Microsoft. Rust has been noted for its growth as a newer language and has been the subject of academic programming languages research.",3683892729837042385,related_concept,K-medoids,2024-06-23 21:17:09.728,[],['Data'],set(),set(),0,3.694915254237288,1.3725569114178935
1135,2027,Python (programming language),http://dbpedia.org/resource/Python_(programming_language),http://en.wikipedia.org/wiki/Python_(programming_language),"Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a ""batteries included"" language due to its comprehensive standard library. Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000 and introduced new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 3.0, released in 2008, was a major revision that is not completely backward-compatible with earlier versions. Python 2 was discontinued with version 2.7.18 in 2020. Python consistently ranks as one of the most popular programming languages.",3437846386585390517,related_concept,K-medoids,2024-06-23 21:17:09.728,[],"['Integrated development environment', 'SQL', 'Prolog', 'AI']",set(),set(),0,11.111751152073733,1.3758770670901688
1136,2028,Silhouette (clustering),http://dbpedia.org/resource/Silhouette_(clustering),http://en.wikipedia.org/wiki/Silhouette_(clustering),"Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified. It was proposed by Belgian statistician Peter Rousseeuw in 1987. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.",1580429621521082807,related_concept,K-medoids,2024-06-23 21:17:09.728,"['Manhattan distance', 'Euclidean distance', 'Peter Rousseeuw']","['Peter Rousseeuw', 'Manhattan distance', 'Euclidean distance']",set(),set(),0,1.5,0.577140534622643
1137,2029,MATLAB,http://dbpedia.org/resource/MATLAB,http://en.wikipedia.org/wiki/MATLAB,"MATLAB (an abbreviation of ""MATrix LABoratory"") is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages. Although MATLAB is intended primarily for numeric computing, an optional toolbox uses the MuPAD symbolic engine allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems. As of 2020, MATLAB has more than 4 million users worldwide. They come from various backgrounds of engineering, science, and economics.",1178606749493548710,related_concept,K-medoids,2024-06-23 21:17:09.728,['MATLAB'],"['MATLAB', 'SQL']",set(),set(),0,5.054862842892768,1.4165663680710217
1138,2030,Lloyd's algorithm,http://dbpedia.org/resource/Lloyd's_algorithm,http://en.wikipedia.org/wiki/Lloyd's_algorithm,"In electrical engineering and computer science, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces and partitions of these subsets into well-shaped and uniformly sized convex cells. Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition and then re-partitions the input according to which of these centroids is closest. In this setting, the mean operation is an integral over a region of space, and the nearest centroid operation results in Voronoi diagrams. Although the algorithm may be applied most directly to the Euclidean plane, similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input, which can be used for quantization, dithering, and stippling. Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method.",1823978690050994806,related_concept,K-medoids,2024-06-23 21:17:09.728,"['Euclidean plane', ""Lloyd's algorithm""]","[""Lloyd's algorithm"", 'Euclidean plane']",set(),set(),0,2.4130434782608696,0.9288912936416454
1139,2031,Medoids,http://dbpedia.org/resource/Medoids,http://en.wikipedia.org/wiki/Medoids,,4224656659715524797,related_concept,K-medoids,2024-06-23 21:17:09.728,[],"['Medoids', 'Manhattan distance', 'Euclidean distance', 'Active learning', 'Spectral clustering']",set(),set(),0,0.044444444444444446,4.2063007340924366e-305
1140,2032,Cluster analysis,http://dbpedia.org/resource/Cluster_analysis,http://en.wikipedia.org/wiki/Cluster_analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties. Besides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς ""grape""), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.",949912421046998236,main_concept,K-medoids,2024-06-23 21:17:09.728,['Cluster analysis'],"['Cluster analysis', 'UPGMA', 'WPGMA', 'Mean', ""Lloyd's algorithm"", 'DBSCAN', 'SUBCLU']",set(),set(),0,2.094364351245085,1.0333612318791756
1141,2033,Hierarchical clustering,http://dbpedia.org/resource/Hierarchical_clustering,http://en.wikipedia.org/wiki/Hierarchical_clustering,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories: 
* Agglomerative: This is a ""bottom-up"" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. 
* Divisive: This is a ""top-down"" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram. The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of and requires memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to , an improvement on the aforementioned bound of , at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable. Except for the special case of single-linkage, none of the algorithms (except exhaustive search in ) can be guaranteed to find the optimum solution. Divisive clustering with an exhaustive search is , but it is common to use faster heuristics to choose splits, such as k-means. Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances.",4755790795633543055,related_concept,K-medoids,2024-06-23 21:17:09.728,['Hierarchical clustering'],"['Hierarchical clustering', 'Euclidean distance', 'WPGMA']",set(),set(),0,2.306532663316583,3.3269011469470097e-304
1142,2034,K-medoids,http://dbpedia.org/resource/K-medoids,http://en.wikipedia.org/wiki/K-medoids,"The k-medoids problem is a clustering problem similar to k-means. The name was coined by Leonard Kaufman and Peter J. Rousseeuw with their PAM algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses actual data points as centers (medoids or exemplars), and thereby allows for greater interpretability of the cluster centers than in k-means, where the center of a cluster is not necessarily one of the input data points (it is the average between the points in the cluster). Furthermore, k-medoids can be used with arbitrary dissimilarity measures, whereas k-means generally requires Euclidean distance for efficient solutions. Because k-medoids minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances, it is more robust to noise and outliers than k-means. k-medoids is a classical partitioning technique of clustering that splits the data set of n objects into k clusters, where the number k of clusters assumed known a priori (which implies that the programmer must specify k before the execution of a k-medoids algorithm). The ""goodness"" of the given value of k can be assessed with methods such as the silhouette method. The medoid of a cluster is defined as the object in the cluster whose average dissimilarity to all the objects in the cluster is minimal, that is, it is a most centrally located point in the cluster.",5655145053160081781,main_concept,,2024-06-23 21:17:09.728,['Euclidean distance'],"['Euclidean distance', 'Medoids', 'Algorithm']",set(),set(),0,4.666666666666667,3.322899434223339e-304
